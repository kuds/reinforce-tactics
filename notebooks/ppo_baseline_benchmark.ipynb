{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/ppo_baseline_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "# Reinforce Tactics — PPO Baseline Training Benchmarks\n\nThis notebook trains a **MaskablePPO** agent against `SimpleBot` on the 6×6 beginner map\nand records reference metrics at five training checkpoints:\n\n| Checkpoint | Timesteps |\n|------------|-----------|\n| 1 | 10,000 |\n| 2 | 50,000 |\n| 3 | 200,000 |\n| 4 | 1,000,000 |\n| 5 | 2,000,000 |\n\nAt each checkpoint the agent is evaluated over **50 episodes** and we record:\n- **Win rate** (% of games won against SimpleBot)\n- **Average episode reward**\n- **Average episode length** (steps)\n\nThe goal is to provide a **reference curve** so that users can run the same\nnotebook and compare their results to known-good training runs.\n\n**Runtime:** CPU is fine (~20–40 min total). GPU will be faster.\n\n---\n\n### Why MaskablePPO?\n\nThe game has a `MultiDiscrete` action space where many action combinations\nare invalid at any given time (e.g. you can't attack a tile with no enemy).\n**Action masking** prevents the agent from sampling these invalid actions,\nwhich typically yields 2–3× faster convergence compared to plain PPO."
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q gymnasium stable-baselines3 sb3-contrib tensorboard pandas numpy torch matplotlib\n\nimport torch\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {DEVICE}\")\nif DEVICE == 'cuda':\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo and install as a package\nimport os, sys\nfrom pathlib import Path\n\nREPO_DIR = Path('reinforce-tactics')\nif REPO_DIR.exists():\n    os.chdir(REPO_DIR)\nelif Path('notebooks').exists():\n    # Already inside the repo\n    os.chdir('..')\nelse:\n    print('Cloning repository...')\n    !git clone https://github.com/kuds/reinforce-tactics.git\n    os.chdir(REPO_DIR)\n\n# Install the package so all imports resolve\n!pip install -q -e .\n\nif os.getcwd() not in sys.path:\n    sys.path.insert(0, os.getcwd())\n\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "\n",
    "from reinforcetactics.rl.masking import make_maskable_env, make_maskable_vec_env\n",
    "\n",
    "print('All imports successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gl5pd0w9otg",
   "source": "## 2b. Storage configuration\n\nChoose where to save benchmark outputs. Set `USE_GOOGLE_DRIVE = True` to\npersist results to Google Drive (recommended for Colab -- files survive\nruntime disconnects). Set `False` to use the default local/Colab storage.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "o5gbyhjrjoc",
   "source": "# --- Storage configuration ---\n# Set to True to save all outputs (checkpoints, results, plots) to Google Drive.\n# This is recommended when running on Google Colab, since files on the local\n# Colab filesystem are lost when the runtime disconnects.\n# Set to False to use the default local/Colab storage.\nUSE_GOOGLE_DRIVE = False\n\n# Google Drive save directory (only used when USE_GOOGLE_DRIVE is True).\n# This path is relative to your Google Drive root (MyDrive/).\n# Change it to customize where results are saved in your Drive.\nDRIVE_SAVE_DIR = 'reinforce-tactics/benchmarks/ppo_vs_simplebot'\n\n# --- Mount Google Drive if enabled ---\nif USE_GOOGLE_DRIVE:\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive')\n        SAVE_DIR = Path('/content/drive/MyDrive') / DRIVE_SAVE_DIR\n        SAVE_DIR.mkdir(parents=True, exist_ok=True)\n        print(f'Google Drive mounted successfully.')\n        print(f'Save directory: {SAVE_DIR}')\n    except ImportError:\n        print('WARNING: google.colab not available (not running in Colab).')\n        print('  Falling back to local storage.')\n        USE_GOOGLE_DRIVE = False\n        SAVE_DIR = None\n    except Exception as e:\n        print(f'WARNING: Failed to mount Google Drive: {e}')\n        print('  Falling back to local storage.')\n        USE_GOOGLE_DRIVE = False\n        SAVE_DIR = None\nelse:\n    SAVE_DIR = None\n    print('Using local storage (default).')\n    print('  Tip: Set USE_GOOGLE_DRIVE = True to persist results to Google Drive.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# --- Benchmark settings ---\nMAP_FILE        = 'maps/1v1/beginner.csv'   # 6x6 beginner map\nOPPONENT        = 'random'                   # Start with random for easier wins\nMAX_STEPS       = 1000                       # max steps per episode\nN_ENVS          = 4                          # parallel training envs\nSEED            = 42\n\n# Action space mode:\n#   'flat_discrete'  — exact per-action masks (recommended, eliminates invalid actions)\n#   'multi_discrete' — per-dimension masks (over-approximation, original behaviour)\nACTION_SPACE    = 'flat_discrete'\n\n# Checkpoints to evaluate\nCHECKPOINTS     = [10_000, 50_000, 200_000, 1_000_000, 2_000_000]\nEVAL_EPISODES   = 50                         # episodes per evaluation\n\n# --- Reward configuration ---\n# All reward signals are explicit here for easy tuning.\n# These override the library defaults in gym_env.py.\nREWARD_CONFIG = {\n    # Terminal rewards\n    'win':                1000.0,\n    'loss':              -1000.0,\n    'draw':              -200.0,    # Truncation/draw penalty (prevents stalling)\n\n    # Potential-based shaping (scaled down to avoid local optima)\n    'income_diff':          0.05,\n    'unit_diff':            0.3,\n    'structure_control':    1.0,\n\n    # Per-action rewards\n    'create_unit':          2.0,\n    'move':                 0.1,\n    'damage_scale':         0.2,    # reward per damage point\n    'kill':                10.0,\n    'seize_progress':       1.0,\n    'capture':             20.0,\n    'cure':                 5.0,\n    'heal_scale':           0.5,    # reward per HP healed\n    'paralyze':             8.0,\n    'haste':                6.0,\n    'defence_buff':         5.0,\n    'attack_buff':          5.0,\n\n    # Penalties\n    'invalid_action':     -10.0,\n    'turn_penalty':        -1.0,    # Strong penalty to discourage end_turn spam\n}\n\n# PPO hyperparameters\nPPO_CONFIG = dict(\n    learning_rate = 3e-4,\n    n_steps       = 2048,\n    batch_size    = 64,\n    n_epochs      = 10,\n    gamma         = 0.99,\n    gae_lambda    = 0.95,\n    clip_range    = 0.2,\n    ent_coef      = 0.05,   # Higher entropy for exploration\n    vf_coef       = 0.5,\n    max_grad_norm = 0.5,\n)\n\n# Output paths — uses Google Drive if enabled, otherwise local storage\nif SAVE_DIR is not None:\n    BENCHMARK_DIR = SAVE_DIR\nelse:\n    BENCHMARK_DIR = Path('benchmarks/ppo_vs_simplebot')\nBENCHMARK_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f'Map:          {MAP_FILE}')\nprint(f'Opponent:     {OPPONENT}')\nprint(f'Action space: {ACTION_SPACE}')\nprint(f'Max steps:    {MAX_STEPS}')\nprint(f'Checkpoints:  {CHECKPOINTS}')\nprint(f'Eval eps:     {EVAL_EPISODES}')\nprint(f'ent_coef:     {PPO_CONFIG[\"ent_coef\"]}')\nprint(f'turn_penalty: {REWARD_CONFIG[\"turn_penalty\"]}')\nprint(f'draw:         {REWARD_CONFIG[\"draw\"]}')\nprint(f'Storage:      {\"Google Drive\" if USE_GOOGLE_DRIVE else \"Local\"}')\nprint(f'Output dir:   {BENCHMARK_DIR}')"
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "## 4. Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-envs",
   "metadata": {},
   "outputs": [],
   "source": "# Training envs (vectorized, headless)\nvec_env = make_maskable_vec_env(\n    n_envs=N_ENVS,\n    map_file=MAP_FILE,\n    opponent=OPPONENT,\n    max_steps=MAX_STEPS,\n    reward_config=REWARD_CONFIG,\n    seed=SEED,\n    use_subprocess=False,   # DummyVecEnv (safer in notebooks)\n    action_space_type=ACTION_SPACE,\n)\n\n# Separate eval env (single, deterministic)\neval_env = make_maskable_env(\n    map_file=MAP_FILE,\n    opponent=OPPONENT,\n    max_steps=MAX_STEPS,\n    reward_config=REWARD_CONFIG,\n    action_space_type=ACTION_SPACE,\n)\n\nprint(f'Observation space: {vec_env.observation_space}')\nprint(f'Action space:      {vec_env.action_space}')"
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 5. Create MaskablePPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskablePPO(\n",
    "    'MultiInputPolicy',\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=str(BENCHMARK_DIR / 'tensorboard'),\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    **PPO_CONFIG,\n",
    ")\n",
    "\n",
    "print('MaskablePPO model created.')\n",
    "print(f'Policy:  {model.policy.__class__.__name__}')\n",
    "print(f'Device:  {model.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-fn-header",
   "metadata": {},
   "source": [
    "## 6. Evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, n_episodes=50):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model and return summary statistics.\n",
    "\n",
    "    Returns dict with: win_rate, avg_reward, std_reward,\n",
    "    avg_length, std_length, wins, losses, draws\n",
    "    \"\"\"\n",
    "    wins, losses, draws = 0, 0, 0\n",
    "    rewards, lengths = [], []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        while not done:\n",
    "            masks = env.action_masks()\n",
    "            action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            ep_len += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        lengths.append(ep_len)\n",
    "\n",
    "        winner = info.get('winner')\n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "        elif winner is not None:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return {\n",
    "        'win_rate':    wins / n_episodes,\n",
    "        'avg_reward':  float(np.mean(rewards)),\n",
    "        'std_reward':  float(np.std(rewards)),\n",
    "        'avg_length':  float(np.mean(lengths)),\n",
    "        'std_length':  float(np.std(lengths)),\n",
    "        'wins':        wins,\n",
    "        'losses':      losses,\n",
    "        'draws':       draws,\n",
    "    }\n",
    "\n",
    "print('evaluate_model() defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": "## 7. Train and evaluate at each checkpoint\n\nWe train incrementally: 0 → 10K → 50K → 200K → 1M → 2M timesteps,\nevaluating at each checkpoint."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "trained_so_far = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for checkpoint_ts in CHECKPOINTS:\n",
    "    steps_to_train = checkpoint_ts - trained_so_far\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training {trained_so_far:,} -> {checkpoint_ts:,} '\n",
    "          f'({steps_to_train:,} steps)...')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.learn(\n",
    "        total_timesteps=steps_to_train,\n",
    "        reset_num_timesteps=False,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "    train_time = time.time() - t0\n",
    "    trained_so_far = checkpoint_ts\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt_path = BENCHMARK_DIR / f'model_{checkpoint_ts}.zip'\n",
    "    model.save(str(ckpt_path))\n",
    "    print(f'Saved checkpoint: {ckpt_path}')\n",
    "\n",
    "    # Evaluate\n",
    "    print(f'Evaluating over {EVAL_EPISODES} episodes...')\n",
    "    metrics = evaluate_model(model, eval_env, n_episodes=EVAL_EPISODES)\n",
    "    metrics['timesteps'] = checkpoint_ts\n",
    "    metrics['train_time_s'] = round(train_time, 1)\n",
    "    results.append(metrics)\n",
    "\n",
    "    print(f'  Win rate:       {metrics[\"win_rate\"]*100:.1f}%')\n",
    "    print(f'  Avg reward:     {metrics[\"avg_reward\"]:.1f} '\n",
    "          f'(+/- {metrics[\"std_reward\"]:.1f})')\n",
    "    print(f'  Avg length:     {metrics[\"avg_length\"]:.1f} '\n",
    "          f'(+/- {metrics[\"std_length\"]:.1f})')\n",
    "    print(f'  W/L/D:          {metrics[\"wins\"]}/{metrics[\"losses\"]}/{metrics[\"draws\"]}')\n",
    "    print(f'  Training time:  {train_time:.1f}s')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nTotal wall time: {total_time/60:.1f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 8. Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df['win_rate_pct'] = (df['win_rate'] * 100).round(1)\n",
    "df['avg_reward'] = df['avg_reward'].round(1)\n",
    "df['avg_length'] = df['avg_length'].round(1)\n",
    "\n",
    "display_df = df[['timesteps', 'win_rate_pct', 'avg_reward', 'avg_length',\n",
    "                  'wins', 'losses', 'draws', 'train_time_s']].copy()\n",
    "display_df.columns = ['Timesteps', 'Win Rate (%)', 'Avg Reward',\n",
    "                       'Avg Length', 'Wins', 'Losses', 'Draws',\n",
    "                       'Train Time (s)']\n",
    "display_df = display_df.set_index('Timesteps')\n",
    "display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plots-header",
   "metadata": {},
   "source": [
    "## 9. Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ts = [r['timesteps'] for r in results]\n",
    "\n",
    "# Win rate\n",
    "ax = axes[0]\n",
    "wr = [r['win_rate'] * 100 for r in results]\n",
    "ax.plot(ts, wr, 'o-', color='#2196F3', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title('Win Rate vs SimpleBot')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim(-5, 105)\n",
    "ax.axhline(y=70, color='green', linestyle='--', alpha=0.5, label='70% target')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Average reward\n",
    "ax = axes[1]\n",
    "avg_r = [r['avg_reward'] for r in results]\n",
    "std_r = [r['std_reward'] for r in results]\n",
    "ax.plot(ts, avg_r, 'o-', color='#FF9800', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_r, std_r)],\n",
    "                [a + s for a, s in zip(avg_r, std_r)],\n",
    "                alpha=0.2, color='#FF9800')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Reward')\n",
    "ax.set_title('Average Episode Reward')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode length\n",
    "ax = axes[2]\n",
    "avg_l = [r['avg_length'] for r in results]\n",
    "std_l = [r['std_length'] for r in results]\n",
    "ax.plot(ts, avg_l, 'o-', color='#4CAF50', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_l, std_l)],\n",
    "                [a + s for a, s in zip(avg_l, std_l)],\n",
    "                alpha=0.2, color='#4CAF50')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Length (steps)')\n",
    "ax.set_title('Average Episode Length')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('PPO Baseline Benchmarks  |  6x6 beginner map  |  vs SimpleBot',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(str(BENCHMARK_DIR / 'training_curves.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(f'Saved plot: {BENCHMARK_DIR / \"training_curves.png\"}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zxt9j70tka",
   "source": "## 9b. Diagnose training failures\n\nIf you see **0% win rate** and all games ending as **draws at 500 steps**, the agent\nis not learning to win — it's farming shaping rewards. Run the cell below to\ndiagnose the specific failure mode and get targeted recommendations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gm8kecvhgil",
   "source": "def diagnose_training(results):\n    \"\"\"Analyze benchmark results and print a diagnosis.\"\"\"\n    if not results:\n        print(\"No results to analyze.\")\n        return\n\n    # --- Collect signals ---\n    win_rates = [r['win_rate'] for r in results]\n    avg_rewards = [r['avg_reward'] for r in results]\n    avg_lengths = [r['avg_length'] for r in results]\n    draw_counts = [r['draws'] for r in results]\n    loss_counts = [r['losses'] for r in results]\n    win_counts = [r['wins'] for r in results]\n    n_eval = results[0].get('wins', 0) + results[0].get('losses', 0) + results[0].get('draws', 0)\n\n    all_draws = all(d == n_eval for d in draw_counts)\n    all_max_len = all(abs(l - MAX_STEPS) < 1.0 for l in avg_lengths)\n    rewards_positive = all(r > 0 for r in avg_rewards)\n    rewards_declining = len(avg_rewards) >= 2 and avg_rewards[-1] < avg_rewards[0] * 0.7\n    no_wins = all(w == 0.0 for w in win_rates)\n    late_losses = loss_counts[-1] > 0 and all(lc == 0 for lc in loss_counts[:-1])\n    all_losses_late = loss_counts[-1] == n_eval if len(loss_counts) > 0 else False\n    negative_rewards = all(r < -1000 for r in avg_rewards[:-1]) if len(avg_rewards) > 1 else False\n\n    # Current config values for diagnosis messages\n    rc = REWARD_CONFIG\n    sc = rc.get('structure_control', 1.0)\n    ud = rc.get('unit_diff', 0.3)\n    tp = rc.get('turn_penalty', -1.0)\n    ec = PPO_CONFIG.get('ent_coef', 0.05)\n\n    print(\"=\" * 65)\n    print(\"  TRAINING DIAGNOSTICS\")\n    print(\"=\" * 65)\n\n    # --- Summary table ---\n    print(f\"\\n{'Timesteps':>12}  {'WR':>6}  {'Reward':>10}  {'Length':>8}  {'W/L/D'}\")\n    print(\"-\" * 58)\n    for r in results:\n        ts = r['timesteps']\n        wr = r['win_rate'] * 100\n        rw = r['avg_reward']\n        ln = r['avg_length']\n        wld = f\"{r['wins']}/{r['losses']}/{r['draws']}\"\n        print(f\"{ts:>12,}  {wr:>5.1f}%  {rw:>10.1f}  {ln:>8.1f}  {wld}\")\n\n    # --- Failure mode detection ---\n    print(f\"\\n{'─' * 65}\")\n    print(\"  DIAGNOSIS\")\n    print(f\"{'─' * 65}\")\n\n    if no_wins and all_draws and all_max_len and rewards_positive:\n        print(f\"\"\"\nFAILURE MODE: Shaping-reward stalemate (flat_discrete)\n\nThe agent takes only valid actions (exact masking is working), earns\npositive shaping rewards by creating units and controlling structures,\nbut never finishes a game within the step limit.\n\nRoot cause: The agent has NEVER experienced a +{rc['win']:.0f} win or {rc['loss']:.0f} loss\nreward. With no terminal signal, it optimizes shaping rewards instead\nof pursuing victory. Both sides create units and trade blows without\neither achieving total elimination within {MAX_STEPS} steps.\n\"\"\")\n        if rewards_declining:\n            print(f\"⚠  Reward dropped from {avg_rewards[0]:.0f} → {avg_rewards[-1]:.0f}\")\n            print(\"   at 1M steps, suggesting policy degradation over time.\\n\")\n\n    elif no_wins and negative_rewards and all_max_len:\n        print(f\"\"\"\nFAILURE MODE: Invalid-action penalty flood (multi_discrete)\n\n~99% of sampled actions are game-invalid due to per-dimension mask\nover-approximation. The {rc.get('invalid_action', -10)} penalty per invalid action dominates the\nreward signal, making it impossible to learn from actual gameplay.\n\"\"\")\n        if all_losses_late:\n            print(\"At 1M steps the agent collapsed to spamming end_turn to avoid\\n\"\n                  \"penalties, letting SimpleBot win every game.\\n\")\n\n    elif no_wins and late_losses:\n        print(\"\"\"\nFAILURE MODE: Policy collapse\n\nEarly episodes stalemate (draws), then the agent collapses to a\ndegenerate strategy (e.g., always ending turn) and starts losing.\n\"\"\")\n    else:\n        if max(win_rates) > 0:\n            best_idx = win_rates.index(max(win_rates))\n            print(f\"\\nBest win rate: {max(win_rates)*100:.1f}% at \"\n                  f\"{results[best_idx]['timesteps']:,} timesteps.\")\n            if win_rates[-1] < max(win_rates):\n                print(\"Win rate is declining — possible overfitting or learning rate too high.\")\n        else:\n            print(\"\\n0% win rate across all checkpoints. Review environment configuration.\")\n\n    # --- Should I train longer? ---\n    print(f\"{'─' * 65}\")\n    print(\"  SHOULD YOU TRAIN LONGER?\")\n    print(f\"{'─' * 65}\")\n\n    if no_wins and all_draws:\n        print(\"\"\"\n  NO. Training longer will not help.\n\n  The agent is stuck in a local optimum (maximizing shaping rewards\n  without learning to win). More timesteps will not change this —\n  the agent needs a different reward structure to break out.\n\"\"\")\n    elif no_wins:\n        print(\"\"\"\n  NO. The current configuration has a fundamental issue preventing\n  the agent from learning to win. Fix the issues below first.\n\"\"\")\n    elif win_rates[-1] > win_rates[-2] if len(win_rates) >= 2 else False:\n        print(\"\"\"\n  MAYBE. Win rate is still increasing — more training could help.\n  But check if the rate of improvement is slowing significantly.\n\"\"\")\n    else:\n        print(\"\"\"\n  PROBABLY NOT. Win rate has plateaued or is declining.\n  Consider tuning hyperparameters or reward configuration.\n\"\"\")\n\n    # --- Recommendations ---\n    print(f\"{'─' * 65}\")\n    print(\"  RECOMMENDED FIXES (in priority order)\")\n    print(f\"{'─' * 65}\")\n\n    fixes = []\n    if no_wins and all_draws and all_max_len:\n        fixes = [\n            (\"Reduce MAX_STEPS\",\n             f\"  Current value: {MAX_STEPS}. For a 6×6 map, try 150–200.\\n\"\n             \"  Shorter episodes force earlier confrontation and make terminal\\n\"\n             \"  rewards reachable. Edit MAX_STEPS in the config cell above.\"),\n            (\"Add/increase truncation penalty\",\n             f\"  Current draw penalty: {rc.get('draw', 0.0)}. When episodes are\\n\"\n             \"  truncated (timeout), the agent needs a negative signal.\\n\"\n             \"  Edit REWARD_CONFIG['draw'] in the config cell above (try -200).\"),\n            (\"Reduce shaping reward magnitudes\",\n             f\"  Current: structure_control={sc}, unit_diff={ud}.\\n\"\n             \"  If these are too generous, the agent farms shaping rewards.\\n\"\n             \"  Edit REWARD_CONFIG in the config cell (try structure_control=1.0,\\n\"\n             \"  unit_diff=0.3, income_diff=0.05).\"),\n            (\"Increase turn_penalty\",\n             f\"  Current: {tp}. Make stalling costly so the agent pushes\\n\"\n             \"  to end games decisively.\\n\"\n             \"  Edit REWARD_CONFIG['turn_penalty'] (try -1.0).\"),\n            (\"Increase ent_coef for exploration\",\n             f\"  Current: {ec}. Higher entropy keeps the agent exploring\\n\"\n             \"  aggressive strategies instead of narrowing too quickly.\\n\"\n             \"  Edit PPO_CONFIG['ent_coef'] (try 0.05, range 0.02–0.1).\"),\n            (\"Start with a weaker opponent\",\n             f\"  Current: OPPONENT='{OPPONENT}'. Train against 'random' first\\n\"\n             \"  so the agent experiences wins early, then graduate to 'bot'.\\n\"\n             \"  Edit OPPONENT in the config cell above.\"),\n        ]\n    elif negative_rewards:\n        fixes = [\n            (\"Switch to flat_discrete action space\",\n             \"  Eliminates 99% invalid actions caused by per-dimension masking.\\n\"\n             \"  → Set: ACTION_SPACE = 'flat_discrete'  (already the notebook default)\"),\n            (\"Reduce invalid_action penalty\",\n             f\"  If using multi_discrete, reduce from {rc.get('invalid_action', -10)} to -0.1.\\n\"\n             \"  Edit REWARD_CONFIG['invalid_action'] in the config cell.\"),\n        ]\n\n    if not fixes:\n        fixes = [\n            (\"Review REWARD_CONFIG and PPO_CONFIG\",\n             \"  All tuneable parameters are in the config cell above.\\n\"\n             \"  Check that action masking, opponent, and rewards are correctly set.\"),\n        ]\n\n    for i, (title, detail) in enumerate(fixes, 1):\n        print(f\"\\n  {i}. {title}\\n{detail}\")\n\n    print(f\"\\n{'=' * 65}\")\n\n\ndiagnose_training(results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "z7jaii7t2i",
   "source": "## 9c. Evaluation replay log\n\nRecord every move the agent makes during evaluation and save to JSON.\nThis lets you inspect exactly what the agent is doing each step — is it\nspamming end_turn? Never attacking? Ignoring enemies?\n\nSet `N_REPLAY_EPISODES` to control how many episodes to record\n(default 3 to keep file sizes small).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cw76eafu8a",
   "source": "N_REPLAY_EPISODES = 3   # episodes to record per checkpoint\n\nACTION_NAMES = [\n    'create_unit', 'move', 'attack', 'seize', 'heal',\n    'end_turn', 'paralyze', 'haste', 'defence_buff', 'attack_buff',\n]\nUNIT_NAMES = ['W', 'M', 'C', 'A', 'K', 'R', 'S', 'B']\n\n\ndef _snapshot_game_state(env):\n    \"\"\"Capture a summary of the current game state.\"\"\"\n    gs = env.unwrapped.game_state\n    ap = env.unwrapped.agent_player\n    opp = 3 - ap\n    return {\n        'agent_gold': gs.player_gold.get(ap, 0),\n        'opponent_gold': gs.player_gold.get(opp, 0),\n        'agent_units': sum(1 for u in gs.units if u.player == ap),\n        'opponent_units': sum(1 for u in gs.units if u.player == opp),\n        'agent_structures': len(gs.grid.get_capturable_tiles(player=ap)),\n        'opponent_structures': len(gs.grid.get_capturable_tiles(player=opp)),\n        'turn': gs.turn_number,\n    }\n\n\ndef evaluate_with_replay(model, env, n_episodes=3):\n    \"\"\"\n    Run evaluation episodes and record every action to a replay log.\n\n    Returns:\n        List of episode dicts, each containing 'steps', 'outcome', etc.\n    \"\"\"\n    episodes = []\n\n    for ep_idx in range(n_episodes):\n        obs, _ = env.reset()\n        done = False\n        ep_reward = 0.0\n        steps = []\n        step_num = 0\n\n        while not done:\n            masks = env.action_masks()\n            action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n\n            # Decode the action BEFORE stepping\n            raw_action = action\n            if ACTION_SPACE == 'flat_discrete':\n                action_idx = int(action)\n                inner = env.unwrapped\n                if 0 <= action_idx < len(inner._current_actions):\n                    action_arr = inner._current_actions[action_idx]\n                else:\n                    action_arr = np.array([5, 0, 0, 0, 0, 0])\n            else:\n                action_arr = np.asarray(action)\n\n            action_type = int(action_arr[0])\n            action_name = ACTION_NAMES[action_type] if action_type < len(ACTION_NAMES) else f'unknown_{action_type}'\n            unit_type = UNIT_NAMES[int(action_arr[1]) % 8]\n            from_pos = [int(action_arr[2]), int(action_arr[3])]\n            to_pos = [int(action_arr[4]), int(action_arr[5])]\n\n            # Step\n            obs, reward, terminated, truncated, info = env.step(raw_action)\n            ep_reward += reward\n            step_num += 1\n            done = terminated or truncated\n\n            step_record = {\n                'step': step_num,\n                'action': action_name,\n                'unit_type': unit_type if action_type == 0 else None,\n                'from': from_pos,\n                'to': to_pos,\n                'reward': round(float(reward), 3),\n                'cumulative_reward': round(float(ep_reward), 3),\n                'valid': info.get('valid_action', True),\n                'game_state': _snapshot_game_state(env),\n            }\n            steps.append(step_record)\n\n        winner = info.get('winner')\n        if winner == 1:\n            outcome = 'win'\n        elif winner is not None:\n            outcome = 'loss'\n        else:\n            outcome = 'draw'\n\n        # Extract final turn count from last step's game state\n        final_turn = steps[-1]['game_state']['turn'] if steps else 0\n\n        episodes.append({\n            'episode': ep_idx,\n            'outcome': outcome,\n            'total_reward': round(float(ep_reward), 2),\n            'length': step_num,\n            'turns': final_turn,\n            'steps': steps,\n        })\n\n    return episodes\n\n\nprint('evaluate_with_replay() defined.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "97p28vin5o7",
   "source": "# Record replays from the final checkpoint\nprint(f'Recording {N_REPLAY_EPISODES} replay episodes from final model...\\n')\nreplay_episodes = evaluate_with_replay(model, eval_env, n_episodes=N_REPLAY_EPISODES)\n\n# Save to JSON\nreplay_path = BENCHMARK_DIR / 'eval_replays.json'\nreplay_data = {\n    'metadata': {\n        'timesteps': CHECKPOINTS[-1],\n        'map': MAP_FILE,\n        'opponent': OPPONENT,\n        'action_space': ACTION_SPACE,\n        'max_steps': MAX_STEPS,\n    },\n    'episodes': replay_episodes,\n}\nwith open(replay_path, 'w') as f:\n    json.dump(replay_data, f, indent=2)\nprint(f'Saved replays: {replay_path}')\n\n# --- Print summary for each episode ---\nfor ep in replay_episodes:\n    # Extract turn count from the last step's game state\n    final_turn = ep['steps'][-1]['game_state']['turn'] if ep['steps'] else 0\n\n    print(f'\\n{\"─\" * 55}')\n    print(f'Episode {ep[\"episode\"]}  |  outcome={ep[\"outcome\"]}  |  '\n          f'length={ep[\"length\"]}  |  turns={final_turn}  |  '\n          f'reward={ep[\"total_reward\"]}')\n    print(f'{\"─\" * 55}')\n\n    # Action distribution\n    from collections import Counter\n    action_counts = Counter(s['action'] for s in ep['steps'])\n    total = len(ep['steps'])\n    print(f'\\n  Action distribution ({total} steps):')\n    for action, count in action_counts.most_common():\n        pct = count / total * 100\n        bar = '#' * int(pct / 2)\n        print(f'    {action:15s}  {count:4d}  ({pct:5.1f}%)  {bar}')\n\n    # Unit creation breakdown (by unit type)\n    create_steps = [s for s in ep['steps'] if s['action'] == 'create_unit']\n    if create_steps:\n        unit_counts = Counter(s['unit_type'] for s in create_steps)\n        print(f'\\n  Units created ({len(create_steps)} total):')\n        for ut, count in unit_counts.most_common():\n            bar = '#' * count\n            print(f'    {ut:3s}  {count:3d}  {bar}')\n\n    # Captures: detect when agent_structures increases after a seize action\n    captures = 0\n    for i, s in enumerate(ep['steps']):\n        if s['action'] == 'seize' and s['valid'] and i > 0:\n            prev_structures = ep['steps'][i - 1]['game_state']['agent_structures']\n            curr_structures = s['game_state']['agent_structures']\n            if curr_structures > prev_structures:\n                captures += 1\n    seize_count = action_counts.get('seize', 0)\n    print(f'\\n  Structures captured: {captures}'\n          f' (from {seize_count} seize actions)')\n\n    # Per-turn breakdown: how many steps (actions) the agent took each turn\n    turns = {}\n    for s in ep['steps']:\n        t = s['game_state']['turn']\n        turns.setdefault(t, 0)\n        turns[t] += 1\n    print(f'\\n  Steps per turn ({len(turns)} turns):')\n    for t in sorted(turns.keys()):\n        bar = '#' * min(turns[t], 40)\n        print(f'    turn {t:3d}: {turns[t]:3d} steps  {bar}')\n    if final_turn > 0:\n        print(f'    avg: {total / final_turn:.1f} steps/turn')\n\n    # Invalid action count\n    invalid = sum(1 for s in ep['steps'] if not s['valid'])\n    if invalid > 0:\n        print(f'\\n  Invalid actions: {invalid}/{total} ({invalid/total*100:.1f}%)')\n\n    # Game state at end\n    final = ep['steps'][-1]['game_state']\n    print(f'\\n  Final state (step {ep[\"length\"]}, turn {final[\"turn\"]}):')\n    print(f'    Agent:    {final[\"agent_units\"]} units, '\n          f'{final[\"agent_structures\"]} structures, '\n          f'{final[\"agent_gold\"]} gold')\n    print(f'    Opponent: {final[\"opponent_units\"]} units, '\n          f'{final[\"opponent_structures\"]} structures, '\n          f'{final[\"opponent_gold\"]} gold')\n\n    # Show first 10 and last 10 moves\n    steps = ep['steps']\n    print(f'\\n  First 10 moves:')\n    for s in steps[:10]:\n        gs = s['game_state']\n        ut = f' ({s[\"unit_type\"]})' if s['unit_type'] else ''\n        valid_marker = '' if s['valid'] else ' [INVALID]'\n        print(f'    step {s[\"step\"]:3d}  {s[\"action\"]:15s}{ut:5s}  '\n              f'{s[\"from\"]}→{s[\"to\"]}  r={s[\"reward\"]:+.1f}  '\n              f'units={gs[\"agent_units\"]}v{gs[\"opponent_units\"]}  '\n              f'turn={gs[\"turn\"]}{valid_marker}')\n\n    if len(steps) > 20:\n        print(f'    ... ({len(steps) - 20} steps omitted) ...')\n\n    if len(steps) > 10:\n        print(f'  Last 10 moves:')\n        for s in steps[-10:]:\n            gs = s['game_state']\n            ut = f' ({s[\"unit_type\"]})' if s['unit_type'] else ''\n            valid_marker = '' if s['valid'] else ' [INVALID]'\n            print(f'    step {s[\"step\"]:3d}  {s[\"action\"]:15s}{ut:5s}  '\n                  f'{s[\"from\"]}→{s[\"to\"]}  r={s[\"reward\"]:+.1f}  '\n                  f'units={gs[\"agent_units\"]}v{gs[\"opponent_units\"]}  '\n                  f'turn={gs[\"turn\"]}{valid_marker}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 10. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": "# Save benchmark results as JSON\nbenchmark_data = {\n    'metadata': {\n        'date': datetime.now().isoformat(),\n        'map': MAP_FILE,\n        'opponent': OPPONENT,\n        'max_steps': MAX_STEPS,\n        'n_envs': N_ENVS,\n        'eval_episodes': EVAL_EPISODES,\n        'seed': SEED,\n        'device': DEVICE,\n        'ppo_config': PPO_CONFIG,\n        'reward_config': REWARD_CONFIG,\n        'storage': 'google_drive' if USE_GOOGLE_DRIVE else 'local',\n    },\n    'results': results,\n}\n\nresults_path = BENCHMARK_DIR / 'benchmark_results.json'\nwith open(results_path, 'w') as f:\n    json.dump(benchmark_data, f, indent=2)\n\nprint(f'Saved results:  {results_path}')\n\n# Also save as CSV for easy viewing\ncsv_path = BENCHMARK_DIR / 'benchmark_results.csv'\ndf.to_csv(csv_path, index=False)\nprint(f'Saved CSV:      {csv_path}')\n\n# List all saved files\nprint(f'\\nAll benchmark files:')\nfor p in sorted(BENCHMARK_DIR.iterdir()):\n    if p.is_file():\n        size = p.stat().st_size\n        if size > 1024 * 1024:\n            size_str = f'{size / 1024 / 1024:.1f} MB'\n        elif size > 1024:\n            size_str = f'{size / 1024:.1f} KB'\n        else:\n            size_str = f'{size} B'\n        print(f'  {p.name:40s}  {size_str}')\n\nif USE_GOOGLE_DRIVE:\n    print(f'\\nFiles are saved to Google Drive at:')\n    print(f'  My Drive/{DRIVE_SAVE_DIR}/')\n    print(f'  These files will persist after the Colab runtime disconnects.')\nelse:\n    print(f'\\nFiles are saved to local storage at:')\n    print(f'  {BENCHMARK_DIR.resolve()}')\n    print(f'  WARNING: These files will be lost if the Colab runtime disconnects.')\n    print(f'  Set USE_GOOGLE_DRIVE = True in the storage config cell to persist them.')"
  },
  {
   "cell_type": "markdown",
   "id": "tensorboard-header",
   "metadata": {},
   "source": [
    "## 11. TensorBoard (optional)\n",
    "\n",
    "Launch TensorBoard to inspect detailed training metrics (loss, entropy, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to launch TensorBoard inline:\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir benchmarks/ppo_vs_simplebot/tensorboard\n",
    "\n",
    "print('To view TensorBoard locally, run:')\n",
    "print(f'  tensorboard --logdir {BENCHMARK_DIR / \"tensorboard\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpret-header",
   "metadata": {},
   "source": "## 12. Interpreting the results\n\n### What to expect\n\n| Timesteps | Expected Win Rate | Notes |\n|-----------|-------------------|-------|\n| 10K | 0–15% | Agent is mostly random, learning basic actions |\n| 50K | 15–40% | Agent starts making meaningful moves |\n| 200K | 40–70% | Competent play, learns unit creation and combat |\n| 1M | 60–90%+ | Strong play against SimpleBot |\n| 2M | 75–95%+ | Refined play, diminishing returns |\n\n**Note:** Exact numbers depend on hardware and random seed. The important\nthing is that your curve has a similar *shape* — monotonically increasing\nwin rate with diminishing returns after ~200K steps.\n\n### If your results differ significantly\n\n- **Much worse:** Check that action masking is working (the agent should\n  rarely attempt invalid actions). Verify the map file path is correct.\n- **Much better:** You may have found better hyperparameters! Consider\n  contributing them back.\n- **Unstable (oscillating win rate):** Try reducing the learning rate\n  or increasing the batch size.\n\n### Next steps\n\n1. **Try different maps:** Larger maps (10×10, 14×14) are harder\n2. **Tune hyperparameters:** Adjust `ent_coef`, `learning_rate`, etc.\n3. **Self-play training:** See `train/train_self_play.py`\n4. **AlphaZero:** See `train/train_alphazero.py` for MCTS-based training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up environments\n",
    "vec_env.close()\n",
    "eval_env.close()\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}