{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Reinforce Tactics \u2014 Kaggle Demo\n",
    "\n",
    "Train a **MaskablePPO** agent to play a turn-based strategy game using\n",
    "reinforcement learning, right here on Kaggle.\n",
    "\n",
    "**What this notebook does:**\n",
    "\n",
    "1. Installs the game and RL dependencies\n",
    "2. Creates the Gymnasium environment (6\u00d76 beginner map, headless)\n",
    "3. Trains a MaskablePPO agent with action masking against a random opponent\n",
    "4. Evaluates the agent at three checkpoints (10K, 50K, 200K timesteps)\n",
    "5. Visualises training curves and replays agent behaviour\n",
    "\n",
    "**Runtime:** CPU is fine (\u223c10\u201315 min). GPU (P100/T4) will be faster.\n",
    "\n",
    "---\n",
    "\n",
    "### About the game\n",
    "\n",
    "Reinforce Tactics is a turn-based strategy game where players create units,\n",
    "capture structures, and eliminate the opponent. The RL agent observes a grid\n",
    "with terrain, units, and global features (gold, turn number, etc.), and\n",
    "chooses from a flat-discrete action space with exact action masking\n",
    "to avoid invalid moves.\n",
    "\n",
    "| Unit | Cost | Move | HP | Role |\n",
    "|------|------|------|----|------|\n",
    "| Warrior | 200 | 3 | 15 | Melee tank |\n",
    "| Mage | 300 | 2 | 10 | Ranged + Paralyze |\n",
    "| Cleric | 200 | 2 | 8 | Heal / Cure |\n",
    "| Archer | 250 | 3 | 15 | Long range |\n",
    "| Knight | 350 | 4 | 18 | Charge bonus |\n",
    "| Rogue | 350 | 4 | 12 | Flank + Evade |\n",
    "| Sorcerer | 400 | 2 | 10 | Buffs |\n",
    "| Barbarian | 400 | 5 | 20 | Fast melee |\n",
    "\n",
    "**Repository:** [github.com/kuds/reinforce-tactics](https://github.com/kuds/reinforce-tactics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q gymnasium stable-baselines3 sb3-contrib tensorboard pandas numpy torch matplotlib\n",
    "\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {DEVICE}\")\n",
    "if DEVICE == 'cuda':\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo and install as a package\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_DIR = Path('reinforce-tactics')\n",
    "\n",
    "# Detect environment\n",
    "if REPO_DIR.exists():\n",
    "    os.chdir(REPO_DIR)\n",
    "elif Path('notebooks').exists():\n",
    "    # Already inside the repo\n",
    "    os.chdir('..')\n",
    "else:\n",
    "    print('Cloning repository...')\n",
    "    !git clone https://github.com/kuds/reinforce-tactics.git\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "# Install the package so all imports resolve\n",
    "!pip install -q -e .\n",
    "\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sb3_contrib import MaskablePPO\n",
    "\n",
    "from reinforcetactics.rl.masking import make_maskable_env, make_maskable_vec_env\n",
    "\n",
    "print('All imports successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "All tuneable knobs are in this one cell. The defaults are chosen for a\n",
    "quick demo (\u223c10\u201315 min on CPU). To train a stronger agent, increase\n",
    "`CHECKPOINTS` and optionally switch `OPPONENT` to `'bot'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Benchmark settings ---\n",
    "MAP_FILE        = 'maps/1v1/beginner.csv'   # 6x6 beginner map\n",
    "OPPONENT        = 'random'                   # 'random' for easy wins, 'bot' for SimpleBot\n",
    "MAX_STEPS       = 200                        # Max steps per episode\n",
    "N_ENVS          = 4                          # Parallel training envs\n",
    "SEED            = 42\n",
    "\n",
    "# Action space mode:\n",
    "#   'flat_discrete'  - exact per-action masks (recommended)\n",
    "#   'multi_discrete' - per-dimension masks (original, ~99% invalid actions)\n",
    "ACTION_SPACE    = 'flat_discrete'\n",
    "\n",
    "# Checkpoints to evaluate\n",
    "CHECKPOINTS     = [10_000, 50_000, 200_000]\n",
    "EVAL_EPISODES   = 30                         # Episodes per evaluation\n",
    "\n",
    "# --- Reward configuration ---\n",
    "REWARD_CONFIG = {\n",
    "    # Terminal rewards\n",
    "    'win':                1000.0,\n",
    "    'loss':              -1000.0,\n",
    "    'draw':              -200.0,\n",
    "\n",
    "    # Potential-based shaping\n",
    "    'income_diff':          0.05,\n",
    "    'unit_diff':            0.3,\n",
    "    'structure_control':    1.0,\n",
    "\n",
    "    # Per-action rewards\n",
    "    'create_unit':          1.0,\n",
    "    'move':                 0.1,\n",
    "    'damage_scale':         0.2,\n",
    "    'kill':                15.0,\n",
    "    'seize_progress':       1.0,\n",
    "    'capture':             30.0,\n",
    "    'cure':                 5.0,\n",
    "    'heal_scale':           0.5,\n",
    "    'paralyze':             8.0,\n",
    "    'haste':                6.0,\n",
    "    'defence_buff':         5.0,\n",
    "    'attack_buff':          5.0,\n",
    "\n",
    "    # Penalties\n",
    "    'invalid_action':     -10.0,\n",
    "    'turn_penalty':        -2.0,\n",
    "}\n",
    "\n",
    "# PPO hyperparameters\n",
    "PPO_CONFIG = dict(\n",
    "    learning_rate = 3e-4,\n",
    "    n_steps       = 2048,\n",
    "    batch_size    = 64,\n",
    "    n_epochs      = 10,\n",
    "    gamma         = 0.99,\n",
    "    gae_lambda    = 0.95,\n",
    "    clip_range    = 0.2,\n",
    "    ent_coef      = 0.05,\n",
    "    vf_coef       = 0.5,\n",
    "    max_grad_norm = 0.5,\n",
    ")\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = Path('kaggle_demo_output')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Map:          {MAP_FILE}')\n",
    "print(f'Opponent:     {OPPONENT}')\n",
    "print(f'Action space: {ACTION_SPACE}')\n",
    "print(f'Max steps:    {MAX_STEPS}')\n",
    "print(f'Checkpoints:  {CHECKPOINTS}')\n",
    "print(f'Eval eps:     {EVAL_EPISODES}')\n",
    "print(f'Output dir:   {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "## 4. Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-envs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training envs (vectorized, headless)\n",
    "vec_env = make_maskable_vec_env(\n",
    "    n_envs=N_ENVS,\n",
    "    map_file=MAP_FILE,\n",
    "    opponent=OPPONENT,\n",
    "    max_steps=MAX_STEPS,\n",
    "    reward_config=REWARD_CONFIG,\n",
    "    seed=SEED,\n",
    "    use_subprocess=False,   # DummyVecEnv (safer in notebooks)\n",
    "    action_space_type=ACTION_SPACE,\n",
    ")\n",
    "\n",
    "# Separate eval env (single, deterministic)\n",
    "eval_env = make_maskable_env(\n",
    "    map_file=MAP_FILE,\n",
    "    opponent=OPPONENT,\n",
    "    max_steps=MAX_STEPS,\n",
    "    reward_config=REWARD_CONFIG,\n",
    "    action_space_type=ACTION_SPACE,\n",
    ")\n",
    "\n",
    "print(f'Observation space: {vec_env.observation_space}')\n",
    "print(f'Action space:      {vec_env.action_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explore-header",
   "metadata": {},
   "source": [
    "## 5. Explore the environment\n",
    "\n",
    "Before training, let's take a quick look at what the agent sees and\n",
    "what actions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-env",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = eval_env.reset()\n",
    "\n",
    "print('Observation keys and shapes:')\n",
    "for key, val in obs.items():\n",
    "    if hasattr(val, 'shape'):\n",
    "        print(f'  {key:20s}  shape={val.shape}  dtype={val.dtype}')\n",
    "    else:\n",
    "        print(f'  {key:20s}  {val}')\n",
    "\n",
    "masks = eval_env.action_masks()\n",
    "n_legal = int(masks.sum())\n",
    "n_total = len(masks)\n",
    "print(f'\\nAction mask: {n_legal} legal actions out of {n_total} total ({n_legal/n_total*100:.1f}%)')\n",
    "\n",
    "# Show the grid observation as a heatmap\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\n",
    "channel_names = ['Terrain type', 'Owner', 'Structure HP']\n",
    "for i, (ax, name) in enumerate(zip(axes, channel_names)):\n",
    "    im = ax.imshow(obs['grid'][:, :, i], cmap='viridis', interpolation='nearest')\n",
    "    ax.set_title(name)\n",
    "    plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "fig.suptitle('Grid observation channels (initial state)', fontweight='bold')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 6. Create MaskablePPO model\n",
    "\n",
    "We use **MaskablePPO** from `sb3-contrib` which supports action masking\n",
    "natively. The flat-discrete action space maps each legal game action to\n",
    "a unique integer, and the mask tells the policy which integers are valid\n",
    "on each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskablePPO(\n",
    "    'MultiInputPolicy',\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=str(OUTPUT_DIR / 'tensorboard'),\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    **PPO_CONFIG,\n",
    ")\n",
    "\n",
    "print('MaskablePPO model created.')\n",
    "print(f'Policy:  {model.policy.__class__.__name__}')\n",
    "print(f'Device:  {model.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-fn-header",
   "metadata": {},
   "source": [
    "## 7. Evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, n_episodes=30):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model and return summary statistics.\n",
    "\n",
    "    Returns dict with: win_rate, avg_reward, std_reward,\n",
    "    avg_length, std_length, wins, losses, draws\n",
    "    \"\"\"\n",
    "    wins, losses, draws = 0, 0, 0\n",
    "    rewards, lengths = [], []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        while not done:\n",
    "            masks = env.action_masks()\n",
    "            action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            ep_len += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        lengths.append(ep_len)\n",
    "\n",
    "        winner = info.get('winner')\n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "        elif winner is not None:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return {\n",
    "        'win_rate':    wins / n_episodes,\n",
    "        'avg_reward':  float(np.mean(rewards)),\n",
    "        'std_reward':  float(np.std(rewards)),\n",
    "        'avg_length':  float(np.mean(lengths)),\n",
    "        'std_length':  float(np.std(lengths)),\n",
    "        'wins':        wins,\n",
    "        'losses':      losses,\n",
    "        'draws':       draws,\n",
    "    }\n",
    "\n",
    "print('evaluate_model() defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 8. Train and evaluate at each checkpoint\n",
    "\n",
    "We train incrementally: 0 \u2192 10K \u2192 50K \u2192 200K timesteps,\n",
    "evaluating at each checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "trained_so_far = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for checkpoint_ts in CHECKPOINTS:\n",
    "    steps_to_train = checkpoint_ts - trained_so_far\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training {trained_so_far:,} -> {checkpoint_ts:,} '\n",
    "          f'({steps_to_train:,} steps)...')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.learn(\n",
    "        total_timesteps=steps_to_train,\n",
    "        reset_num_timesteps=False,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "    train_time = time.time() - t0\n",
    "    trained_so_far = checkpoint_ts\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt_path = OUTPUT_DIR / f'model_{checkpoint_ts}.zip'\n",
    "    model.save(str(ckpt_path))\n",
    "    print(f'Saved checkpoint: {ckpt_path}')\n",
    "\n",
    "    # Evaluate\n",
    "    print(f'Evaluating over {EVAL_EPISODES} episodes...')\n",
    "    metrics = evaluate_model(model, eval_env, n_episodes=EVAL_EPISODES)\n",
    "    metrics['timesteps'] = checkpoint_ts\n",
    "    metrics['train_time_s'] = round(train_time, 1)\n",
    "    results.append(metrics)\n",
    "\n",
    "    print(f'  Win rate:       {metrics[\"win_rate\"]*100:.1f}%')\n",
    "    print(f'  Avg reward:     {metrics[\"avg_reward\"]:.1f} '\n",
    "          f'(+/- {metrics[\"std_reward\"]:.1f})')\n",
    "    print(f'  Avg length:     {metrics[\"avg_length\"]:.1f} '\n",
    "          f'(+/- {metrics[\"std_length\"]:.1f})')\n",
    "    print(f'  W/L/D:          {metrics[\"wins\"]}/{metrics[\"losses\"]}/{metrics[\"draws\"]}')\n",
    "    print(f'  Training time:  {train_time:.1f}s')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nTotal wall time: {total_time/60:.1f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 9. Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df['win_rate_pct'] = (df['win_rate'] * 100).round(1)\n",
    "df['avg_reward'] = df['avg_reward'].round(1)\n",
    "df['avg_length'] = df['avg_length'].round(1)\n",
    "\n",
    "display_df = df[['timesteps', 'win_rate_pct', 'avg_reward', 'avg_length',\n",
    "                  'wins', 'losses', 'draws', 'train_time_s']].copy()\n",
    "display_df.columns = ['Timesteps', 'Win Rate (%)', 'Avg Reward',\n",
    "                       'Avg Length', 'Wins', 'Losses', 'Draws',\n",
    "                       'Train Time (s)']\n",
    "display_df = display_df.set_index('Timesteps')\n",
    "display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plots-header",
   "metadata": {},
   "source": [
    "## 10. Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ts = [r['timesteps'] for r in results]\n",
    "\n",
    "# Win rate\n",
    "ax = axes[0]\n",
    "wr = [r['win_rate'] * 100 for r in results]\n",
    "ax.plot(ts, wr, 'o-', color='#2196F3', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title('Win Rate vs Opponent')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim(-5, 105)\n",
    "ax.axhline(y=70, color='green', linestyle='--', alpha=0.5, label='70% target')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Average reward\n",
    "ax = axes[1]\n",
    "avg_r = [r['avg_reward'] for r in results]\n",
    "std_r = [r['std_reward'] for r in results]\n",
    "ax.plot(ts, avg_r, 'o-', color='#FF9800', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_r, std_r)],\n",
    "                [a + s for a, s in zip(avg_r, std_r)],\n",
    "                alpha=0.2, color='#FF9800')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Reward')\n",
    "ax.set_title('Average Episode Reward')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode length\n",
    "ax = axes[2]\n",
    "avg_l = [r['avg_length'] for r in results]\n",
    "std_l = [r['std_length'] for r in results]\n",
    "ax.plot(ts, avg_l, 'o-', color='#4CAF50', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_l, std_l)],\n",
    "                [a + s for a, s in zip(avg_l, std_l)],\n",
    "                alpha=0.2, color='#4CAF50')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Length (steps)')\n",
    "ax.set_title('Average Episode Length')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(f'MaskablePPO Training  |  6x6 beginner map  |  vs {OPPONENT}',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(str(OUTPUT_DIR / 'training_curves.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(f'Saved plot: {OUTPUT_DIR / \"training_curves.png\"}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "replay-header",
   "metadata": {},
   "source": [
    "## 11. Watch the agent play\n",
    "\n",
    "Record a few evaluation episodes and inspect exactly what the agent does\n",
    "each step: which actions it takes, how units and gold change over time,\n",
    "and whether it's learning meaningful tactics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "replay-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REPLAY_EPISODES = 3\n",
    "\n",
    "ACTION_NAMES = [\n",
    "    'create_unit', 'move', 'attack', 'seize', 'heal',\n",
    "    'end_turn', 'paralyze', 'haste', 'defence_buff', 'attack_buff',\n",
    "]\n",
    "UNIT_NAMES = ['W', 'M', 'C', 'A', 'K', 'R', 'S', 'B']\n",
    "\n",
    "\n",
    "def _snapshot_game_state(env):\n",
    "    \"\"\"Capture a summary of the current game state.\"\"\"\n",
    "    gs = env.unwrapped.game_state\n",
    "    ap = env.unwrapped.agent_player\n",
    "    opp = 3 - ap\n",
    "    return {\n",
    "        'agent_gold': gs.player_gold.get(ap, 0),\n",
    "        'opponent_gold': gs.player_gold.get(opp, 0),\n",
    "        'agent_units': sum(1 for u in gs.units if u.player == ap),\n",
    "        'opponent_units': sum(1 for u in gs.units if u.player == opp),\n",
    "        'agent_structures': len(gs.grid.get_capturable_tiles(player=ap)),\n",
    "        'opponent_structures': len(gs.grid.get_capturable_tiles(player=opp)),\n",
    "        'turn': gs.turn_number,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_with_replay(model, env, n_episodes=3):\n",
    "    \"\"\"Run evaluation episodes and record every action.\"\"\"\n",
    "    episodes = []\n",
    "\n",
    "    for ep_idx in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        steps = []\n",
    "        step_num = 0\n",
    "\n",
    "        while not done:\n",
    "            masks = env.action_masks()\n",
    "            action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n",
    "\n",
    "            # Decode action before stepping\n",
    "            raw_action = action\n",
    "            if ACTION_SPACE == 'flat_discrete':\n",
    "                action_idx = int(action)\n",
    "                inner = env.unwrapped\n",
    "                if 0 <= action_idx < len(inner._current_actions):\n",
    "                    action_arr = inner._current_actions[action_idx]\n",
    "                else:\n",
    "                    action_arr = np.array([5, 0, 0, 0, 0, 0])\n",
    "            else:\n",
    "                action_arr = np.asarray(action)\n",
    "\n",
    "            action_type = int(action_arr[0])\n",
    "            action_name = ACTION_NAMES[action_type] if action_type < len(ACTION_NAMES) else f'unknown_{action_type}'\n",
    "            unit_type = UNIT_NAMES[int(action_arr[1]) % 8]\n",
    "            from_pos = [int(action_arr[2]), int(action_arr[3])]\n",
    "            to_pos = [int(action_arr[4]), int(action_arr[5])]\n",
    "\n",
    "            obs, reward, terminated, truncated, info = env.step(raw_action)\n",
    "            ep_reward += reward\n",
    "            step_num += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "            step_record = {\n",
    "                'step': step_num,\n",
    "                'action': action_name,\n",
    "                'unit_type': unit_type if action_type == 0 else None,\n",
    "                'from': from_pos,\n",
    "                'to': to_pos,\n",
    "                'reward': round(float(reward), 3),\n",
    "                'cumulative_reward': round(float(ep_reward), 3),\n",
    "                'valid': info.get('valid_action', True),\n",
    "                'game_state': _snapshot_game_state(env),\n",
    "            }\n",
    "            steps.append(step_record)\n",
    "\n",
    "        winner = info.get('winner')\n",
    "        if winner == 1:\n",
    "            outcome = 'win'\n",
    "        elif winner is not None:\n",
    "            outcome = 'loss'\n",
    "        else:\n",
    "            outcome = 'draw'\n",
    "\n",
    "        final_turn = steps[-1]['game_state']['turn'] if steps else 0\n",
    "        episodes.append({\n",
    "            'episode': ep_idx,\n",
    "            'outcome': outcome,\n",
    "            'total_reward': round(float(ep_reward), 2),\n",
    "            'length': step_num,\n",
    "            'turns': final_turn,\n",
    "            'steps': steps,\n",
    "        })\n",
    "\n",
    "    return episodes\n",
    "\n",
    "print('Replay helpers defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-replays",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Recording {N_REPLAY_EPISODES} replay episodes from final model...\\n')\n",
    "replay_episodes = evaluate_with_replay(model, eval_env, n_episodes=N_REPLAY_EPISODES)\n",
    "\n",
    "for ep in replay_episodes:\n",
    "    final_turn = ep['steps'][-1]['game_state']['turn'] if ep['steps'] else 0\n",
    "\n",
    "    print(f'\\n{\"\\u2500\" * 55}')\n",
    "    print(f'Episode {ep[\"episode\"]}  |  outcome={ep[\"outcome\"]}  |  '\n",
    "          f'length={ep[\"length\"]}  |  turns={final_turn}  |  '\n",
    "          f'reward={ep[\"total_reward\"]}')\n",
    "    print(f'{\"\\u2500\" * 55}')\n",
    "\n",
    "    # Action distribution\n",
    "    action_counts = Counter(s['action'] for s in ep['steps'])\n",
    "    total = len(ep['steps'])\n",
    "    print(f'\\n  Action distribution ({total} steps):')\n",
    "    for action, count in action_counts.most_common():\n",
    "        pct = count / total * 100\n",
    "        bar = '#' * int(pct / 2)\n",
    "        print(f'    {action:15s}  {count:4d}  ({pct:5.1f}%)  {bar}')\n",
    "\n",
    "    # Unit creation breakdown\n",
    "    create_steps = [s for s in ep['steps'] if s['action'] == 'create_unit']\n",
    "    if create_steps:\n",
    "        unit_counts = Counter(s['unit_type'] for s in create_steps)\n",
    "        print(f'\\n  Units created ({len(create_steps)} total):')\n",
    "        for ut, count in unit_counts.most_common():\n",
    "            bar = '#' * count\n",
    "            print(f'    {ut:3s}  {count:3d}  {bar}')\n",
    "\n",
    "    # Final state\n",
    "    final = ep['steps'][-1]['game_state']\n",
    "    print(f'\\n  Final state (step {ep[\"length\"]}, turn {final[\"turn\"]}):')\n",
    "    print(f'    Agent:    {final[\"agent_units\"]} units, '\n",
    "          f'{final[\"agent_structures\"]} structures, '\n",
    "          f'{final[\"agent_gold\"]} gold')\n",
    "    print(f'    Opponent: {final[\"opponent_units\"]} units, '\n",
    "          f'{final[\"opponent_structures\"]} structures, '\n",
    "          f'{final[\"opponent_gold\"]} gold')\n",
    "\n",
    "    # First 5 moves\n",
    "    steps = ep['steps']\n",
    "    print(f'\\n  First 5 moves:')\n",
    "    for s in steps[:5]:\n",
    "        gs = s['game_state']\n",
    "        ut = f' ({s[\"unit_type\"]})' if s['unit_type'] else ''\n",
    "        print(f'    step {s[\"step\"]:3d}  {s[\"action\"]:15s}{ut:5s}  '\n",
    "              f'{s[\"from\"]}\\u2192{s[\"to\"]}  r={s[\"reward\"]:+.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 12. Replay visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour palette\n",
    "ACTION_COLORS = {\n",
    "    'move':         '#4CAF50',\n",
    "    'attack':       '#F44336',\n",
    "    'end_turn':     '#9E9E9E',\n",
    "    'create_unit':  '#2196F3',\n",
    "    'seize':        '#FF9800',\n",
    "    'heal':         '#E91E63',\n",
    "    'paralyze':     '#9C27B0',\n",
    "    'haste':        '#00BCD4',\n",
    "    'defence_buff': '#795548',\n",
    "    'attack_buff':  '#FF5722',\n",
    "}\n",
    "AGENT_COLOR = '#2196F3'\n",
    "OPP_COLOR   = '#F44336'\n",
    "\n",
    "ORDERED_ACTIONS = [\n",
    "    'move', 'attack', 'create_unit', 'end_turn',\n",
    "    'seize', 'heal', 'paralyze', 'haste', 'defence_buff', 'attack_buff',\n",
    "]\n",
    "\n",
    "\n",
    "def _per_turn(steps, key):\n",
    "    \"\"\"Aggregate a game-state key per turn (value at end of turn).\"\"\"\n",
    "    turn_vals = {}\n",
    "    for s in steps:\n",
    "        t = s['game_state']['turn']\n",
    "        turn_vals[t] = s['game_state'][key]\n",
    "    turns = sorted(turn_vals)\n",
    "    return turns, [turn_vals[t] for t in turns]\n",
    "\n",
    "\n",
    "def plot_episode(ep, ep_idx=None):\n",
    "    \"\"\"Create a 2x3 dashboard for a single replay episode.\"\"\"\n",
    "    steps = ep['steps']\n",
    "    idx = ep_idx if ep_idx is not None else ep.get('episode', '?')\n",
    "    outcome = ep['outcome'].upper()\n",
    "    total_reward = ep['total_reward']\n",
    "    length = ep['length']\n",
    "    final_turn = steps[-1]['game_state']['turn'] if steps else 0\n",
    "\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    gs = gridspec.GridSpec(2, 3, hspace=0.35, wspace=0.35)\n",
    "\n",
    "    outcome_color = {'WIN': '#4CAF50', 'LOSS': '#F44336', 'DRAW': '#FF9800'}\n",
    "    fig.suptitle(\n",
    "        f'Episode {idx}  \\u2014  {outcome}  |  '\n",
    "        f'{length} steps  |  {final_turn} turns  |  reward {total_reward:+.0f}',\n",
    "        fontsize=14, fontweight='bold',\n",
    "        color=outcome_color.get(outcome, 'black'),\n",
    "    )\n",
    "\n",
    "    # 1. Action distribution\n",
    "    ax = fig.add_subplot(gs[0, 0])\n",
    "    action_counts = Counter(s['action'] for s in steps)\n",
    "    actions = [a for a in ORDERED_ACTIONS if action_counts.get(a, 0) > 0]\n",
    "    counts = [action_counts[a] for a in actions]\n",
    "    colors = [ACTION_COLORS.get(a, '#607D8B') for a in actions]\n",
    "    bars = ax.barh(actions, counts, color=colors, edgecolor='white', linewidth=0.5)\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_title('Action Distribution')\n",
    "    ax.invert_yaxis()\n",
    "    for bar, c in zip(bars, counts):\n",
    "        pct = c / len(steps) * 100\n",
    "        ax.text(bar.get_width() + max(counts) * 0.02, bar.get_y() + bar.get_height() / 2,\n",
    "                f'{pct:.0f}%', va='center', fontsize=9, color='#555')\n",
    "\n",
    "    # 2. Unit count over turns\n",
    "    ax = fig.add_subplot(gs[0, 1])\n",
    "    turns_a, agent_units = _per_turn(steps, 'agent_units')\n",
    "    _, opp_units = _per_turn(steps, 'opponent_units')\n",
    "    ax.plot(turns_a, agent_units, color=AGENT_COLOR, linewidth=1.5, label='Agent')\n",
    "    ax.plot(turns_a, opp_units, color=OPP_COLOR, linewidth=1.5, label='Opponent')\n",
    "    ax.fill_between(turns_a, agent_units, alpha=0.1, color=AGENT_COLOR)\n",
    "    ax.fill_between(turns_a, opp_units, alpha=0.1, color=OPP_COLOR)\n",
    "    ax.set_xlabel('Turn')\n",
    "    ax.set_ylabel('Units')\n",
    "    ax.set_title('Army Size')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "    # 3. Gold over turns\n",
    "    ax = fig.add_subplot(gs[0, 2])\n",
    "    _, agent_gold = _per_turn(steps, 'agent_gold')\n",
    "    _, opp_gold = _per_turn(steps, 'opponent_gold')\n",
    "    ax.plot(turns_a, agent_gold, color=AGENT_COLOR, linewidth=1.5, label='Agent')\n",
    "    ax.plot(turns_a, opp_gold, color=OPP_COLOR, linewidth=1.5, label='Opponent')\n",
    "    ax.set_xlabel('Turn')\n",
    "    ax.set_ylabel('Gold')\n",
    "    ax.set_title('Economy')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "    # 4. Cumulative reward\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    cum_r = [s['cumulative_reward'] for s in steps]\n",
    "    step_nums = [s['step'] for s in steps]\n",
    "    ax.plot(step_nums, cum_r, color='#FF9800', linewidth=1.2)\n",
    "    ax.axhline(0, color='grey', linewidth=0.5, linestyle='--')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Cumulative Reward')\n",
    "    ax.set_title('Reward Curve')\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "    # 5. Steps per turn\n",
    "    ax = fig.add_subplot(gs[1, 1])\n",
    "    turn_steps = {}\n",
    "    for s in steps:\n",
    "        t = s['game_state']['turn']\n",
    "        turn_steps[t] = turn_steps.get(t, 0) + 1\n",
    "    ts_sorted = sorted(turn_steps)\n",
    "    spt = [turn_steps[t] for t in ts_sorted]\n",
    "    bar_colors = ['#F44336' if v == 1 else '#4CAF50' for v in spt]\n",
    "    ax.bar(ts_sorted, spt, color=bar_colors, width=1.0, edgecolor='white', linewidth=0.3)\n",
    "    avg_spt = np.mean(spt)\n",
    "    ax.axhline(avg_spt, color='#2196F3', linewidth=1, linestyle='--',\n",
    "               label=f'avg {avg_spt:.1f}')\n",
    "    ax.set_xlabel('Turn')\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_title('Steps per Turn')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "    # 6. Unit creation breakdown\n",
    "    ax = fig.add_subplot(gs[1, 2])\n",
    "    create_steps = [s for s in steps if s['action'] == 'create_unit']\n",
    "    if create_steps:\n",
    "        unit_counts = Counter(s['unit_type'] for s in create_steps)\n",
    "        labels = list(unit_counts.keys())\n",
    "        sizes = list(unit_counts.values())\n",
    "        wedge_colors = plt.cm.Set2(np.linspace(0, 1, len(labels)))\n",
    "        ax.pie(sizes, labels=labels, autopct='%1.0f%%',\n",
    "               colors=wedge_colors, startangle=90,\n",
    "               textprops={'fontsize': 10})\n",
    "        ax.set_title(f'Units Created ({sum(sizes)})')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No units\\ncreated', ha='center', va='center',\n",
    "                fontsize=14, color='#999')\n",
    "        ax.set_title('Units Created')\n",
    "\n",
    "    plot_path = OUTPUT_DIR / f'replay_episode_{idx}.png'\n",
    "    fig.savefig(str(plot_path), dpi=150, bbox_inches='tight')\n",
    "    print(f'Saved: {plot_path}')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print('Visualisation helpers defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-replays",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in replay_episodes:\n",
    "    plot_episode(ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 13. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results as JSON\n",
    "benchmark_data = {\n",
    "    'metadata': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'map': MAP_FILE,\n",
    "        'opponent': OPPONENT,\n",
    "        'max_steps': MAX_STEPS,\n",
    "        'n_envs': N_ENVS,\n",
    "        'eval_episodes': EVAL_EPISODES,\n",
    "        'seed': SEED,\n",
    "        'device': DEVICE,\n",
    "        'ppo_config': PPO_CONFIG,\n",
    "        'reward_config': REWARD_CONFIG,\n",
    "    },\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "results_path = OUTPUT_DIR / 'benchmark_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(benchmark_data, f, indent=2)\n",
    "print(f'Saved results:  {results_path}')\n",
    "\n",
    "csv_path = OUTPUT_DIR / 'benchmark_results.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f'Saved CSV:      {csv_path}')\n",
    "\n",
    "# List all saved files\n",
    "print(f'\\nAll output files:')\n",
    "for p in sorted(OUTPUT_DIR.iterdir()):\n",
    "    if p.is_file():\n",
    "        size = p.stat().st_size\n",
    "        if size > 1024 * 1024:\n",
    "            size_str = f'{size / 1024 / 1024:.1f} MB'\n",
    "        elif size > 1024:\n",
    "            size_str = f'{size / 1024:.1f} KB'\n",
    "        else:\n",
    "            size_str = f'{size} B'\n",
    "        print(f'  {p.name:40s}  {size_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-header",
   "metadata": {},
   "source": [
    "## 14. What's next?\n",
    "\n",
    "This demo trained a basic agent on the smallest map against a random\n",
    "opponent. Here are ways to go further:\n",
    "\n",
    "### Train longer / harder\n",
    "\n",
    "| Change | How |\n",
    "|--------|-----|\n",
    "| More timesteps | Add `1_000_000` and `2_000_000` to `CHECKPOINTS` |\n",
    "| Harder opponent | Set `OPPONENT = 'bot'` (SimpleBot) |\n",
    "| Bigger map | Change `MAP_FILE` to a 10\u00d710 or 14\u00d714 map |\n",
    "\n",
    "### Other algorithms\n",
    "\n",
    "The repo includes:\n",
    "- **Self-play** (`train/train_self_play.py`) \u2014 train against past versions of itself\n",
    "- **AlphaZero** (`train/train_alphazero.py`) \u2014 MCTS + neural network\n",
    "- **Feudal RL** (`train/train_feudal_rl.py`) \u2014 hierarchical manager-worker\n",
    "\n",
    "### Run a tournament\n",
    "\n",
    "See `notebooks/bot_tournament.ipynb` to pit SimpleBot, MediumBot,\n",
    "AdvancedBot, and your trained agent against each other with Elo ratings.\n",
    "\n",
    "### Repository\n",
    "\n",
    "[github.com/kuds/reinforce-tactics](https://github.com/kuds/reinforce-tactics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up environments\n",
    "vec_env.close()\n",
    "eval_env.close()\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
