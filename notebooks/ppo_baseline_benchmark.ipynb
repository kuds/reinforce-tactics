{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/ppo_baseline_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Reinforce Tactics — PPO Baseline Training Benchmarks\n",
    "\n",
    "This notebook trains a **MaskablePPO** agent against `SimpleBot` on the 6×6 beginner map\n",
    "and records reference metrics at four training checkpoints:\n",
    "\n",
    "| Checkpoint | Timesteps |\n",
    "|------------|-----------|\n",
    "| 1 | 10,000 |\n",
    "| 2 | 50,000 |\n",
    "| 3 | 200,000 |\n",
    "| 4 | 1,000,000 |\n",
    "\n",
    "At each checkpoint the agent is evaluated over **50 episodes** and we record:\n",
    "- **Win rate** (% of games won against SimpleBot)\n",
    "- **Average episode reward**\n",
    "- **Average episode length** (steps)\n",
    "\n",
    "The goal is to provide a **reference curve** so that users can run the same\n",
    "notebook and compare their results to known-good training runs.\n",
    "\n",
    "**Runtime:** CPU is fine (~20–40 min total). GPU will be faster.\n",
    "\n",
    "---\n",
    "\n",
    "### Why MaskablePPO?\n",
    "\n",
    "The game has a `MultiDiscrete` action space where many action combinations\n",
    "are invalid at any given time (e.g. you can’t attack a tile with no enemy).\n",
    "**Action masking** prevents the agent from sampling these invalid actions,\n",
    "which typically yields 2–3× faster convergence compared to plain PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q gymnasium stable-baselines3 sb3-contrib tensorboard pandas numpy torch matplotlib\n\nimport torch\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {DEVICE}\")\nif DEVICE == 'cuda':\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo and install as a package\nimport os, sys\nfrom pathlib import Path\n\nREPO_DIR = Path('reinforce-tactics')\nif REPO_DIR.exists():\n    os.chdir(REPO_DIR)\nelif Path('notebooks').exists():\n    # Already inside the repo\n    os.chdir('..')\nelse:\n    print('Cloning repository...')\n    !git clone https://github.com/kuds/reinforce-tactics.git\n    os.chdir(REPO_DIR)\n\n# Install the package so all imports resolve\n!pip install -q -e .\n\nif os.getcwd() not in sys.path:\n    sys.path.insert(0, os.getcwd())\n\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport time\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sb3_contrib import MaskablePPO\nfrom stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n\nfrom reinforcetactics.rl.masking import make_maskable_env, make_maskable_vec_env\nfrom reinforcetactics.core.game_state import GameState\n\nprint('All imports successful.')"
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# --- Benchmark settings ---\nMAP_FILE        = 'maps/1v1/beginner.csv'   # 6x6 beginner map\nOPPONENT        = 'bot'                      # SimpleBot\nMAX_STEPS       = 500                        # max steps per episode\nN_ENVS          = 4                          # parallel training envs\nSEED            = 42\n\n# Action space mode:\n#   'flat_discrete'  — exact per-action masks (recommended, eliminates invalid actions)\n#   'multi_discrete' — per-dimension masks (over-approximation, original behaviour)\nACTION_SPACE    = 'flat_discrete'\n\n# Checkpoints to evaluate\nCHECKPOINTS     = [10_000, 50_000, 200_000, 1_000_000]\nEVAL_EPISODES   = 50                         # episodes per evaluation\n\n# PPO hyperparameters\nPPO_CONFIG = dict(\n    learning_rate = 3e-4,\n    n_steps       = 2048,\n    batch_size    = 64,\n    n_epochs      = 10,\n    gamma         = 0.99,\n    gae_lambda    = 0.95,\n    clip_range    = 0.2,\n    ent_coef      = 0.01,\n    vf_coef       = 0.5,\n    max_grad_norm = 0.5,\n)\n\n# Replay settings — save evaluation game replays for later viewing\nSAVE_REPLAYS           = True                # save evaluation game replays\nREPLAYS_PER_CHECKPOINT = 5                   # number of replays to save per checkpoint\n\n# Output paths\nBENCHMARK_DIR = Path('benchmarks/ppo_vs_simplebot')\nBENCHMARK_DIR.mkdir(parents=True, exist_ok=True)\n\nREPLAY_DIR = BENCHMARK_DIR / 'replays'\nREPLAY_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f'Map:          {MAP_FILE}')\nprint(f'Opponent:     {OPPONENT}')\nprint(f'Action space: {ACTION_SPACE}')\nprint(f'Checkpoints:  {CHECKPOINTS}')\nprint(f'Eval eps:     {EVAL_EPISODES}')\nprint(f'Save replays: {SAVE_REPLAYS} ({REPLAYS_PER_CHECKPOINT} per checkpoint)')\nprint(f'Output dir:   {BENCHMARK_DIR}')"
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "## 4. Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-envs",
   "metadata": {},
   "outputs": [],
   "source": "# Training envs (vectorized, headless)\nvec_env = make_maskable_vec_env(\n    n_envs=N_ENVS,\n    map_file=MAP_FILE,\n    opponent=OPPONENT,\n    max_steps=MAX_STEPS,\n    seed=SEED,\n    use_subprocess=False,   # DummyVecEnv (safer in notebooks)\n    action_space_type=ACTION_SPACE,\n)\n\n# Separate eval env (single, deterministic)\neval_env = make_maskable_env(\n    map_file=MAP_FILE,\n    opponent=OPPONENT,\n    max_steps=MAX_STEPS,\n    action_space_type=ACTION_SPACE,\n)\n\nprint(f'Observation space: {vec_env.observation_space}')\nprint(f'Action space:      {vec_env.action_space}')"
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 5. Create MaskablePPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskablePPO(\n",
    "    'MultiInputPolicy',\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=str(BENCHMARK_DIR / 'tensorboard'),\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    **PPO_CONFIG,\n",
    ")\n",
    "\n",
    "print('MaskablePPO model created.')\n",
    "print(f'Policy:  {model.policy.__class__.__name__}')\n",
    "print(f'Device:  {model.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-fn-header",
   "metadata": {},
   "source": [
    "## 6. Evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-fn",
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_model(model, env, n_episodes=50, save_replays=False,\n                   replay_dir=None, checkpoint_name=None, max_replays=5):\n    \"\"\"\n    Evaluate a trained model and return summary statistics.\n    Optionally save game replays to JSON files for later viewing.\n\n    Returns dict with: win_rate, avg_reward, std_reward,\n    avg_length, std_length, wins, losses, draws, replays_saved\n    \"\"\"\n    wins, losses, draws = 0, 0, 0\n    rewards, lengths = [], []\n    replays_saved = 0\n\n    for ep in range(n_episodes):\n        obs, _ = env.reset()\n\n        # Set up game state metadata for replay saving\n        if save_replays and replays_saved < max_replays:\n            gs = env.game_state\n            gs.map_file_used = MAP_FILE\n            gs.player_configs = [\n                GameState.build_player_config(1, f'PPO-{checkpoint_name}', 'rl'),\n                GameState.build_player_config(2, 'SimpleBot', 'bot'),\n            ]\n\n        done = False\n        ep_reward = 0.0\n        ep_len = 0\n\n        while not done:\n            masks = env.action_masks()\n            action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n            obs, reward, terminated, truncated, info = env.step(action)\n            ep_reward += reward\n            ep_len += 1\n            done = terminated or truncated\n\n        rewards.append(ep_reward)\n        lengths.append(ep_len)\n\n        winner = info.get('winner')\n        if winner == 1:\n            wins += 1\n        elif winner is not None:\n            losses += 1\n        else:\n            draws += 1\n\n        # Save replay if enabled\n        if save_replays and replays_saved < max_replays:\n            gs = env.game_state\n            outcome = 'win' if winner == 1 else ('loss' if winner is not None else 'draw')\n            replay_path = replay_dir / f'{checkpoint_name}_ep{ep}_{outcome}.json'\n            gs.save_replay_to_file(str(replay_path))\n            replays_saved += 1\n\n    return {\n        'win_rate':    wins / n_episodes,\n        'avg_reward':  float(np.mean(rewards)),\n        'std_reward':  float(np.std(rewards)),\n        'avg_length':  float(np.mean(lengths)),\n        'std_length':  float(np.std(lengths)),\n        'wins':        wins,\n        'losses':      losses,\n        'draws':       draws,\n        'replays_saved': replays_saved,\n    }\n\nprint('evaluate_model() defined.')"
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 7. Train and evaluate at each checkpoint\n",
    "\n",
    "We train incrementally: 0 → 10K → 50K → 200K → 1M timesteps,\n",
    "evaluating at each checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": "results = []\ntrained_so_far = 0\nstart_time = time.time()\n\nfor checkpoint_ts in CHECKPOINTS:\n    steps_to_train = checkpoint_ts - trained_so_far\n    print(f'\\n{\"=\"*60}')\n    print(f'Training {trained_so_far:,} -> {checkpoint_ts:,} '\n          f'({steps_to_train:,} steps)...')\n    print(f'{\"=\"*60}')\n\n    t0 = time.time()\n    model.learn(\n        total_timesteps=steps_to_train,\n        reset_num_timesteps=False,\n        progress_bar=True,\n    )\n    train_time = time.time() - t0\n    trained_so_far = checkpoint_ts\n\n    # Save checkpoint\n    ckpt_path = BENCHMARK_DIR / f'model_{checkpoint_ts}.zip'\n    model.save(str(ckpt_path))\n    print(f'Saved checkpoint: {ckpt_path}')\n\n    # Evaluate\n    print(f'Evaluating over {EVAL_EPISODES} episodes...')\n    metrics = evaluate_model(\n        model, eval_env, n_episodes=EVAL_EPISODES,\n        save_replays=SAVE_REPLAYS,\n        replay_dir=REPLAY_DIR,\n        checkpoint_name=f'{checkpoint_ts}',\n        max_replays=REPLAYS_PER_CHECKPOINT,\n    )\n    metrics['timesteps'] = checkpoint_ts\n    metrics['train_time_s'] = round(train_time, 1)\n    results.append(metrics)\n\n    print(f'  Win rate:       {metrics[\"win_rate\"]*100:.1f}%')\n    print(f'  Avg reward:     {metrics[\"avg_reward\"]:.1f} '\n          f'(+/- {metrics[\"std_reward\"]:.1f})')\n    print(f'  Avg length:     {metrics[\"avg_length\"]:.1f} '\n          f'(+/- {metrics[\"std_length\"]:.1f})')\n    print(f'  W/L/D:          {metrics[\"wins\"]}/{metrics[\"losses\"]}/{metrics[\"draws\"]}')\n    print(f'  Training time:  {train_time:.1f}s')\n    if SAVE_REPLAYS:\n        print(f'  Replays saved:  {metrics[\"replays_saved\"]}')\n\ntotal_time = time.time() - start_time\nprint(f'\\nTotal wall time: {total_time/60:.1f} minutes')"
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 8. Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df['win_rate_pct'] = (df['win_rate'] * 100).round(1)\n",
    "df['avg_reward'] = df['avg_reward'].round(1)\n",
    "df['avg_length'] = df['avg_length'].round(1)\n",
    "\n",
    "display_df = df[['timesteps', 'win_rate_pct', 'avg_reward', 'avg_length',\n",
    "                  'wins', 'losses', 'draws', 'train_time_s']].copy()\n",
    "display_df.columns = ['Timesteps', 'Win Rate (%)', 'Avg Reward',\n",
    "                       'Avg Length', 'Wins', 'Losses', 'Draws',\n",
    "                       'Train Time (s)']\n",
    "display_df = display_df.set_index('Timesteps')\n",
    "display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plots-header",
   "metadata": {},
   "source": [
    "## 9. Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ts = [r['timesteps'] for r in results]\n",
    "\n",
    "# Win rate\n",
    "ax = axes[0]\n",
    "wr = [r['win_rate'] * 100 for r in results]\n",
    "ax.plot(ts, wr, 'o-', color='#2196F3', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title('Win Rate vs SimpleBot')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim(-5, 105)\n",
    "ax.axhline(y=70, color='green', linestyle='--', alpha=0.5, label='70% target')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Average reward\n",
    "ax = axes[1]\n",
    "avg_r = [r['avg_reward'] for r in results]\n",
    "std_r = [r['std_reward'] for r in results]\n",
    "ax.plot(ts, avg_r, 'o-', color='#FF9800', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_r, std_r)],\n",
    "                [a + s for a, s in zip(avg_r, std_r)],\n",
    "                alpha=0.2, color='#FF9800')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Reward')\n",
    "ax.set_title('Average Episode Reward')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode length\n",
    "ax = axes[2]\n",
    "avg_l = [r['avg_length'] for r in results]\n",
    "std_l = [r['std_length'] for r in results]\n",
    "ax.plot(ts, avg_l, 'o-', color='#4CAF50', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_l, std_l)],\n",
    "                [a + s for a, s in zip(avg_l, std_l)],\n",
    "                alpha=0.2, color='#4CAF50')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Length (steps)')\n",
    "ax.set_title('Average Episode Length')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('PPO Baseline Benchmarks  |  6x6 beginner map  |  vs SimpleBot',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(str(BENCHMARK_DIR / 'training_curves.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(f'Saved plot: {BENCHMARK_DIR / \"training_curves.png\"}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 10. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": "# Save benchmark results as JSON\nbenchmark_data = {\n    'metadata': {\n        'date': datetime.now().isoformat(),\n        'map': MAP_FILE,\n        'opponent': OPPONENT,\n        'max_steps': MAX_STEPS,\n        'n_envs': N_ENVS,\n        'eval_episodes': EVAL_EPISODES,\n        'seed': SEED,\n        'device': DEVICE,\n        'ppo_config': PPO_CONFIG,\n    },\n    'results': results,\n}\n\nresults_path = BENCHMARK_DIR / 'benchmark_results.json'\nwith open(results_path, 'w') as f:\n    json.dump(benchmark_data, f, indent=2)\n\nprint(f'Saved results:  {results_path}')\n\n# Also save as CSV for easy viewing\ncsv_path = BENCHMARK_DIR / 'benchmark_results.csv'\ndf.to_csv(csv_path, index=False)\nprint(f'Saved CSV:      {csv_path}')\n\n# List all saved files\nprint(f'\\nAll benchmark files:')\nfor p in sorted(BENCHMARK_DIR.iterdir()):\n    if p.is_dir():\n        # List files inside subdirectories (e.g. replays/)\n        sub_files = sorted(p.iterdir())\n        print(f'  {p.name}/ ({len(sub_files)} files)')\n        for sp in sub_files[:5]:\n            size = sp.stat().st_size\n            if size > 1024 * 1024:\n                size_str = f'{size / 1024 / 1024:.1f} MB'\n            elif size > 1024:\n                size_str = f'{size / 1024:.1f} KB'\n            else:\n                size_str = f'{size} B'\n            print(f'    {sp.name:38s}  {size_str}')\n        if len(sub_files) > 5:\n            print(f'    ... and {len(sub_files) - 5} more')\n    else:\n        size = p.stat().st_size\n        if size > 1024 * 1024:\n            size_str = f'{size / 1024 / 1024:.1f} MB'\n        elif size > 1024:\n            size_str = f'{size / 1024:.1f} KB'\n        else:\n            size_str = f'{size} B'\n        print(f'  {p.name:40s}  {size_str}')"
  },
  {
   "cell_type": "markdown",
   "id": "m838y5ig54",
   "source": "## 11. Viewing saved replays\n\nReplays are saved as JSON files in `benchmarks/ppo_vs_simplebot/replays/`.\nEach file captures every action taken during an evaluation game, allowing\nyou to watch the PPO agent play at different training stages.\n\n**To view a replay in the game UI:**\n```bash\npython main.py --replay benchmarks/ppo_vs_simplebot/replays/10000_ep0_win.json\n```\n\n**Replay filenames** follow the pattern `{timesteps}_ep{episode}_{outcome}.json`\nwhere outcome is `win`, `loss`, or `draw`.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "p8oeytt5z1",
   "source": "# List saved replays by checkpoint\nif SAVE_REPLAYS and REPLAY_DIR.exists():\n    replay_files = sorted(REPLAY_DIR.glob('*.json'))\n    print(f'Total replays saved: {len(replay_files)}')\n    print()\n    for ckpt in CHECKPOINTS:\n        ckpt_replays = [f for f in replay_files if f.name.startswith(f'{ckpt}_')]\n        if ckpt_replays:\n            outcomes = [f.stem.rsplit('_', 1)[-1] for f in ckpt_replays]\n            print(f'  {ckpt:>10,} timesteps: {len(ckpt_replays)} replays '\n                  f'({outcomes.count(\"win\")}W / {outcomes.count(\"loss\")}L / {outcomes.count(\"draw\")}D)')\n            for f in ckpt_replays:\n                print(f'    {f.name}')\nelse:\n    print('No replays saved (SAVE_REPLAYS is False).')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tensorboard-header",
   "metadata": {},
   "source": "## 12. TensorBoard (optional)\n\nLaunch TensorBoard to inspect detailed training metrics (loss, entropy, etc.)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to launch TensorBoard inline:\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir benchmarks/ppo_vs_simplebot/tensorboard\n",
    "\n",
    "print('To view TensorBoard locally, run:')\n",
    "print(f'  tensorboard --logdir {BENCHMARK_DIR / \"tensorboard\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpret-header",
   "metadata": {},
   "source": "## 13. Interpreting the results\n\n### What to expect\n\n| Timesteps | Expected Win Rate | Notes |\n|-----------|-------------------|-------|\n| 10K | 0-15% | Agent is mostly random, learning basic actions |\n| 50K | 15-40% | Agent starts making meaningful moves |\n| 200K | 40-70% | Competent play, learns unit creation and combat |\n| 1M | 60-90%+ | Strong play against SimpleBot |\n\n**Note:** Exact numbers depend on hardware and random seed. The important\nthing is that your curve has a similar *shape* -- monotonically increasing\nwin rate with diminishing returns after ~200K steps.\n\n### If your results differ significantly\n\n- **Much worse:** Check that action masking is working (the agent should\n  rarely attempt invalid actions). Verify the map file path is correct.\n- **Much better:** You may have found better hyperparameters! Consider\n  contributing them back.\n- **Unstable (oscillating win rate):** Try reducing the learning rate\n  or increasing the batch size.\n\n### Viewing replays\n\nUse the saved replay files to visually compare how the agent plays at\neach checkpoint. Early replays (10K) will show mostly random behaviour,\nwhile later replays (1M) should demonstrate strategic play such as\ncreating units, capturing structures, and coordinated attacks.\n\n### Next steps\n\n1. **Try different maps:** Larger maps (10x10, 14x14) are harder\n2. **Tune hyperparameters:** Adjust `ent_coef`, `learning_rate`, etc.\n3. **Self-play training:** See `train/train_self_play.py`\n4. **AlphaZero:** See `train/train_alphazero.py` for MCTS-based training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up environments\n",
    "vec_env.close()\n",
    "eval_env.close()\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}