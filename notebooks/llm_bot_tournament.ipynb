{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/llm_bot_tournament.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# \ud83e\udd16 LLM Bot Tournament - Reinforce Tactics\n",
    "\n",
    "Run interactive tournaments between LLM-powered bots (OpenAI GPT, Claude, Gemini) and the SimpleBot!\n",
    "\n",
    "**Features:**\n",
    "- \ud83c\udfae Single game runner with detailed turn-by-turn logging\n",
    "- \ud83c\udfc6 Round-robin tournament system with multiple games per matchup\n",
    "- \ud83d\udd11 Flexible API key configuration (environment variables or Google Colab secrets)\n",
    "- \ud83d\udcca Comprehensive statistics: wins, losses, draws, win rates\n",
    "- \ud83c\udfaf Customizable model selection (GPT-4o, Claude Sonnet, Gemini Pro, etc.)\n",
    "- \ud83d\uddfa\ufe0f Support for all map sizes (6x6 beginner to 32x32 expert)\n",
    "- \ud83d\udcdd Conversation logging to debug and analyze LLM reasoning\n",
    "\n",
    "**Supported Bots:**\n",
    "- **SimpleBot**: Built-in rule-based bot (always available)\n",
    "- **OpenAIBot**: Uses OpenAI GPT models (requires `OPENAI_API_KEY`)\n",
    "- **ClaudeBot**: Uses Anthropic Claude models (requires `ANTHROPIC_API_KEY`)\n",
    "- **GeminiBot**: Uses Google Gemini models (requires `GOOGLE_API_KEY`)\n",
    "\n",
    "**Quick Start:**\n",
    "1. Install dependencies and clone the repository\n",
    "2. Configure API keys for the bots you want to use\n",
    "3. Run a single game or full tournament\n",
    "4. Analyze the results!\n",
    "\n",
    "**Estimated API Costs:**\n",
    "- **GPT-4o-mini**: ~$0.0001-0.0005 per game (recommended for testing)\n",
    "- **Claude Haiku**: ~$0.0001-0.0003 per game (recommended for testing)\n",
    "- **Gemini Flash**: Free tier available, ~$0.0001 per game\n",
    "- **GPT-4o**: ~$0.005-0.02 per game (stronger play, higher cost)\n",
    "- **Claude Sonnet**: ~$0.003-0.015 per game (stronger play, higher cost)\n",
    "\n",
    "*Costs vary based on game length and map complexity. Use mini/haiku/flash models for testing!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## \ud83d\udce6 Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "# Clone the Reinforce Tactics repository if not already present\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path('reinforce-tactics').exists():\n",
    "    print(\"\ud83d\udce5 Cloning Reinforce Tactics repository...\")\n",
    "    !git clone https://github.com/kuds/reinforce-tactics.git\n",
    "    print(\"\u2705 Repository cloned!\")\n",
    "else:\n",
    "    print(\"\u2705 Repository already cloned\")\n",
    "\n",
    "# Change to repository directory\n",
    "os.chdir('reinforce-tactics')\n",
    "print(f\"\\n\ud83d\udcc2 Current directory: {os.getcwd()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install_llm_deps"
   },
   "source": [
    "# Install LLM bot dependencies\n",
    "# These are optional - only install for the bots you plan to use\n",
    "print(\"\ud83d\udce6 Installing LLM dependencies...\\n\")\n",
    "\n",
    "# Install OpenAI (for GPT models)\n",
    "print(\"Installing OpenAI...\")\n",
    "!pip install -q openai>=1.0.0\n",
    "\n",
    "# Install Anthropic (for Claude models)\n",
    "print(\"Installing Anthropic...\")\n",
    "!pip install -q anthropic>=0.18.0\n",
    "\n",
    "# Install Google Generative AI (for Gemini models)\n",
    "print(\"Installing Google Generative AI...\")\n",
    "!pip install -q google-generativeai>=0.4.0\n",
    "\n",
    "# Install other dependencies if needed\n",
    "!pip install -q pandas numpy\n",
    "\n",
    "print(\"\\n\u2705 All LLM dependencies installed!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "add_to_path"
   },
   "source": [
    "# Add repository to Python path\n",
    "import sys\n",
    "repo_path = os.getcwd()\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "    print(f\"\u2705 Added to Python path: {repo_path}\")\n",
    "else:\n",
    "    print(f\"\u2705 Already in Python path: {repo_path}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api_keys"
   },
   "source": [
    "## \ud83d\udd11 API Key Configuration\n",
    "\n",
    "You have two options for setting API keys:\n",
    "\n",
    "### Option 1: Direct Environment Variables (Quick Setup)\n",
    "Set API keys directly in the cells below. **Note:** These will be visible in the notebook.\n",
    "\n",
    "### Option 2: Google Colab Secrets (Recommended for Colab)\n",
    "1. Click the \ud83d\udd11 key icon in the left sidebar\n",
    "2. Add secrets with names: `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`\n",
    "3. Toggle \"Notebook access\" on for each secret\n",
    "\n",
    "The code below will check both sources and use Colab secrets if available."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "set_api_keys"
   },
   "source": [
    "import os\n",
    "\n",
    "# Try to use Google Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    print(\"\u2705 Google Colab detected - checking for secrets...\\n\")\n",
    "\n",
    "    # Try to get OpenAI key from secrets\n",
    "    try:\n",
    "        openai_key = userdata.get('OPENAI_API_KEY')\n",
    "        os.environ['OPENAI_API_KEY'] = openai_key\n",
    "        print(\"\u2705 OPENAI_API_KEY loaded from Colab secrets\")\n",
    "    except:\n",
    "        if 'OPENAI_API_KEY' not in os.environ:\n",
    "            print(\"\u26a0\ufe0f  OPENAI_API_KEY not found in Colab secrets\")\n",
    "\n",
    "    # Try to get Anthropic key from secrets\n",
    "    try:\n",
    "        anthropic_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "        os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
    "        print(\"\u2705 ANTHROPIC_API_KEY loaded from Colab secrets\")\n",
    "    except:\n",
    "        if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "            print(\"\u26a0\ufe0f  ANTHROPIC_API_KEY not found in Colab secrets\")\n",
    "\n",
    "    # Try to get Google key from secrets\n",
    "    try:\n",
    "        google_key = userdata.get('GOOGLE_API_KEY')\n",
    "        os.environ['GOOGLE_API_KEY'] = google_key\n",
    "        print(\"\u2705 GOOGLE_API_KEY loaded from Colab secrets\")\n",
    "    except:\n",
    "        if 'GOOGLE_API_KEY' not in os.environ:\n",
    "            print(\"\u26a0\ufe0f  GOOGLE_API_KEY not found in Colab secrets\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\u2139\ufe0f  Not running in Google Colab - using environment variables\")\n",
    "\n",
    "# Option 1: Set API keys directly (if not using Colab secrets)\n",
    "# Uncomment and set your keys below if needed:\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
    "# os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'\n",
    "# os.environ['GOOGLE_API_KEY'] = 'AI...'\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"API Key Status:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"OpenAI:    {'\u2705 Configured' if os.environ.get('OPENAI_API_KEY') else '\u274c Not set'}\")\n",
    "print(f\"Anthropic: {'\u2705 Configured' if os.environ.get('ANTHROPIC_API_KEY') else '\u274c Not set'}\")\n",
    "print(f\"Google:    {'\u2705 Configured' if os.environ.get('GOOGLE_API_KEY') else '\u274c Not set'}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n\u2139\ufe0f  You can run tournaments with any bots that have API keys configured.\")\n",
    "print(\"   SimpleBot is always available and doesn't require an API key.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## \ud83d\udcda Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "import_modules"
   },
   "source": [
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "\n",
    "# Configure logging to see bot actions\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Import game components\n",
    "from reinforcetactics.core.game_state import GameState\n",
    "from reinforcetactics.game.bot import SimpleBot\n",
    "from reinforcetactics.utils.file_io import FileIO\n",
    "\n",
    "# Import LLM bots (with graceful fallback)\n",
    "llm_bots_available = {}\n",
    "\n",
    "try:\n",
    "    from reinforcetactics.game.llm_bot import OpenAIBot\n",
    "    llm_bots_available['openai'] = OpenAIBot\n",
    "    print(\"\u2705 OpenAIBot available\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0\ufe0f  OpenAIBot not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from reinforcetactics.game.llm_bot import ClaudeBot\n",
    "    llm_bots_available['claude'] = ClaudeBot\n",
    "    print(\"\u2705 ClaudeBot available\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0\ufe0f  ClaudeBot not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from reinforcetactics.game.llm_bot import GeminiBot\n",
    "    llm_bots_available['gemini'] = GeminiBot\n",
    "    print(\"\u2705 GeminiBot available\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u26a0\ufe0f  GeminiBot not available: {e}\")\n",
    "\n",
    "print(f\"\\n\u2705 Imports complete! {len(llm_bots_available)} LLM bot types available.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "single_game"
   },
   "source": [
    "## \ud83c\udfae Single Game Runner\n",
    "\n",
    "Run a single game between two bots with detailed logging."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_single_game_function"
   },
   "source": [
    "def run_single_game(",
    "    player1_bot: Union[str, type],",
    "    player2_bot: Union[str, type],",
    "    map_file: str = 'maps/1v1/6x6_beginner.csv',",
    "    max_turns: int = 500,",
    "    verbose: bool = True,",
    "    player1_model: Optional[str] = None,",
    "    player2_model: Optional[str] = None,",
    "    log_conversations: bool = False,",
    "    conversation_log_dir: Optional[str] = None",
    ") -> int:",
    "    \"\"\"",
    "    Run a single game between two bots.",
    "",
    "    Args:",
    "        player1_bot: Bot class or 'simple' for SimpleBot (plays as Player 1)",
    "        player2_bot: Bot class or 'simple' for SimpleBot (plays as Player 2)",
    "        map_file: Path to map file (default: maps/1v1/6x6_beginner.csv)",
    "        max_turns: Maximum number of turns to prevent infinite games (default: 500)",
    "        verbose: Show turn-by-turn details (default: True)",
    "        player1_model: Optional model name for player 1 LLM bot",
    "        player2_model: Optional model name for player 2 LLM bot",
    "        log_conversations: Enable conversation logging for LLM bots (default: False)",
    "        conversation_log_dir: Directory for conversation logs (optional)",
    "",
    "    Returns:",
    "        Winner: 1 (player 1 wins), 2 (player 2 wins), or 0 (draw)",
    "    \"\"\"",
    "    # Load map",
    "    map_data = FileIO.load_map(map_file)",
    "    if map_data is None:",
    "        raise ValueError(f\"Failed to load map: {map_file}\")",
    "",
    "    # Create game state",
    "    game_state = GameState(map_data, num_players=2)",
    "    ",
    "    # Handle conversation log directory",
    "    abs_log_dir = None",
    "    log_file_count_before = 0",
    "    if log_conversations and conversation_log_dir:",
    "        # Convert to absolute path",
    "        abs_log_dir = os.path.abspath(conversation_log_dir)",
    "        # Create directory if it doesn't exist",
    "        try:",
    "            os.makedirs(abs_log_dir, exist_ok=True)",
    "            if verbose:",
    "                print(f\"\ud83d\udcc1 Log directory created/verified: {abs_log_dir}\")",
    "        except Exception as e:",
    "            print(f\"\u26a0\ufe0f  Warning: Could not create log directory: {e}\")",
    "            abs_log_dir = None",
    "        ",
    "        # Count existing log files",
    "        if abs_log_dir and os.path.exists(abs_log_dir):",
    "            log_file_count_before = len([f for f in os.listdir(abs_log_dir) if f.endswith('.json')])",
    "",
    "    # Create bot instances",
    "    def create_bot(bot_spec, player_num, model=None):",
    "        if bot_spec == 'simple' or bot_spec is None:",
    "            return SimpleBot(game_state, player_num)",
    "        else:",
    "            # It's an LLM bot class",
    "            kwargs = {'game_state': game_state, 'player': player_num}",
    "            if model:",
    "                kwargs['model'] = model",
    "            if log_conversations:",
    "                kwargs['log_conversations'] = log_conversations",
    "            if abs_log_dir:",
    "                kwargs['conversation_log_dir'] = abs_log_dir",
    "",
    "            return bot_spec(**kwargs)",
    "",
    "    bot1 = create_bot(player1_bot, 1, player1_model)",
    "    bot2 = create_bot(player2_bot, 2, player2_model)",
    "    bots = {1: bot1, 2: bot2}",
    "",
    "    # Get bot names",
    "    bot1_name = bot1.__class__.__name__",
    "    bot2_name = bot2.__class__.__name__",
    "",
    "    if verbose:",
    "        print(\"\\n\" + \"=\"*60)",
    "        print(f\"Game Start: {bot1_name} (P1) vs {bot2_name} (P2)\")",
    "        print(f\"Map: {map_file}\")",
    "        if log_conversations:",
    "            print(f\"Conversation Logging: ENABLED\")",
    "            if abs_log_dir:",
    "                print(f\"\ud83d\udcc1 Absolute Log Path: {abs_log_dir}\")",
    "        print(\"=\"*60)",
    "",
    "    # Play the game",
    "    turn_count = 0",
    "    last_gold = {1: game_state.player_gold[1], 2: game_state.player_gold[2]}",
    "",
    "    while not game_state.game_over and turn_count < max_turns:",
    "        current_player = game_state.current_player",
    "        current_bot = bots[current_player]",
    "        bot_name = bot1_name if current_player == 1 else bot2_name",
    "",
    "        # Show turn info",
    "        if verbose:",
    "            print(f\"\\n--- Turn {turn_count + 1} - {bot_name} (P{current_player}) ---\")",
    "            print(f\"  Gold: P1={game_state.player_gold[1]}, P2={game_state.player_gold[2]}\")",
    "",
    "        # Bot takes turn",
    "        try:",
    "            current_bot.take_turn()",
    "        except Exception as e:",
    "            print(f\"\u26a0\ufe0f  Error during {bot_name} turn: {e}\")",
    "            # Bot forfeits on error",
    "            game_state.game_over = True",
    "            game_state.winner = 1 if current_player == 2 else 2",
    "            break",
    "",
    "        # Show gold changes",
    "        if verbose:",
    "            gold_change = game_state.player_gold[current_player] - last_gold[current_player]",
    "            if gold_change != 0:",
    "                print(f\"  Gold change: {gold_change:+d}\")",
    "            last_gold[current_player] = game_state.player_gold[current_player]",
    "",
    "        turn_count += 1",
    "",
    "        # Check for game over",
    "        if game_state.game_over:",
    "            break",
    "",
    "    # Determine winner",
    "    if game_state.game_over and game_state.winner:",
    "        winner = game_state.winner",
    "        winner_name = bot1_name if winner == 1 else bot2_name",
    "    elif turn_count >= max_turns:",
    "        winner = 0",
    "        winner_name = \"Draw (max turns)\"",
    "    else:",
    "        winner = 0",
    "        winner_name = \"Draw\"",
    "",
    "    if verbose:",
    "        print(\"\\n\" + \"=\"*60)",
    "        print(f\"Game Over! Winner: {winner_name}\")",
    "        print(f\"Total turns: {turn_count}\")",
    "        print(f\"Final gold - P1: {game_state.player_gold[1]}, P2: {game_state.player_gold[2]}\")",
    "        print(\"=\"*60 + \"\\n\")",
    "    ",
    "    # Show log file summary",
    "    if log_conversations and abs_log_dir and os.path.exists(abs_log_dir):",
    "        log_files = [f for f in os.listdir(abs_log_dir) if f.endswith('.json')]",
    "        new_log_count = len(log_files) - log_file_count_before",
    "        if new_log_count > 0:",
    "            print(\"\\n\" + \"=\"*60)",
    "            print(\"\ud83d\udcca Conversation Logs Summary\")",
    "            print(\"=\"*60)",
    "            print(f\"\u2705 Generated {new_log_count} new log file(s)\")",
    "            print(f\"\ud83d\udcc1 Absolute path: {abs_log_dir}\")",
    "            print(f\"\ud83d\udcdd Total log files in directory: {len(log_files)}\")",
    "            print(f\"\\n\ud83d\udca1 To list log files, run:\")",
    "            print(f\"   !ls -lh {abs_log_dir}\")",
    "            print(\"=\"*60 + \"\\n\")",
    "",
    "    return winner",
    "",
    "print(\"\u2705 run_single_game() function defined\")",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tournament"
   },
   "source": [
    "## \ud83c\udfc6 Tournament Runner\n",
    "\n",
    "Run a round-robin tournament between multiple bots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_tournament_function"
   },
   "source": [
    "def run_tournament(\n",
    "    bots: List[Tuple[str, Union[str, type], Optional[str]]],\n",
    "    map_file: str = 'maps/1v1/6x6_beginner.csv',\n",
    "    games_per_matchup: int = 2,\n",
    "    max_turns: int = 500,\n",
    "    log_conversations: bool = False,\n",
    "    conversation_log_dir: Optional[str] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Run a round-robin tournament between multiple bots.\n",
    "\n",
    "    Args:\n",
    "        bots: List of (name, bot_class_or_'simple', optional_model) tuples\n",
    "        map_file: Path to map file (default: maps/1v1/6x6_beginner.csv)\n",
    "        games_per_matchup: Games per side (total = 2 * games_per_matchup)\n",
    "        max_turns: Maximum turns per game (default: 500)\n",
    "        log_conversations: Enable conversation logging for LLM bots (default: False)\n",
    "        conversation_log_dir: Directory for conversation logs (optional)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with tournament results and standings\n",
    "\n",
    "    Example:\n",
    "        bots = [\n",
    "            ('SimpleBot', 'simple', None),\n",
    "            ('GPT-4o-mini', OpenAIBot, 'gpt-4o-mini'),\n",
    "            ('Claude', ClaudeBot, None)  # Uses default model\n",
    "        ]\n",
    "        results = run_tournament(bots, log_conversations=True)\n",
    "    \"\"\"\n",
    "    if len(bots) < 2:\n",
    "        raise ValueError(\"Need at least 2 bots for a tournament\")\n",
    "    \n",
    "    # Handle conversation log directory\n",
    "    abs_log_dir = None\n",
    "    initial_log_count = 0\n",
    "    if log_conversations and conversation_log_dir:\n",
    "        # Convert to absolute path\n",
    "        abs_log_dir = os.path.abspath(conversation_log_dir)\n",
    "        # Create directory if it doesn't exist\n",
    "        try:\n",
    "            os.makedirs(abs_log_dir, exist_ok=True)\n",
    "            print(f\"\ud83d\udcc1 Log directory created/verified: {abs_log_dir}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f  Warning: Could not create log directory: {e}\")\n",
    "            abs_log_dir = None\n",
    "        \n",
    "        # Count existing log files\n",
    "        if abs_log_dir and os.path.exists(abs_log_dir):\n",
    "            initial_log_count = len([f for f in os.listdir(abs_log_dir) if f.endswith('.json')])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"\ud83c\udfc6 TOURNAMENT START\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Map: {map_file}\")\n",
    "    print(f\"Participants: {len(bots)}\")\n",
    "    for name, bot_type, model in bots:\n",
    "        model_str = f\" ({model})\" if model else \"\"\n",
    "        bot_type_str = \"SimpleBot\" if bot_type == 'simple' else bot_type.__name__\n",
    "        print(f\"  - {name}: {bot_type_str}{model_str}\")\n",
    "    print(f\"Games per matchup: {games_per_matchup * 2} ({games_per_matchup} per side)\")\n",
    "    if log_conversations:\n",
    "        print(f\"LLM Conversation Logging: ENABLED\")\n",
    "        if abs_log_dir:\n",
    "            print(f\"\ud83d\udcc1 Absolute Log Path: {abs_log_dir}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    # Initialize results tracking\n",
    "    results = defaultdict(lambda: {'wins': 0, 'losses': 0, 'draws': 0})\n",
    "    matchup_details = []\n",
    "\n",
    "    # Generate all matchups (round-robin)\n",
    "    matchups = []\n",
    "    for i in range(len(bots)):\n",
    "        for j in range(i + 1, len(bots)):\n",
    "            matchups.append((i, j))\n",
    "\n",
    "    total_games = len(matchups) * games_per_matchup * 2\n",
    "    print(f\"\ud83d\udcca Total matchups: {len(matchups)}\")\n",
    "    print(f\"\ud83d\udcca Total games: {total_games}\\n\")\n",
    "\n",
    "    game_num = 0\n",
    "\n",
    "    # Run all matchups\n",
    "    for matchup_idx, (i, j) in enumerate(matchups, 1):\n",
    "        bot1_name, bot1_class, bot1_model = bots[i]\n",
    "        bot2_name, bot2_class, bot2_model = bots[j]\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Matchup {matchup_idx}/{len(matchups)}: {bot1_name} vs {bot2_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        matchup_results = {\n",
    "            'bot1': bot1_name,\n",
    "            'bot2': bot2_name,\n",
    "            'bot1_wins': 0,\n",
    "            'bot2_wins': 0,\n",
    "            'draws': 0\n",
    "        }\n",
    "\n",
    "        # Play games_per_matchup with bot1 as player 1\n",
    "        for game in range(games_per_matchup):\n",
    "            game_num += 1\n",
    "            print(f\"\\n  Game {game_num}/{total_games}: {bot1_name} (P1) vs {bot2_name} (P2)\")\n",
    "\n",
    "            winner = run_single_game(\n",
    "                bot1_class, bot2_class,\n",
    "                map_file=map_file,\n",
    "                max_turns=max_turns,\n",
    "                verbose=False,\n",
    "                player1_model=bot1_model,\n",
    "                player2_model=bot2_model,\n",
    "                log_conversations=log_conversations,\n",
    "                conversation_log_dir=abs_log_dir if abs_log_dir else conversation_log_dir\n",
    "            )\n",
    "\n",
    "            if winner == 1:\n",
    "                results[bot1_name]['wins'] += 1\n",
    "                results[bot2_name]['losses'] += 1\n",
    "                matchup_results['bot1_wins'] += 1\n",
    "                print(f\"    \u2705 {bot1_name} wins!\")\n",
    "            elif winner == 2:\n",
    "                results[bot2_name]['wins'] += 1\n",
    "                results[bot1_name]['losses'] += 1\n",
    "                matchup_results['bot2_wins'] += 1\n",
    "                print(f\"    \u2705 {bot2_name} wins!\")\n",
    "            else:\n",
    "                results[bot1_name]['draws'] += 1\n",
    "                results[bot2_name]['draws'] += 1\n",
    "                matchup_results['draws'] += 1\n",
    "                print(f\"    \u2696\ufe0f  Draw\")\n",
    "\n",
    "        # Play games_per_matchup with bot2 as player 1 (swap sides)\n",
    "        for game in range(games_per_matchup):\n",
    "            game_num += 1\n",
    "            print(f\"\\n  Game {game_num}/{total_games}: {bot2_name} (P1) vs {bot1_name} (P2)\")\n",
    "\n",
    "            winner = run_single_game(\n",
    "                bot2_class, bot1_class,\n",
    "                map_file=map_file,\n",
    "                max_turns=max_turns,\n",
    "                verbose=False,\n",
    "                player1_model=bot2_model,\n",
    "                player2_model=bot1_model,\n",
    "                log_conversations=log_conversations,\n",
    "                conversation_log_dir=abs_log_dir if abs_log_dir else conversation_log_dir\n",
    "            )\n",
    "\n",
    "            if winner == 1:\n",
    "                results[bot2_name]['wins'] += 1\n",
    "                results[bot1_name]['losses'] += 1\n",
    "                matchup_results['bot2_wins'] += 1\n",
    "                print(f\"    \u2705 {bot2_name} wins!\")\n",
    "            elif winner == 2:\n",
    "                results[bot1_name]['wins'] += 1\n",
    "                results[bot2_name]['losses'] += 1\n",
    "                matchup_results['bot1_wins'] += 1\n",
    "                print(f\"    \u2705 {bot1_name} wins!\")\n",
    "            else:\n",
    "                results[bot1_name]['draws'] += 1\n",
    "                results[bot2_name]['draws'] += 1\n",
    "                matchup_results['draws'] += 1\n",
    "                print(f\"    \u2696\ufe0f  Draw\")\n",
    "\n",
    "        # Show matchup summary\n",
    "        print(f\"\\n  Matchup Summary: {bot1_name} {matchup_results['bot1_wins']}-{matchup_results['bot2_wins']}-{matchup_results['draws']} {bot2_name}\")\n",
    "        matchup_details.append(matchup_results)\n",
    "\n",
    "    # Calculate final standings\n",
    "    standings = []\n",
    "    for bot_name, stats in results.items():\n",
    "        total_games = stats['wins'] + stats['losses'] + stats['draws']\n",
    "        win_rate = stats['wins'] / total_games if total_games > 0 else 0.0\n",
    "\n",
    "        standings.append({\n",
    "            'name': bot_name,\n",
    "            'wins': stats['wins'],\n",
    "            'losses': stats['losses'],\n",
    "            'draws': stats['draws'],\n",
    "            'total': total_games,\n",
    "            'win_rate': win_rate\n",
    "        })\n",
    "\n",
    "    # Sort by wins (descending), then win_rate\n",
    "    standings.sort(key=lambda x: (x['wins'], x['win_rate']), reverse=True)\n",
    "\n",
    "    # Display final standings\n",
    "    print(\"\\n\\n\" + \"=\"*70)\n",
    "    print(\"\ud83c\udfc6 FINAL STANDINGS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'Rank':<6}{'Bot':<25}{'Wins':<8}{'Losses':<8}{'Draws':<8}{'Win Rate':<10}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    for rank, standing in enumerate(standings, 1):\n",
    "        medal = \"\ud83e\udd47\" if rank == 1 else (\"\ud83e\udd48\" if rank == 2 else (\"\ud83e\udd49\" if rank == 3 else \"  \"))\n",
    "        print(f\"{medal} {rank:<3}{standing['name']:<25}{standing['wins']:<8}{standing['losses']:<8}\"\n",
    "              f\"{standing['draws']:<8}{standing['win_rate']:.3f}\")\n",
    "\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Show log file summary\n",
    "    if log_conversations and abs_log_dir and os.path.exists(abs_log_dir):\n",
    "        log_files = [f for f in os.listdir(abs_log_dir) if f.endswith('.json')]\n",
    "        new_log_count = len(log_files) - initial_log_count\n",
    "        print(\"=\"*70)\n",
    "        print(\"\ud83d\udcca Tournament Logs Summary\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\u2705 Total games played: {total_games}\")\n",
    "        print(f\"\ud83d\udcdd New log files generated: {new_log_count}\")\n",
    "        print(f\"\ud83d\udcc1 Absolute path: {abs_log_dir}\")\n",
    "        print(f\"\ud83d\udcc4 Total log files in directory: {len(log_files)}\")\n",
    "        print(f\"\\n\ud83d\udca1 To list log files, run:\")\n",
    "        print(f\"   !ls -lh {abs_log_dir}\")\n",
    "        print(f\"\\n\ud83d\udca1 To review logs, see Example 2.6\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "    return {\n",
    "        'standings': standings,\n",
    "        'matchups': matchup_details,\n",
    "        'map': map_file,\n",
    "        'games_per_matchup': games_per_matchup\n",
    "    }\n",
    "\n",
    "print(\"\u2705 run_tournament() function defined\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "examples"
   },
   "source": [
    "## \ud83c\udfaf Example Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_single_game"
   },
   "source": [
    "### Example 1: Single Game - SimpleBot vs SimpleBot\n",
    "\n",
    "Let's start with a simple game between two SimpleBots to test the system."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_example_simple"
   },
   "source": [
    "# Run a single game: SimpleBot vs SimpleBot\n",
    "winner = run_single_game(\n",
    "    player1_bot='simple',\n",
    "    player2_bot='simple',\n",
    "    map_file='maps/1v1/6x6_beginner.csv',\n",
    "    max_turns=100,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nWinner: Player {winner}\" if winner else \"\\nResult: Draw\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_llm_game"
   },
   "source": [
    "### Example 2: Single Game - LLM Bot vs SimpleBot\n",
    "\n",
    "Test an LLM bot against SimpleBot. Make sure you have the appropriate API key configured!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_example_llm"
   },
   "source": [
    "# Run a single game: OpenAI Bot vs SimpleBot\n",
    "# Uncomment and run if you have OpenAI API key configured\n",
    "\n",
    "# if 'openai' in llm_bots_available:\n",
    "#     winner = run_single_game(\n",
    "#         player1_bot=llm_bots_available['openai'],\n",
    "#         player2_bot='simple',\n",
    "#         map_file='maps/1v1/6x6_beginner.csv',\n",
    "#         max_turns=100,\n",
    "#         verbose=True,\n",
    "#         player1_model='gpt-4o-mini'  # Use mini model for lower cost\n",
    "#     )\n",
    "#     print(f\"\\nWinner: Player {winner}\" if winner else \"\\nResult: Draw\")\n",
    "# else:\n",
    "#     print(\"\u26a0\ufe0f  OpenAIBot not available. Please install openai and configure API key.\")\n",
    "\n",
    "print(\"Uncomment the code above to run an LLM bot game\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_logging"
   },
   "source": [
    "### Example 2.5: Single Game with Conversation Logging\n",
    "\n",
    "Enable conversation logging to see the LLM's reasoning process. Logs are saved as JSON files."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_example_logging"
   },
   "source": [
    "# Run a single game with conversation logging enabled\n",
    "# Uncomment and run if you have OpenAI API key configured\n",
    "\n",
    "# if 'openai' in llm_bots_available:\n",
    "#     import tempfile\n",
    "#     import logging\n",
    "#     \n",
    "#     # Enable DEBUG logging to see conversation logs\n",
    "#     logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)\n",
    "#     \n",
    "#     # Create a temporary directory for logs\n",
    "#     with tempfile.TemporaryDirectory() as tmpdir:\n",
    "#         print(f\"Conversation logs will be saved to: {tmpdir}\")\n",
    "#         \n",
    "#         winner = run_single_game(\n",
    "#             player1_bot=llm_bots_available['openai'],\n",
    "#             player2_bot='simple',\n",
    "#             map_file='maps/1v1/6x6_beginner.csv',\n",
    "#             max_turns=100,\n",
    "#             verbose=True,\n",
    "#             player1_model='gpt-4o-mini',\n",
    "#             log_conversations=True,\n",
    "#             conversation_log_dir=tmpdir\n",
    "#         )\n",
    "#         \n",
    "#         # List the log files\n",
    "#         import os\n",
    "#         log_files = [f for f in os.listdir(tmpdir) if f.endswith('.json')]\n",
    "#         print(f\"\\n\ud83d\udcdd Generated {len(log_files)} conversation log files\")\n",
    "#         \n",
    "#         # Show a sample log entry\n",
    "#         if log_files:\n",
    "#             import json\n",
    "#             with open(os.path.join(tmpdir, log_files[0])) as f:\n",
    "#                 sample_log = json.load(f)\n",
    "#             print(f\"\\n\ud83d\udcc4 Sample log entry:\")\n",
    "#             print(json.dumps(sample_log, indent=2)[:500] + \"...\")\n",
    "# else:\n",
    "#     print(\"\u26a0\ufe0f  OpenAIBot not available. Please install openai and configure API key.\")\n",
    "\n",
    "print(\"Uncomment the code above to run a game with conversation logging\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2.6: Reviewing Logged Conversations\n",
    "\n",
    "Learn how to review and analyze the logged conversations from your games. This example shows how to:\n",
    "- Use a persistent log directory for Google Colab\n",
    "- List and inspect log files\n",
    "- Extract key information from conversation logs\n",
    "- Pretty-print log content for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2.6: Reviewing Logged Conversations\n",
    "# Run a game with persistent logging and then review the logs\n",
    "# Uncomment and run if you have OpenAI API key configured\n",
    "\n",
    "# if 'openai' in llm_bots_available:\n",
    "#     import os\n",
    "#     import json\n",
    "#     from pathlib import Path\n",
    "#     import logging\n",
    "#     \n",
    "#     # Enable DEBUG logging to see conversation logs\n",
    "#     logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)\n",
    "#     \n",
    "#     # Create persistent log directory (recommended for Google Colab)\n",
    "#     log_dir = '/content/llm_logs/'\n",
    "#     os.makedirs(log_dir, exist_ok=True)\n",
    "#     print(f\"\ud83d\udcc1 Logs will be saved to: {os.path.abspath(log_dir)}\\n\")\n",
    "#     \n",
    "#     # Run game with logging\n",
    "#     print(\"\ud83c\udfae Running game with conversation logging...\\n\")\n",
    "#     winner = run_single_game(\n",
    "#         player1_bot=llm_bots_available['openai'],\n",
    "#         player2_bot='simple',\n",
    "#         map_file='maps/1v1/6x6_beginner.csv',\n",
    "#         max_turns=50,  # Shorter game for demo\n",
    "#         verbose=True,\n",
    "#         player1_model='gpt-4o-mini',\n",
    "#         log_conversations=True,\n",
    "#         conversation_log_dir=log_dir\n",
    "#     )\n",
    "#     \n",
    "#     # Helper function to review conversation logs\n",
    "#     def review_conversation_logs(log_dir):\n",
    "#         \"\"\"Review and display conversation logs with pretty printing.\"\"\"\n",
    "#         log_path = Path(log_dir)\n",
    "#         log_files = sorted(log_path.glob('conversation_*.json'))\n",
    "#         \n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"\ud83d\udcca CONVERSATION LOGS REVIEW\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"\ud83d\udcc1 Absolute path: {log_path.absolute()}\")\n",
    "#         print(f\"\ud83d\udcdd Total log files: {len(log_files)}\\n\")\n",
    "#         \n",
    "#         if not log_files:\n",
    "#             print(\"\u26a0\ufe0f  No log files found. Make sure logging was enabled.\")\n",
    "#             return log_files\n",
    "#         \n",
    "#         # Show sample logs - first and last\n",
    "#         indices_to_show = [0]\n",
    "#         if len(log_files) > 1:\n",
    "#             indices_to_show.append(-1)\n",
    "#         \n",
    "#         for idx in indices_to_show:\n",
    "#             log_file = log_files[idx]\n",
    "#             print(f\"\\n{'='*70}\")\n",
    "#             print(f\"\ud83d\udcc4 Log file: {log_file.name}\")\n",
    "#             print(f\"\ud83d\udcc1 Full path: {log_file.absolute()}\")\n",
    "#             print('='*70)\n",
    "#             \n",
    "#             with open(log_file) as f:\n",
    "#                 log_data = json.load(f)\n",
    "#             \n",
    "#             # Display key information\n",
    "#             print(f\"\ud83d\udd50 Timestamp: {log_data.get('timestamp', 'N/A')}\")\n",
    "#             print(f\"\ud83e\udd16 Model: {log_data.get('model', 'N/A')}\")\n",
    "#             print(f\"\ud83c\udfe2 Provider: {log_data.get('provider', 'N/A')}\")\n",
    "#             print(f\"\ud83d\udc64 Player: {log_data.get('player', 'N/A')}\")\n",
    "#             print(f\"\ud83d\udd22 Turn: {log_data.get('turn_number', 'N/A')}\")\n",
    "#             \n",
    "#             # Extract reasoning from response\n",
    "#             conversation = log_data.get('conversation', {})\n",
    "#             response = conversation.get('assistant_response', '')\n",
    "#             \n",
    "#             print(f\"\\n\ud83d\udcdd Reasoning (first 200 chars):\")\n",
    "#             try:\n",
    "#                 # Parse the JSON response\n",
    "#                 response_json = json.loads(response)\n",
    "#                 reasoning = response_json.get('reasoning', 'N/A')\n",
    "#                 actions = response_json.get('actions', [])\n",
    "#                 \n",
    "#                 print(f\"  {reasoning[:200]}...\")\n",
    "#                 print(f\"\\n\u26a1 Actions: {len(actions)} action(s)\")\n",
    "#                 \n",
    "#                 # Show action types\n",
    "#                 if actions:\n",
    "#                     action_types = [a.get('action_type', 'unknown') for a in actions[:3]]\n",
    "#                     print(f\"  First few: {', '.join(action_types)}\")\n",
    "#             except json.JSONDecodeError:\n",
    "#                 print(f\"  {response[:200]}...\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"  Error parsing response: {e}\")\n",
    "#         \n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"\ud83d\udca1 Tips:\")\n",
    "#         print(\"  - Use 'jq' for advanced JSON parsing: !jq . path/to/log.json\")\n",
    "#         print(\"  - Search logs by player: [f for f in log_files if 'player=1' in str(f)]\")\n",
    "#         print(\"  - Compare reasoning across different models\")\n",
    "#         print(\"=\"*70 + \"\\n\")\n",
    "#         \n",
    "#         return log_files\n",
    "#     \n",
    "#     # Review the logs\n",
    "#     log_files = review_conversation_logs(log_dir)\n",
    "#     \n",
    "#     # Optional: Search logs by turn number\n",
    "#     def find_log_by_turn(log_dir, turn_number):\n",
    "#         \"\"\"Find log file for a specific turn number.\"\"\"\n",
    "#         log_path = Path(log_dir)\n",
    "#         for log_file in log_path.glob('conversation_*.json'):\n",
    "#             with open(log_file) as f:\n",
    "#                 log_data = json.load(f)\n",
    "#                 if log_data.get('turn_number') == turn_number:\n",
    "#                     return log_file\n",
    "#         return None\n",
    "#     \n",
    "#     print(\"\\n\ud83d\udccc Helper functions defined:\")\n",
    "#     print(\"  - review_conversation_logs(log_dir): Review all logs\")\n",
    "#     print(\"  - find_log_by_turn(log_dir, turn_number): Find specific turn\")\n",
    "# else:\n",
    "#     print(\"\u26a0\ufe0f  OpenAIBot not available. Please install openai and configure API key.\")\n",
    "\n",
    "print(\"Uncomment the code above to run Example 2.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_tournament"
   },
   "source": [
    "### Example 3: Mini Tournament\n",
    "\n",
    "Run a small tournament with available bots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_example_tournament"
   },
   "source": [
    "# Define tournament participants\n",
    "# Format: (display_name, bot_class_or_'simple', optional_model_name)\n",
    "\n",
    "tournament_bots = [\n",
    "    ('SimpleBot', 'simple', None),\n",
    "]\n",
    "\n",
    "# Add LLM bots if available and configured\n",
    "if 'openai' in llm_bots_available and os.environ.get('OPENAI_API_KEY'):\n",
    "    tournament_bots.append(('GPT-4o-mini', llm_bots_available['openai'], 'gpt-4o-mini'))\n",
    "\n",
    "if 'claude' in llm_bots_available and os.environ.get('ANTHROPIC_API_KEY'):\n",
    "    tournament_bots.append(('Claude Haiku', llm_bots_available['claude'], 'claude-3-haiku-20240307'))\n",
    "\n",
    "if 'gemini' in llm_bots_available and os.environ.get('GOOGLE_API_KEY'):\n",
    "    tournament_bots.append(('Gemini Flash', llm_bots_available['gemini'], 'gemini-1.5-flash'))\n",
    "\n",
    "# Run tournament if we have at least 2 bots\n",
    "if len(tournament_bots) >= 2:\n",
    "    results = run_tournament(\n",
    "        bots=tournament_bots,\n",
    "        map_file='maps/1v1/6x6_beginner.csv',\n",
    "        games_per_matchup=2,  # 2 games per side = 4 total per matchup\n",
    "        max_turns=100\n",
    "    )\n",
    "    print(\"\\n\u2705 Tournament complete! Results saved in 'results' variable.\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Need at least 2 bots for a tournament.\")\n",
    "    print(\"   Configure API keys for LLM bots or add more SimpleBots for testing.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_models"
   },
   "source": [
    "## \ud83c\udfa8 Custom Model Configuration\n",
    "\n",
    "You can specify different models for each LLM provider. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "openai_models"
   },
   "source": [
    "### OpenAI Models\n",
    "\n",
    "**Available models:**\n",
    "- `gpt-4o-mini` (default) - Fastest and cheapest, good for testing\n",
    "- `gpt-4o` - More capable, higher cost\n",
    "- `gpt-4-turbo` - Previous generation, balanced\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Using GPT-4o for stronger gameplay\n",
    "winner = run_single_game(\n",
    "    player1_bot=OpenAIBot,\n",
    "    player2_bot='simple',\n",
    "    player1_model='gpt-4o'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "claude_models"
   },
   "source": [
    "### Claude Models\n",
    "\n",
    "**Available models:**\n",
    "- `claude-3-haiku-20240307` (default) - Fastest and cheapest\n",
    "- `claude-3-5-sonnet-20241022` - Most capable, balanced cost\n",
    "- `claude-3-opus-20240229` - Highest capability, highest cost\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Using Claude Sonnet for better strategic play\n",
    "winner = run_single_game(\n",
    "    player1_bot=ClaudeBot,\n",
    "    player2_bot='simple',\n",
    "    player1_model='claude-3-5-sonnet-20241022'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gemini_models"
   },
   "source": [
    "### Gemini Models\n",
    "\n",
    "**Available models:**\n",
    "- `gemini-1.5-flash` (default) - Fast and efficient, free tier available\n",
    "- `gemini-1.5-pro` - More capable, higher cost\n",
    "- `gemini-2.0-flash-exp` - Experimental, cutting edge\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Using Gemini Pro for better performance\n",
    "winner = run_single_game(\n",
    "    player1_bot=GeminiBot,\n",
    "    player2_bot='simple',\n",
    "    player1_model='gemini-1.5-pro'\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_tournament_example"
   },
   "source": [
    "### Example: Tournament with Custom Models\n",
    "\n",
    "Compare different models from different providers:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_custom_tournament"
   },
   "source": [
    "# Advanced tournament: Compare different models\n",
    "# Only run this if you have all API keys configured and don't mind the cost!\n",
    "\n",
    "# advanced_bots = [\n",
    "#     ('SimpleBot', 'simple', None),\n",
    "#     ('GPT-4o-mini', OpenAIBot, 'gpt-4o-mini'),\n",
    "#     ('GPT-4o', OpenAIBot, 'gpt-4o'),\n",
    "#     ('Claude Haiku', ClaudeBot, 'claude-3-haiku-20240307'),\n",
    "#     ('Claude Sonnet', ClaudeBot, 'claude-3-5-sonnet-20241022'),\n",
    "#     ('Gemini Flash', GeminiBot, 'gemini-1.5-flash'),\n",
    "#     ('Gemini Pro', GeminiBot, 'gemini-1.5-pro'),\n",
    "# ]\n",
    "\n",
    "# results = run_tournament(\n",
    "#     bots=advanced_bots,\n",
    "#     map_file='maps/1v1/6x6_beginner.csv',\n",
    "#     games_per_matchup=2,\n",
    "#     max_turns=100\n",
    "# )\n",
    "\n",
    "print(\"Uncomment the code above to run a full model comparison tournament\")\n",
    "print(\"\u26a0\ufe0f  Warning: This will make many API calls and may incur costs!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "documentation"
   },
   "source": [
    "## \ud83d\udcd6 Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cost_estimates"
   },
   "source": [
    "### \ud83d\udcb0 API Cost Estimates\n",
    "\n",
    "**OpenAI Pricing (approximate, as of 2024):**\n",
    "- GPT-4o-mini: $0.15/1M input tokens, $0.60/1M output tokens\n",
    "- GPT-4o: $2.50/1M input tokens, $10.00/1M output tokens\n",
    "- Typical game: 2,000-10,000 tokens per side\n",
    "- **Cost per game:** $0.0001-0.0005 (mini), $0.005-0.02 (4o)\n",
    "\n",
    "**Anthropic Pricing:**\n",
    "- Claude Haiku: $0.25/1M input tokens, $1.25/1M output tokens\n",
    "- Claude Sonnet: $3.00/1M input tokens, $15.00/1M output tokens\n",
    "- **Cost per game:** $0.0001-0.0003 (Haiku), $0.003-0.015 (Sonnet)\n",
    "\n",
    "**Google Gemini Pricing:**\n",
    "- Gemini Flash: Free tier available (15 RPM), $0.075/1M input, $0.30/1M output\n",
    "- Gemini Pro: $1.25/1M input tokens, $5.00/1M output tokens\n",
    "- **Cost per game:** ~$0 (Flash free tier), $0.0001-0.001 (Flash paid), $0.001-0.005 (Pro)\n",
    "\n",
    "**Tournament Cost Estimates:**\n",
    "- 3 bots, 2 games/matchup: 12 games total\n",
    "- Using mini/haiku/flash: ~$0.001-0.01 total\n",
    "- Using premium models: ~$0.05-0.20 total\n",
    "\n",
    "**Cost Saving Tips:**\n",
    "1. Use mini/haiku/flash models for development and testing\n",
    "2. Use smaller maps (6x6) which need fewer tokens\n",
    "3. Set lower `max_turns` to prevent long games\n",
    "4. Use Gemini Flash free tier for unlimited testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_tips"
   },
   "source": [
    "### \u26a1 Performance Tips\n",
    "\n",
    "**Model Selection for Different Purposes:**\n",
    "\n",
    "**For Testing & Development:**\n",
    "- \u2705 GPT-4o-mini - Fast, cheap, decent strategy\n",
    "- \u2705 Claude Haiku - Very fast, cheap, good baseline\n",
    "- \u2705 Gemini Flash - Free tier, fast, great for testing\n",
    "\n",
    "**For Competitive Play:**\n",
    "- \ud83c\udfc6 GPT-4o - Strong strategic thinking\n",
    "- \ud83c\udfc6 Claude Sonnet 3.5 - Excellent reasoning, good value\n",
    "- \ud83c\udfc6 Gemini Pro 1.5 - Good balance of cost/performance\n",
    "\n",
    "**For Research/Analysis:**\n",
    "- \ud83d\udd2c Claude Opus - Highest reasoning capability\n",
    "- \ud83d\udd2c GPT-4 Turbo - Consistent performance\n",
    "\n",
    "**Game Speed:**\n",
    "- Smaller maps (6x6) complete in 1-3 minutes per game\n",
    "- Larger maps (32x32) can take 10-30 minutes per game\n",
    "- LLM API calls add 1-5 seconds per turn\n",
    "- SimpleBot is nearly instant\n",
    "\n",
    "**Tournament Duration Estimates:**\n",
    "- 2 bots, 4 games: ~5-15 minutes\n",
    "- 3 bots, 12 games: ~15-45 minutes\n",
    "- 4 bots, 24 games: ~30-90 minutes\n",
    "- 5 bots, 40 games: ~1-2 hours\n",
    "\n",
    "**Optimization Strategies:**\n",
    "1. Start with 6x6 maps for quick iterations\n",
    "2. Use `games_per_matchup=1` for initial testing\n",
    "3. Set `max_turns=100` for faster games\n",
    "4. Run tournaments with fewer bots initially\n",
    "5. Use verbose=False in run_single_game() to reduce output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conversation_logging"
   },
   "source": [
    "### \ud83d\udcdd Conversation Logging Deep Dive",
    "",
    "**What is Conversation Logging?**",
    "",
    "Conversation logging captures the complete interaction between your code and the LLM, including:",
    "- System prompts (game rules and instructions)",
    "- User prompts (current game state)",
    "- Assistant responses (LLM's reasoning and actions)",
    "",
    "**When to Use Logging:**",
    "- \ud83d\udc1b **Debugging**: Understand why an LLM bot made invalid moves",
    "- \ud83d\udcca **Analysis**: Study strategic decision-making patterns",
    "- \ud83c\udf93 **Learning**: See how different models reason about game states",
    "- \ud83d\udd2c **Research**: Collect data for prompt engineering improvements",
    "",
    "**How to Enable Logging:**",
    "",
    "1. **Single Game with Absolute Paths:**",
    "```python",
    "import logging",
    "import os",
    "",
    "# Enable DEBUG logging for LLM bot",
    "logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)",
    "",
    "# Use absolute path for clarity",
    "log_dir = '/content/llm_logs/'",
    "print(f\"\ud83d\udcc1 Absolute log path: {os.path.abspath(log_dir)}\")",
    "",
    "winner = run_single_game(",
    "    player1_bot=OpenAIBot,",
    "    player2_bot='simple',",
    "    log_conversations=True,",
    "    conversation_log_dir=log_dir  # Automatically converted to absolute",
    ")",
    "```",
    "",
    "2. **Tournament with Path Resolution:**",
    "```python",
    "# The functions automatically convert relative to absolute paths",
    "results = run_tournament(",
    "    bots=tournament_bots,",
    "    log_conversations=True,",
    "    conversation_log_dir='./tournament_logs/'  # Converted automatically",
    ")",
    "```",
    "",
    "**\ud83d\udcc1 Best Practices for Log Directories:**",
    "",
    "**For Google Colab:**",
    "- \u2705 **Recommended**: Use `/content/llm_logs/` for temporary storage",
    "- \u2705 **Better**: Mount Google Drive for persistence across sessions",
    "- \u274c **Avoid**: Using `tempfile.TemporaryDirectory()` - hard to access logs later",
    "",
    "**Persistent Storage in Google Colab:**",
    "```python",
    "from google.colab import drive",
    "drive.mount('/content/drive')",
    "",
    "# Logs persist even after runtime disconnects",
    "log_dir = '/content/drive/MyDrive/reinforce_tactics_logs/'",
    "os.makedirs(log_dir, exist_ok=True)",
    "",
    "winner = run_single_game(",
    "    player1_bot=OpenAIBot,",
    "    player2_bot='simple',",
    "    log_conversations=True,",
    "    conversation_log_dir=log_dir",
    ")",
    "```",
    "",
    "**\u26a0\ufe0f Important Notes About Colab Storage:**",
    "- Files in `/content/` are **deleted** when the runtime disconnects",
    "- Mount Google Drive (`/content/drive/MyDrive/`) for permanent storage",
    "- Check available space: `!df -h /content/`",
    "",
    "**Log File Format:**",
    "",
    "Each turn generates a JSON file with format `conversation_{timestamp}_{microseconds}_turn{N}.json`:",
    "",
    "```json",
    "{",
    "  \"timestamp\": \"2024-01-15T10:30:45.123456\",",
    "  \"model\": \"gpt-4o-mini\",",
    "  \"provider\": \"OpenAI\",",
    "  \"player\": 1,",
    "  \"turn_number\": 5,",
    "  \"conversation\": {",
    "    \"system_prompt\": \"You are an AI playing...\",",
    "    \"user_prompt\": \"Current game state...\",",
    "    \"assistant_response\": \"{\\\"reasoning\\\": \\\"I should...\\\", \\\"actions\\\": [...]}\"",
    "  }",
    "}",
    "```",
    "",
    "**Storage and Privacy:**",
    "- \u26a0\ufe0f **Logging requires DEBUG level**: Set `logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)`",
    "- \ud83d\udcbe **Storage**: Log files can be large in long games (500KB-5MB per game)",
    "- \ud83d\udd12 **Privacy**: Log files may contain API responses; keep them secure",
    "- \ud83e\uddf9 **Cleanup**: Remember to delete old log files to save disk space",
    "- \ud83d\udcc1 **Absolute paths**: Both functions now show absolute paths for easy access",
    "",
    "**Analysis Tips:**",
    "- See **Example 2.6** for a complete review workflow",
    "- Use `jq` or Python's `json` module to parse log files",
    "- Compare reasoning across different models",
    "- Identify patterns in successful vs. failed moves",
    "- Extract statistics on action types and frequencies",
    "- Use `!ls -lh /absolute/path/` to list log files with sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "### \ud83d\udd27 Troubleshooting\n",
    "\n",
    "**Problem: \"API key not provided\" error**\n",
    "- **Solution:** Make sure you've set the API key in the environment or Colab secrets\n",
    "- Check the API Key Configuration section output\n",
    "- Uncomment and set the API key directly in the configuration cell\n",
    "\n",
    "**Problem: \"openai package not installed\" error**\n",
    "- **Solution:** Run the installation cell again\n",
    "- Or manually install: `!pip install openai>=1.0.0`\n",
    "\n",
    "**Problem: Bot makes invalid moves or errors**\n",
    "- **Solution:** This is expected occasionally with LLMs\n",
    "- The game will skip invalid actions and continue\n",
    "- Try using a more capable model (e.g., GPT-4o instead of mini)\n",
    "- Check the logs to see what actions failed\n",
    "\n",
    "**Problem: Games taking too long**\n",
    "- **Solution:** Reduce `max_turns` parameter\n",
    "- Use smaller maps (6x6 instead of 32x32)\n",
    "- Faster models: Haiku, Flash, or mini\n",
    "\n",
    "**Problem: API rate limits exceeded**\n",
    "- **Solution:** Add delays between games\n",
    "- Use free tier models (Gemini Flash)\n",
    "- Reduce `games_per_matchup`\n",
    "- Spread tournament over multiple sessions\n",
    "\n",
    "**Problem: Out of memory error**\n",
    "- **Solution:** Restart the notebook runtime\n",
    "- Run fewer games at once\n",
    "- Use smaller maps\n",
    "\n",
    "**Problem: \"Map file not found\" error**\n",
    "- **Solution:** Make sure you're in the reinforce-tactics directory\n",
    "- Check the path: `!ls maps/1v1/`\n",
    "- Use absolute paths if needed\n",
    "\n",
    "**Problem: Import errors for LLM bots**\n",
    "- **Solution:** Check that dependencies installed correctly\n",
    "- Verify the repository was cloned successfully\n",
    "- Make sure the repository is in your Python path\n",
    "\n",
    "**Problem: Tournament results seem random**\n",
    "- **Solution:** LLMs have inherent randomness\n",
    "- Increase `games_per_matchup` for more stable results\n",
    "- Temperature parameter affects consistency (set in llm_bot.py)\n",
    "- SimpleBot is deterministic and provides a good baseline\n",
    "\n",
    "**Problem: Cost concerns**\n",
    "- **Solution:** Always use mini/haiku/flash for testing\n",
    "- Monitor API usage in your provider dashboard\n",
    "- Set spending limits in your API account\n",
    "- Test with SimpleBot first (free)\n",
    "- Use Gemini Flash free tier for unlimited testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "advanced"
   },
   "source": [
    "## \ud83d\ude80 Advanced Usage\n",
    "\n",
    "### Different Map Sizes\n",
    "\n",
    "Test bots on different map complexities:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_different_maps"
   },
   "source": [
    "# Test on different map sizes\n",
    "maps = [\n",
    "    'maps/1v1/6x6_beginner.csv',\n",
    "    'maps/1v1/10x10_easy.csv',\n",
    "    'maps/1v1/14x14_medium.csv',\n",
    "]\n",
    "\n",
    "# for map_file in maps:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Testing on: {map_file}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#\n",
    "#     winner = run_single_game(\n",
    "#         player1_bot='simple',\n",
    "#         player2_bot='simple',\n",
    "#         map_file=map_file,\n",
    "#         max_turns=200,\n",
    "#         verbose=False\n",
    "#     )\n",
    "#     print(f\"Winner: Player {winner}\" if winner else \"Result: Draw\")\n",
    "\n",
    "print(\"Uncomment the code above to test different map sizes\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## \ud83c\udf93 Next Steps\n",
    "\n",
    "**Experiment Ideas:**\n",
    "1. Compare different models from the same provider\n",
    "2. Test how map size affects bot performance\n",
    "3. Analyze which bots excel at different strategies\n",
    "4. Track game length and resource management\n",
    "5. Create a \"ladder\" system with Elo ratings\n",
    "\n",
    "**Code Customization:**\n",
    "1. Modify system prompts in `llm_bot.py` for different strategies\n",
    "2. Add logging to track specific metrics\n",
    "3. Create visualization of tournament brackets\n",
    "4. Export results to CSV for analysis\n",
    "5. Build a web interface for live tournaments\n",
    "\n",
    "**Advanced Tournaments:**\n",
    "1. Swiss-system tournament format\n",
    "2. Double elimination brackets\n",
    "3. Time-limited games\n",
    "4. Asymmetric maps\n",
    "5. Team battles (coming soon)\n",
    "\n",
    "**Contributing:**\n",
    "- Found a bug? Open an issue on GitHub\n",
    "- Have an improvement? Submit a pull request\n",
    "- Share your tournament results!\n",
    "\n",
    "**Resources:**\n",
    "- Repository: https://github.com/kuds/reinforce-tactics\n",
    "- Game Rules: See `reinforcetactics/game/llm_bot.py` SYSTEM_PROMPT\n",
    "- Tournament Script: `scripts/tournament.py`\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Gaming! \ud83c\udfae**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "llm_bot_tournament.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}