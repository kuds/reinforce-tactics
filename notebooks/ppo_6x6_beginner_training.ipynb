{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/ppo_6x6_beginner_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üéÆ Reinforce Tactics - PPO Training on 6x6 Beginner Map\n",
        "\n",
        "This notebook trains a PPO (Proximal Policy Optimization) agent to play against SimpleBot on the 6x6 beginner map in headless mode.\n",
        "\n",
        "**Features:**\n",
        "- Headless training (no GUI rendering) for fast RL training\n",
        "- 6x6 beginner map for quick training iterations\n",
        "- SimpleBot opponent for consistent baseline\n",
        "- Stable-Baselines3 PPO algorithm with MultiInputPolicy\n",
        "- TensorBoard monitoring\n",
        "- Checkpoint saving and model evaluation\n",
        "- GPU acceleration support\n",
        "\n",
        "**Map Layout (6x6 beginner):**\n",
        "```\n",
        "h_1,b_1,p,p,p,p\n",
        "b_1,p,p,p,p,p\n",
        "p,p,t,t,p,p\n",
        "p,p,t,t,p,p\n",
        "p,p,p,p,p,b_2\n",
        "p,p,p,p,b_2,h_2\n",
        "```\n",
        "\n",
        "**Runtime:** Use GPU runtime for faster training (Runtime ‚Üí Change runtime type ‚Üí GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üì¶ Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "source": [
        "# Install dependencies\n",
        "# Note: For production use, pin specific versions for reproducibility:\n",
        "# !pip install -q gymnasium==0.29.1 stable-baselines3[extra]==2.0.0 tensorboard==2.14.0\n",
        "!pip install -q gymnasium stable-baselines3[extra] tensorboard pandas numpy torch\n",
        "\n",
        "# Check if GPU is available\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n‚úÖ Using device: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  No GPU detected. Training will be slower. Consider switching to GPU runtime.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone_repo"
      },
      "source": [
        "# Clone the Reinforce Tactics repository\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "if not Path('reinforce-tactics').exists():\n",
        "    print(\"üì• Cloning Reinforce Tactics repository...\")\n",
        "    !git clone https://github.com/kuds/reinforce-tactics.git\n",
        "    print(\"‚úÖ Repository cloned!\")\n",
        "else:\n",
        "    print(\"‚úÖ Repository already cloned\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir('reinforce-tactics')\n",
        "print(f\"\\nüìÇ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# Add to Python path\n",
        "import sys\n",
        "if os.getcwd() not in sys.path:\n",
        "    sys.path.insert(0, os.getcwd())\n",
        "    print(\"‚úÖ Added to Python path\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## üìö Import Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "import_modules"
      },
      "source": [
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import gymnasium as gym\n",
        "\n",
        "# Import Reinforce Tactics environment\n",
        "from reinforcetactics.rl.gym_env import StrategyGameEnv\n",
        "\n",
        "print(\"‚úÖ All modules imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "env_config"
      },
      "source": [
        "## üéØ Environment Configuration\n",
        "\n",
        "We'll create the StrategyGameEnv with:\n",
        "- **Map**: 6x6 beginner map (`maps/1v1/6x6_beginner.csv`)\n",
        "- **Opponent**: SimpleBot (consistent baseline opponent)\n",
        "- **Render Mode**: None (headless mode for fast training)\n",
        "- **Max Steps**: 500 steps per episode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_env"
      },
      "source": [
        "# Create directories for logs and models\n",
        "!mkdir -p logs/ppo_6x6_beginner\n",
        "!mkdir -p models/ppo_6x6_beginner\n",
        "\n",
        "# Environment configuration\n",
        "MAP_FILE = 'maps/1v1/6x6_beginner.csv'\n",
        "LOG_DIR = './logs/ppo_6x6_beginner'\n",
        "MODEL_DIR = './models/ppo_6x6_beginner'\n",
        "\n",
        "def make_env():\n",
        "    \"\"\"Create and wrap the environment.\"\"\"\n",
        "    env = StrategyGameEnv(\n",
        "        map_file=MAP_FILE,\n",
        "        opponent='bot',  # SimpleBot opponent\n",
        "        render_mode=None,  # Headless mode\n",
        "        max_steps=500\n",
        "    )\n",
        "    env = Monitor(env, LOG_DIR)\n",
        "    return env\n",
        "\n",
        "# Create vectorized environment\n",
        "env = DummyVecEnv([make_env])\n",
        "\n",
        "print(\"‚úÖ Environment created successfully!\")\n",
        "print(f\"\\nüìä Environment Details:\")\n",
        "print(f\"   Map: {MAP_FILE}\")\n",
        "print(f\"   Opponent: SimpleBot\")\n",
        "print(f\"   Render Mode: None (headless)\")\n",
        "print(f\"   Observation Space: {env.envs[0].observation_space}\")\n",
        "print(f\"   Action Space: {env.envs[0].action_space}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_config"
      },
      "source": [
        "## ü§ñ PPO Model Configuration\n",
        "\n",
        "We'll configure PPO with the following hyperparameters:\n",
        "- **Policy**: MultiInputPolicy (for Dict observation space)\n",
        "- **Learning Rate**: 3e-4\n",
        "- **Steps per Rollout**: 2048\n",
        "- **Batch Size**: 64\n",
        "- **Epochs**: 10\n",
        "- **Gamma**: 0.99 (discount factor)\n",
        "- **GAE Lambda**: 0.95\n",
        "- **Clip Range**: 0.2\n",
        "- **Entropy Coefficient**: 0.01 (for exploration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_model"
      },
      "source": [
        "# PPO hyperparameters\n",
        "ppo_config = {\n",
        "    'learning_rate': 3e-4,\n",
        "    'n_steps': 2048,\n",
        "    'batch_size': 64,\n",
        "    'n_epochs': 10,\n",
        "    'gamma': 0.99,\n",
        "    'gae_lambda': 0.95,\n",
        "    'clip_range': 0.2,\n",
        "    'ent_coef': 0.01,\n",
        "    'verbose': 1,\n",
        "    'tensorboard_log': LOG_DIR,\n",
        "    'device': device\n",
        "}\n",
        "\n",
        "# Create PPO model\n",
        "model = PPO(\n",
        "    policy='MultiInputPolicy',  # Required for Dict observation space\n",
        "    env=env,\n",
        "    **ppo_config\n",
        ")\n",
        "\n",
        "print(\"‚úÖ PPO model created successfully!\")\n",
        "print(f\"\\nüìä Model Configuration:\")\n",
        "for key, value in ppo_config.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# Print model architecture\n",
        "print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
        "print(model.policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "callbacks"
      },
      "source": [
        "## üíæ Set Up Training Callbacks\n",
        "\n",
        "We'll use callbacks to:\n",
        "- Save model checkpoints every 10,000 steps\n",
        "- Evaluate the model periodically during training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_callbacks"
      },
      "source": [
        "# Checkpoint callback - save model every 10k steps\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=10000,\n",
        "    save_path=MODEL_DIR,\n",
        "    name_prefix='ppo_6x6_beginner',\n",
        "    save_replay_buffer=False,\n",
        "    save_vecnormalize=False,\n",
        ")\n",
        "\n",
        "# Create evaluation environment\n",
        "eval_env = DummyVecEnv([make_env])\n",
        "\n",
        "# Evaluation callback - evaluate every 20k steps\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=MODEL_DIR,\n",
        "    log_path=LOG_DIR,\n",
        "    eval_freq=20000,\n",
        "    n_eval_episodes=5,\n",
        "    deterministic=True,\n",
        "    render=False\n",
        ")\n",
        "\n",
        "# Combine callbacks\n",
        "callbacks = [checkpoint_callback, eval_callback]\n",
        "\n",
        "print(\"‚úÖ Callbacks configured successfully!\")\n",
        "print(f\"   - Checkpoint every 10,000 steps\")\n",
        "print(f\"   - Evaluation every 20,000 steps (5 episodes)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## üèãÔ∏è Training Section\n",
        "\n",
        "Now we'll train the PPO agent. For this demo, we'll train for **100,000 timesteps**.\n",
        "\n",
        "**Note**: For better results, increase to 500,000 - 1,000,000 timesteps. Training time depends on hardware:\n",
        "- With GPU: ~5-10 minutes for 100k steps\n",
        "- Without GPU: ~30-60 minutes for 100k steps\n",
        "\n",
        "You can monitor training progress in TensorBoard by running the cell below after training starts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "start_training"
      },
      "source": [
        "# Training configuration\n",
        "TOTAL_TIMESTEPS = 100000  # Increase to 500k-1M for better results\n",
        "\n",
        "print(f\"üèãÔ∏è  Starting PPO training...\")\n",
        "print(f\"   Total timesteps: {TOTAL_TIMESTEPS:,}\")\n",
        "print(f\"   Device: {device}\")\n",
        "print(f\"   Map: 6x6 beginner\")\n",
        "print(f\"   Opponent: SimpleBot\")\n",
        "print(f\"\\nüìä Training progress will be displayed below...\\n\")\n",
        "\n",
        "# Train the model\n",
        "model.learn(\n",
        "    total_timesteps=TOTAL_TIMESTEPS,\n",
        "    callback=callbacks,\n",
        "    progress_bar=True\n",
        ")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = f\"{MODEL_DIR}/ppo_6x6_beginner_final\"\n",
        "model.save(final_model_path)\n",
        "\n",
        "print(f\"\\n‚úÖ Training complete!\")\n",
        "print(f\"   Final model saved to: {final_model_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tensorboard"
      },
      "source": [
        "## üìà Monitor Training with TensorBoard\n",
        "\n",
        "Launch TensorBoard to visualize training metrics in real-time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "launch_tensorboard"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOG_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "## üéØ Evaluation Section\n",
        "\n",
        "Now let's evaluate the trained model against SimpleBot and calculate the win rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "# Load the trained model\n",
        "print(\"üì• Loading trained model...\")\n",
        "trained_model = PPO.load(final_model_path, env=env)\n",
        "print(\"‚úÖ Model loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evaluate_model"
      },
      "source": [
        "# Evaluate the model\n",
        "print(\"üéØ Evaluating trained model...\\n\")\n",
        "\n",
        "n_eval_episodes = 20\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    trained_model,\n",
        "    eval_env,\n",
        "    n_eval_episodes=n_eval_episodes,\n",
        "    deterministic=True,\n",
        "    return_episode_rewards=False\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Evaluation Results ({n_eval_episodes} episodes):\")\n",
        "print(f\"   Mean Reward: {mean_reward:.2f} ¬± {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "detailed_evaluation"
      },
      "source": [
        "# Detailed evaluation with episode-by-episode results\n",
        "print(\"\\nüîç Detailed Episode-by-Episode Evaluation...\\n\")\n",
        "\n",
        "wins = 0\n",
        "losses = 0\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "\n",
        "for episode in range(n_eval_episodes):\n",
        "    obs = eval_env.reset()\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    episode_length = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = trained_model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, info = eval_env.step(action)\n",
        "        episode_reward += reward[0]\n",
        "        episode_length += 1\n",
        "\n",
        "    # Check winner from info\n",
        "    episode_stats = info[0].get('episode_stats', {})\n",
        "    winner = episode_stats.get('winner', None)\n",
        "\n",
        "    if winner == 1:\n",
        "        wins += 1\n",
        "        result = \"WIN ‚úÖ\"\n",
        "    elif winner == 2:\n",
        "        losses += 1\n",
        "        result = \"LOSS ‚ùå\"\n",
        "    else:\n",
        "        result = \"DRAW ‚öñÔ∏è\"\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "    episode_lengths.append(episode_length)\n",
        "\n",
        "    print(f\"Episode {episode + 1:2d}: {result} | Reward: {episode_reward:7.2f} | Length: {episode_length:3d} steps\")\n",
        "\n",
        "# Calculate statistics\n",
        "win_rate = (wins / n_eval_episodes) * 100\n",
        "avg_reward = np.mean(episode_rewards)\n",
        "avg_length = np.mean(episode_lengths)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(f\"üìä EVALUATION SUMMARY\")\n",
        "print(f\"=\"*60)\n",
        "print(f\"Total Episodes:     {n_eval_episodes}\")\n",
        "print(f\"Wins:               {wins} ({win_rate:.1f}%)\")\n",
        "print(f\"Losses:             {losses} ({(losses/n_eval_episodes)*100:.1f}%)\")\n",
        "print(f\"Average Reward:     {avg_reward:.2f}\")\n",
        "print(f\"Average Length:     {avg_length:.1f} steps\")\n",
        "print(f\"=\"*60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "watch_game"
      },
      "source": [
        "## üëÄ Watch a Game (Optional)\n",
        "\n",
        "If you want to see the agent play, you can run a single episode and print the actions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "watch_single_game"
      },
      "source": [
        "print(\"üéÆ Watching a single game...\\n\")\n",
        "\n",
        "obs = eval_env.reset()\n",
        "done = False\n",
        "step_count = 0\n",
        "total_reward = 0\n",
        "\n",
        "while not done and step_count < 50:  # Limit to 50 steps for display\n",
        "    action, _ = trained_model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "\n",
        "    step_count += 1\n",
        "    total_reward += reward[0]\n",
        "\n",
        "    # Decode action for display\n",
        "    action_types = ['CREATE_UNIT', 'MOVE', 'ATTACK', 'SEIZE', 'HEAL', 'END_TURN']\n",
        "    action_type_idx = action[0][0]\n",
        "    action_type = action_types[action_type_idx] if action_type_idx < len(action_types) else 'UNKNOWN'\n",
        "\n",
        "    print(f\"Step {step_count:2d}: Action={action_type:12s} | Reward={reward[0]:7.2f} | Total={total_reward:7.2f}\")\n",
        "\n",
        "    if done:\n",
        "        episode_stats = info[0].get('episode_stats', {})\n",
        "        winner = episode_stats.get('winner', None)\n",
        "        if winner == 1:\n",
        "            print(\"\\n‚úÖ Agent WON!\")\n",
        "        elif winner == 2:\n",
        "            print(\"\\n‚ùå Agent LOST!\")\n",
        "        else:\n",
        "            print(\"\\n‚öñÔ∏è  Game ended in a draw or timeout\")\n",
        "        break\n",
        "\n",
        "if not done:\n",
        "    print(f\"\\n‚è∏Ô∏è  Game still in progress after {step_count} steps\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_saving"
      },
      "source": [
        "## üíæ Model Saving and Loading\n",
        "\n",
        "Your trained model has been saved automatically. Here's how to load and use it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save_load_instructions"
      },
      "source": [
        "print(\"üì¶ Model Locations:\")\n",
        "print(f\"\\n   Final Model:\")\n",
        "print(f\"   {final_model_path}.zip\")\n",
        "print(f\"\\n   Checkpoints (every 10k steps):\")\n",
        "!ls -lh {MODEL_DIR}/ppo_6x6_beginner_*.zip 2>/dev/null | tail -5 || echo \"   No checkpoints found yet\"\n",
        "print(f\"\\n   Best Model (from evaluation):\")\n",
        "!ls -lh {MODEL_DIR}/best_model.zip 2>/dev/null || echo \"   No best model found yet\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìù How to Load and Use the Model:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n1. Load the model:\")\n",
        "print(f\"   model = PPO.load('{final_model_path}')\")\n",
        "print(\"\\n2. Create environment:\")\n",
        "print(\"   env = StrategyGameEnv(map_file='maps/1v1/6x6_beginner.csv', opponent='bot')\")\n",
        "print(\"\\n3. Use the model:\")\n",
        "print(\"   obs, info = env.reset()\")\n",
        "print(\"   action, _states = model.predict(obs, deterministic=True)\")\n",
        "print(\"   obs, reward, terminated, truncated, info = env.step(action)\")\n",
        "print(\"\\n4. Download from Colab (optional):\")\n",
        "print(\"   from google.colab import files\")\n",
        "print(f\"   files.download('{final_model_path}.zip')\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_model"
      },
      "source": [
        "## ‚¨áÔ∏è Download Model (Optional)\n",
        "\n",
        "If you're running in Colab, you can download the trained model to your local machine:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_model_code"
      },
      "source": [
        "# Uncomment to download the model\n",
        "# from google.colab import files\n",
        "# files.download(f'{final_model_path}.zip')\n",
        "print(\"üí° Uncomment the code above to download the trained model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyperparameter_tuning"
      },
      "source": [
        "## üî¨ Hyperparameter Tuning (Advanced)\n",
        "\n",
        "Try different hyperparameters to improve performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyperparam_suggestions"
      },
      "source": [
        "print(\"üî¨ Suggested Hyperparameter Configurations:\\n\")\n",
        "\n",
        "configs = [\n",
        "    {\n",
        "        'name': 'Baseline (Current)',\n",
        "        'learning_rate': 3e-4,\n",
        "        'n_steps': 2048,\n",
        "        'batch_size': 64,\n",
        "        'ent_coef': 0.01,\n",
        "        'description': 'Standard PPO configuration'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Conservative',\n",
        "        'learning_rate': 1e-4,\n",
        "        'n_steps': 4096,\n",
        "        'batch_size': 128,\n",
        "        'ent_coef': 0.005,\n",
        "        'description': 'Slower, more stable learning'\n",
        "    },\n",
        "    {\n",
        "        'name': 'Aggressive',\n",
        "        'learning_rate': 5e-4,\n",
        "        'n_steps': 1024,\n",
        "        'batch_size': 32,\n",
        "        'ent_coef': 0.02,\n",
        "        'description': 'Faster learning with more exploration'\n",
        "    },\n",
        "    {\n",
        "        'name': 'High Exploration',\n",
        "        'learning_rate': 3e-4,\n",
        "        'n_steps': 2048,\n",
        "        'batch_size': 64,\n",
        "        'ent_coef': 0.05,\n",
        "        'description': 'More exploration for diverse strategies'\n",
        "    }\n",
        "]\n",
        "\n",
        "for i, config in enumerate(configs, 1):\n",
        "    print(f\"{i}. {config['name']}:\")\n",
        "    print(f\"   Description: {config['description']}\")\n",
        "    print(f\"   - learning_rate: {config['learning_rate']}\")\n",
        "    print(f\"   - n_steps: {config['n_steps']}\")\n",
        "    print(f\"   - batch_size: {config['batch_size']}\")\n",
        "    print(f\"   - ent_coef: {config['ent_coef']}\")\n",
        "    print()\n",
        "\n",
        "print(\"üí° To try a different configuration:\")\n",
        "print(\"   1. Modify the ppo_config dictionary in the 'PPO Model Configuration' cell\")\n",
        "print(\"   2. Re-run the training cells\")\n",
        "print(\"   3. Compare results using TensorBoard\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tips"
      },
      "source": [
        "## üí° Tips for Better Training\n",
        "\n",
        "### Performance Optimization\n",
        "1. **Use GPU Runtime**: Change to GPU runtime in Colab for 5-10x faster training\n",
        "   - Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
        "2. **Parallel Environments**: Use `SubprocVecEnv` instead of `DummyVecEnv` for CPU parallelization\n",
        "3. **Increase Batch Size**: If you have enough memory, larger batch sizes can stabilize training\n",
        "\n",
        "### Training Duration\n",
        "1. **Quick Test**: 50k-100k timesteps (~5-10 min on GPU)\n",
        "2. **Decent Agent**: 500k timesteps (~30-60 min on GPU)\n",
        "3. **Strong Agent**: 1M-2M timesteps (~2-4 hours on GPU)\n",
        "\n",
        "### Improving Agent Performance\n",
        "1. **Reward Shaping**: Adjust reward coefficients in the environment configuration\n",
        "2. **Curriculum Learning**: Start with easier opponents, gradually increase difficulty\n",
        "3. **Hyperparameter Tuning**: Try different learning rates, entropy coefficients\n",
        "4. **Longer Rollouts**: Increase `n_steps` for better credit assignment\n",
        "\n",
        "### Monitoring and Debugging\n",
        "1. **TensorBoard**: Monitor loss curves, reward trends, and policy entropy\n",
        "2. **Episode Stats**: Track win rate, average reward, episode length\n",
        "3. **Action Distribution**: Check if agent is exploring enough\n",
        "4. **Invalid Actions**: Monitor the rate of invalid actions\n",
        "\n",
        "## üêõ Troubleshooting\n",
        "\n",
        "| Problem | Solution |\n",
        "|---------|----------|\n",
        "| **Out of Memory** | Reduce `n_steps`, `batch_size`, or number of parallel environments |\n",
        "| **Slow Training** | Enable GPU runtime, use `SubprocVecEnv` |\n",
        "| **Unstable Learning** | Reduce learning rate, increase batch size |\n",
        "| **Not Learning** | Check reward shaping, increase `ent_coef` for more exploration |\n",
        "| **High Invalid Actions** | Implement action masking in the environment |\n",
        "| **Agent Gets Stuck** | Increase entropy coefficient, add reward shaping |\n",
        "\n",
        "## üìö Next Steps\n",
        "\n",
        "1. **Scale Up**: Train on larger maps (10x10, 14x14)\n",
        "2. **Stronger Opponents**: Test against human players or other trained agents\n",
        "3. **Self-Play**: Implement self-play for continual improvement\n",
        "4. **Hierarchical RL**: Use goal-based policies for complex strategies\n",
        "5. **Multi-Agent**: Train multiple agents in competitive scenarios\n",
        "6. **Transfer Learning**: Use the trained model as a starting point for larger maps\n",
        "\n",
        "## üìñ References\n",
        "\n",
        "- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)\n",
        "- [PPO Paper](https://arxiv.org/abs/1707.06347)\n",
        "- [Reinforce Tactics Repository](https://github.com/kuds/reinforce-tactics)\n",
        "- [Gymnasium Documentation](https://gymnasium.farama.org/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}