{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/llm_bot_tournament.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ü§ñ LLM Bot Tournament - Reinforce Tactics\n",
        "\n",
        "Run interactive tournaments between LLM-powered bots (OpenAI GPT, Claude, Gemini) and built-in rule-based bots (SimpleBot, MediumBot, AdvancedBot)!\n",
        "\n",
        "**Features:**\n",
        "- üéÆ Single game runner with detailed turn-by-turn logging\n",
        "- üèÜ Round-robin tournament system with multiple games per matchup\n",
        "- üìä **Elo rating system** for skill-based rankings\n",
        "- üó∫Ô∏è **Multiple maps support** with configurable map pool modes\n",
        "- üîë Flexible API key configuration (environment variables or Google Colab secrets)\n",
        "- üìà Comprehensive statistics: wins, losses, draws, win rates, Elo ratings\n",
        "- üéØ Customizable model selection (GPT-4o, Claude Sonnet, Gemini Pro, etc.)\n",
        "- üìù Conversation logging to debug and analyze LLM reasoning\n",
        "\n",
        "**Supported Bots:**\n",
        "- **SimpleBot**: Basic rule-based bot (always available)\n",
        "- **MediumBot**: Improved rule-based bot with advanced strategies (always available)\n",
        "- **AdvancedBot**: Most sophisticated rule-based bot (always available)\n",
        "- **OpenAIBot**: Uses OpenAI GPT models (requires `OPENAI_API_KEY`)\n",
        "- **ClaudeBot**: Uses Anthropic Claude models (requires `ANTHROPIC_API_KEY`)\n",
        "- **GeminiBot**: Uses Google Gemini models (requires `GOOGLE_API_KEY`)\n",
        "\n",
        "**Quick Start:**\n",
        "1. Install dependencies and clone the repository\n",
        "2. Configure API keys for the bots you want to use\n",
        "3. Run a single game or full tournament\n",
        "4. Analyze the results!\n",
        "\n",
        "**Estimated API Costs:**\n",
        "- **GPT-4o-mini**: ~$0.0001-0.0005 per game (recommended for testing)\n",
        "- **Claude Haiku**: ~$0.0001-0.0003 per game (recommended for testing)\n",
        "- **Gemini Flash**: Free tier available, ~$0.0001 per game\n",
        "- **GPT-4o**: ~$0.005-0.02 per game (stronger play, higher cost)\n",
        "- **Claude Sonnet**: ~$0.003-0.015 per game (stronger play, higher cost)\n",
        "\n",
        "*Costs vary based on game length and map complexity. Use mini/haiku/flash models for testing!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üì¶ Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone_repo"
      },
      "source": [
        "# Clone the Reinforce Tactics repository if not already present\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "if not Path('reinforce-tactics').exists():\n",
        "    print(\"üì• Cloning Reinforce Tactics repository...\")\n",
        "    !git clone https://github.com/kuds/reinforce-tactics.git\n",
        "    print(\"‚úÖ Repository cloned!\")\n",
        "else:\n",
        "    print(\"‚úÖ Repository already cloned\")\n",
        "\n",
        "# Change to repository directory\n",
        "os.chdir('reinforce-tactics')\n",
        "print(f\"\\nüìÇ Current directory: {os.getcwd()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_llm_deps"
      },
      "source": [
        "# Install LLM bot dependencies\n",
        "# These are optional - only install for the bots you plan to use\n",
        "print(\"üì¶ Installing LLM dependencies...\\n\")\n",
        "\n",
        "# Install OpenAI (for GPT models)\n",
        "print(\"Installing OpenAI...\")\n",
        "!pip install -q openai\n",
        "\n",
        "# Install Anthropic (for Claude models)\n",
        "print(\"Installing Anthropic...\")\n",
        "!pip install -q anthropic\n",
        "\n",
        "# Install Google Generative AI (for Gemini models)\n",
        "print(\"Installing Google Gen AI...\")\n",
        "!pip install -q google-genai\n",
        "\n",
        "# Install other dependencies if needed\n",
        "!pip install -q pandas numpy\n",
        "\n",
        "print(\"\\n‚úÖ All LLM dependencies installed!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "add_to_path"
      },
      "source": [
        "# Add repository to Python path\n",
        "import sys\n",
        "repo_path = os.getcwd()\n",
        "if repo_path not in sys.path:\n",
        "    sys.path.insert(0, repo_path)\n",
        "    print(f\"‚úÖ Added to Python path: {repo_path}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Already in Python path: {repo_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import torch\n",
        "from importlib.metadata import version\n",
        "\n",
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"Torch Version: {version('torch')}\")\n",
        "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
        "print(f\"Cuda Version: {torch.version.cuda}\")\n",
        "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
        "print(f\"Numpy Version: {version('numpy')}\")\n",
        "print(f\"OpenAI Version: {version('openai')}\")\n",
        "print(f\"Anthropic Version: {version('anthropic')}\")\n",
        "print(f\"Gemini Version: {version('google.genai')}\")"
      ],
      "metadata": {
        "id": "0jb8deCuTEwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_keys"
      },
      "source": [
        "## üîë API Key Configuration\n",
        "\n",
        "You have two options for setting API keys:\n",
        "\n",
        "### Option 1: Direct Environment Variables (Quick Setup)\n",
        "Set API keys directly in the cells below. **Note:** These will be visible in the notebook.\n",
        "\n",
        "### Option 2: Google Colab Secrets (Recommended for Colab)\n",
        "1. Click the üîë key icon in the left sidebar\n",
        "2. Add secrets with names: `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`\n",
        "3. Toggle \"Notebook access\" on for each secret\n",
        "\n",
        "The code below will check both sources and use Colab secrets if available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "set_api_keys"
      },
      "source": [
        "import os\n",
        "\n",
        "# Try to use Google Colab secrets first\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    print(\"‚úÖ Google Colab detected - checking for secrets...\\n\")\n",
        "\n",
        "    # Try to get OpenAI key from secrets\n",
        "    try:\n",
        "        openai_key = userdata.get('OPENAI_API_KEY')\n",
        "        os.environ['OPENAI_API_KEY'] = openai_key\n",
        "        print(\"‚úÖ OPENAI_API_KEY loaded from Colab secrets\")\n",
        "    except:\n",
        "        if 'OPENAI_API_KEY' not in os.environ:\n",
        "            print(\"‚ö†Ô∏è  OPENAI_API_KEY not found in Colab secrets\")\n",
        "\n",
        "    # Try to get Anthropic key from secrets\n",
        "    try:\n",
        "        anthropic_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "        os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
        "        print(\"‚úÖ ANTHROPIC_API_KEY loaded from Colab secrets\")\n",
        "    except:\n",
        "        if 'ANTHROPIC_API_KEY' not in os.environ:\n",
        "            print(\"‚ö†Ô∏è  ANTHROPIC_API_KEY not found in Colab secrets\")\n",
        "\n",
        "    # Try to get Google key from secrets\n",
        "    try:\n",
        "        google_key = userdata.get('GOOGLE_API_KEY')\n",
        "        os.environ['GOOGLE_API_KEY'] = google_key\n",
        "        print(\"‚úÖ GOOGLE_API_KEY loaded from Colab secrets\")\n",
        "    except:\n",
        "        if 'GOOGLE_API_KEY' not in os.environ:\n",
        "            print(\"‚ö†Ô∏è  GOOGLE_API_KEY not found in Colab secrets\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"‚ÑπÔ∏è  Not running in Google Colab - using environment variables\")\n",
        "\n",
        "# Option 1: Set API keys directly (if not using Colab secrets)\n",
        "# Uncomment and set your keys below if needed:\n",
        "\n",
        "# os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
        "# os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'\n",
        "# os.environ['GOOGLE_API_KEY'] = 'AI...'\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"API Key Status:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"OpenAI:    {'‚úÖ Configured' if os.environ.get('OPENAI_API_KEY') else '‚ùå Not set'}\")\n",
        "print(f\"Anthropic: {'‚úÖ Configured' if os.environ.get('ANTHROPIC_API_KEY') else '‚ùå Not set'}\")\n",
        "print(f\"Google:    {'‚úÖ Configured' if os.environ.get('GOOGLE_API_KEY') else '‚ùå Not set'}\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n‚ÑπÔ∏è  You can run tournaments with any bots that have API keys configured.\")\n",
        "print(\"   SimpleBot, MediumBot, and AdvancedBot are always available and don't require API keys.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "## üìö Import Required Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "import_modules"
      },
      "source": [
        "import logging\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Any, Optional, Tuple, Union\n",
        "\n",
        "# Configure logging to see bot actions\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Import game components\n",
        "from reinforcetactics.core.game_state import GameState\n",
        "from reinforcetactics.game.bot import SimpleBot, MediumBot, AdvancedBot\n",
        "from reinforcetactics.utils.file_io import FileIO\n",
        "\n",
        "# Import LLM bots (with graceful fallback)\n",
        "llm_bots_available = {}\n",
        "\n",
        "try:\n",
        "    from reinforcetactics.game.llm_bot import OpenAIBot\n",
        "    llm_bots_available['openai'] = OpenAIBot\n",
        "    print(\"‚úÖ OpenAIBot available\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è  OpenAIBot not available: {e}\")\n",
        "\n",
        "try:\n",
        "    from reinforcetactics.game.llm_bot import ClaudeBot\n",
        "    llm_bots_available['claude'] = ClaudeBot\n",
        "    print(\"‚úÖ ClaudeBot available\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è  ClaudeBot not available: {e}\")\n",
        "\n",
        "try:\n",
        "    from reinforcetactics.game.llm_bot import GeminiBot\n",
        "    llm_bots_available['gemini'] = GeminiBot\n",
        "    print(\"‚úÖ GeminiBot available\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ö†Ô∏è  GeminiBot not available: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Imports complete! {len(llm_bots_available)} LLM bot types available.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elo_system"
      },
      "source": [
        "## üìä Elo Rating System\n",
        "\n",
        "The Elo rating system tracks bot performance across games, providing a skill-based ranking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elo_rating_class"
      },
      "source": [
        "class EloRatingSystem:\n",
        "    \"\"\"Manages Elo ratings for tournament participants.\"\"\"\n",
        "\n",
        "    def __init__(self, starting_elo: int = 1500, k_factor: int = 32):\n",
        "        \"\"\"\n",
        "        Initialize Elo rating system.\n",
        "\n",
        "        Args:\n",
        "            starting_elo: Initial Elo rating for all bots (default: 1500)\n",
        "            k_factor: K-factor for rating changes (default: 32)\n",
        "        \"\"\"\n",
        "        self.starting_elo = starting_elo\n",
        "        self.k_factor = k_factor\n",
        "        self.ratings: Dict[str, float] = {}\n",
        "        self.initial_ratings: Dict[str, float] = {}\n",
        "        self.rating_history: Dict[str, List[float]] = {}\n",
        "\n",
        "    def initialize_bot(self, bot_name: str) -> None:\n",
        "        \"\"\"Initialize a bot with starting Elo rating.\"\"\"\n",
        "        if bot_name not in self.ratings:\n",
        "            self.ratings[bot_name] = float(self.starting_elo)\n",
        "            self.initial_ratings[bot_name] = float(self.starting_elo)\n",
        "            self.rating_history[bot_name] = [float(self.starting_elo)]\n",
        "\n",
        "    def calculate_expected_score(self, player_elo: float, opponent_elo: float) -> float:\n",
        "        \"\"\"Calculate expected score for a player (0.0 to 1.0).\"\"\"\n",
        "        return 1.0 / (1.0 + 10 ** ((opponent_elo - player_elo) / 400.0))\n",
        "\n",
        "    def update_ratings(self, bot1_name: str, bot2_name: str, result: int) -> None:\n",
        "        \"\"\"\n",
        "        Update Elo ratings after a game.\n",
        "\n",
        "        Args:\n",
        "            bot1_name: Name of first bot\n",
        "            bot2_name: Name of second bot\n",
        "            result: Game result (1=bot1 wins, 2=bot2 wins, 0=draw)\n",
        "        \"\"\"\n",
        "        self.initialize_bot(bot1_name)\n",
        "        self.initialize_bot(bot2_name)\n",
        "\n",
        "        bot1_elo = self.ratings[bot1_name]\n",
        "        bot2_elo = self.ratings[bot2_name]\n",
        "\n",
        "        bot1_expected = self.calculate_expected_score(bot1_elo, bot2_elo)\n",
        "        bot2_expected = self.calculate_expected_score(bot2_elo, bot1_elo)\n",
        "\n",
        "        if result == 1:  # bot1 wins\n",
        "            bot1_actual, bot2_actual = 1.0, 0.0\n",
        "        elif result == 2:  # bot2 wins\n",
        "            bot1_actual, bot2_actual = 0.0, 1.0\n",
        "        else:  # draw\n",
        "            bot1_actual, bot2_actual = 0.5, 0.5\n",
        "\n",
        "        self.ratings[bot1_name] = bot1_elo + self.k_factor * (bot1_actual - bot1_expected)\n",
        "        self.ratings[bot2_name] = bot2_elo + self.k_factor * (bot2_actual - bot2_expected)\n",
        "\n",
        "        self.rating_history[bot1_name].append(self.ratings[bot1_name])\n",
        "        self.rating_history[bot2_name].append(self.ratings[bot2_name])\n",
        "\n",
        "    def get_rating(self, bot_name: str) -> float:\n",
        "        \"\"\"Get current Elo rating for a bot.\"\"\"\n",
        "        return self.ratings.get(bot_name, float(self.starting_elo))\n",
        "\n",
        "    def get_rating_change(self, bot_name: str) -> float:\n",
        "        \"\"\"Get Elo rating change since tournament start.\"\"\"\n",
        "        initial = self.initial_ratings.get(bot_name, float(self.starting_elo))\n",
        "        current = self.ratings.get(bot_name, float(self.starting_elo))\n",
        "        return current - initial\n",
        "\n",
        "print(\"‚úÖ EloRatingSystem class defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "single_game"
      },
      "source": [
        "## üéÆ Single Game Runner\n",
        "\n",
        "Run a single game between two bots with detailed logging."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_single_game_function"
      },
      "source": [
        "def run_single_game(\n",
        "    player1_bot: Union[str, type],\n",
        "    player2_bot: Union[str, type],\n",
        "    map_file: str = 'maps/1v1/6x6_beginner.csv',\n",
        "    max_turns: int = 500,\n",
        "    verbose: bool = True,\n",
        "    player1_model: Optional[str] = None,\n",
        "    player2_model: Optional[str] = None,\n",
        "    player1_temperature: Optional[float] = None,\n",
        "    player2_temperature: Optional[float] = None,\n",
        "    player1_max_tokens: int = 8000,\n",
        "    player2_max_tokens: int = 8000,\n",
        "    player1_should_reason: bool = False,\n",
        "    player2_should_reason: bool = False,\n",
        "    log_conversations: bool = False,\n",
        "    conversation_log_dir: Optional[str] = None,\n",
        "    save_replay: bool = False,\n",
        "    replay_dir: Optional[str] = None\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Run a single game between two bots.\n",
        "\n",
        "    Args:\n",
        "        player1_bot: Bot class or 'simple'/'medium'/'advanced' for built-in bots (plays as Player 1)\n",
        "        player2_bot: Bot class or 'simple'/'medium'/'advanced' for built-in bots (plays as Player 2)\n",
        "        map_file: Path to map file (default: maps/1v1/6x6_beginner.csv)\n",
        "        max_turns: Maximum number of turns to prevent infinite games (default: 500)\n",
        "        verbose: Show turn-by-turn details (default: True)\n",
        "        player1_model: Optional model name for player 1 LLM bot\n",
        "        player2_model: Optional model name for player 2 LLM bot\n",
        "        player1_temperature: Optional temperature for player 1 LLM bot (0.0 to 1.0)\n",
        "        player2_temperature: Optional temperature for player 2 LLM bot (0.0 to 1.0)\n",
        "        player1_max_tokens: Max output tokens for player 1 LLM bot (default: 8000)\n",
        "        player2_max_tokens: Max output tokens for player 2 LLM bot (default: 8000)\n",
        "        player1_should_reason: Include reasoning field in player 1 LLM response format (default: False)\n",
        "        player2_should_reason: Include reasoning field in player 2 LLM response format (default: False)\n",
        "        log_conversations: Enable conversation logging for LLM bots (default: False)\n",
        "        conversation_log_dir: Directory for conversation logs (optional)\n",
        "        save_replay: Whether to save the game replay (default: False)\n",
        "        replay_dir: Directory for replay files (default: llm_replays/)\n",
        "\n",
        "    Returns:\n",
        "        Winner: 1 (player 1 wins), 2 (player 2 wins), or 0 (draw)\n",
        "    \"\"\"\n",
        "    # Load map\n",
        "    map_data = FileIO.load_map(map_file)\n",
        "    if map_data is None:\n",
        "        raise ValueError(f\"Failed to load map: {map_file}\")\n",
        "\n",
        "    # Create game state\n",
        "    game_state = GameState(map_data, num_players=2)\n",
        "\n",
        "    # Store map file reference for replay (required for replay system to load map)\n",
        "    game_state.map_file_used = map_file\n",
        "\n",
        "    # Handle conversation log directory\n",
        "    abs_log_dir = None\n",
        "    log_file_count_before = 0\n",
        "    if log_conversations and conversation_log_dir:\n",
        "        # Convert to absolute path\n",
        "        abs_log_dir = os.path.abspath(conversation_log_dir)\n",
        "        # Create directory if it doesn't exist\n",
        "        try:\n",
        "            os.makedirs(abs_log_dir, exist_ok=True)\n",
        "            if verbose:\n",
        "                print(f\"üìÅ Log directory created/verified: {abs_log_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not create log directory: {e}\")\n",
        "            abs_log_dir = None\n",
        "\n",
        "        # Count existing log files\n",
        "        if abs_log_dir and os.path.exists(abs_log_dir):\n",
        "            log_file_count_before = len([f for f in os.listdir(abs_log_dir) if f.endswith('.json')])\n",
        "\n",
        "    # Handle replay directory\n",
        "    abs_replay_dir = None\n",
        "    if save_replay:\n",
        "        # Use default directory if not specified\n",
        "        if replay_dir is None:\n",
        "            replay_dir = 'llm_replays'\n",
        "        # Convert to absolute path\n",
        "        abs_replay_dir = os.path.abspath(replay_dir)\n",
        "        # Create directory if it doesn't exist\n",
        "        try:\n",
        "            os.makedirs(abs_replay_dir, exist_ok=True)\n",
        "            if verbose:\n",
        "                print(f\"üìÅ Replay directory created/verified: {abs_replay_dir}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not create replay directory: {e}\")\n",
        "            abs_replay_dir = None\n",
        "            save_replay = False\n",
        "\n",
        "    # Create bot instances\n",
        "    def create_bot(bot_spec, player_num, model=None, temperature=None, max_tokens=None, should_reason=False):\n",
        "        if bot_spec == 'simple' or bot_spec is None:\n",
        "            return SimpleBot(game_state, player_num)\n",
        "        elif bot_spec == 'medium':\n",
        "            return MediumBot(game_state, player_num)\n",
        "        elif bot_spec == 'advanced':\n",
        "            return AdvancedBot(game_state, player_num)\n",
        "        else:\n",
        "            # It's an LLM bot class\n",
        "            kwargs = {'game_state': game_state, 'player': player_num}\n",
        "            if model:\n",
        "                kwargs['model'] = model\n",
        "            if temperature is not None:\n",
        "                kwargs['temperature'] = temperature\n",
        "            if max_tokens is not None:\n",
        "                kwargs['max_tokens'] = max_tokens\n",
        "            kwargs['should_reason'] = should_reason\n",
        "            if log_conversations:\n",
        "                kwargs['log_conversations'] = log_conversations\n",
        "            if abs_log_dir:\n",
        "                kwargs['conversation_log_dir'] = abs_log_dir\n",
        "            return bot_spec(**kwargs)\n",
        "\n",
        "    bot1 = create_bot(player1_bot, 1, player1_model, player1_temperature, player1_max_tokens, player1_should_reason)\n",
        "    bot2 = create_bot(player2_bot, 2, player2_model, player2_temperature, player2_max_tokens, player2_should_reason)\n",
        "    bots = {1: bot1, 2: bot2}\n",
        "\n",
        "    # Get bot names\n",
        "    bot1_name = bot1.__class__.__name__\n",
        "    bot2_name = bot2.__class__.__name__\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"Game Start: {bot1_name} (P1) vs {bot2_name} (P2)\")\n",
        "        print(f\"Map: {map_file}\")\n",
        "        if player1_should_reason or player2_should_reason:\n",
        "            reasoning_info = []\n",
        "            if player1_should_reason:\n",
        "                reasoning_info.append(\"P1\")\n",
        "            if player2_should_reason:\n",
        "                reasoning_info.append(\"P2\")\n",
        "            print(f\"Reasoning Enabled: {', '.join(reasoning_info)}\")\n",
        "        if player1_temperature is not None or player2_temperature is not None:\n",
        "            temp_info = []\n",
        "            if player1_temperature is not None: temp_info.append(f\"P1={player1_temperature}\")\n",
        "            if player2_temperature is not None: temp_info.append(f\"P2={player2_temperature}\")\n",
        "            print(f\"Temperature: {', '.join(temp_info)}\")\n",
        "        if log_conversations:\n",
        "            print(f\"Conversation Logging: ENABLED\")\n",
        "            if abs_log_dir:\n",
        "                print(f\"üìÅ Absolute Log Path: {abs_log_dir}\")\n",
        "        if save_replay:\n",
        "            print(f\"Replay Saving: ENABLED\")\n",
        "            if abs_replay_dir:\n",
        "                print(f\"üé¨ Replay Directory: {abs_replay_dir}\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    # Play the game\n",
        "    turn_count = 0\n",
        "    last_gold = {1: game_state.player_gold[1], 2: game_state.player_gold[2]}\n",
        "\n",
        "    # Import time for retry delays\n",
        "    import time\n",
        "\n",
        "    while not game_state.game_over and turn_count < max_turns:\n",
        "        current_player = game_state.current_player\n",
        "        current_bot = bots[current_player]\n",
        "        bot_name = bot1_name if current_player == 1 else bot2_name\n",
        "\n",
        "        # Identify if current bot is an LLM bot\n",
        "        is_llm_bot = current_bot.__class__.__name__ in ['OpenAIBot', 'ClaudeBot', 'GeminiBot']\n",
        "\n",
        "        # Show turn info\n",
        "        if verbose:\n",
        "            print(f\"\\n--- Turn {turn_count + 1} - {bot_name} (P{current_player}) ---\")\n",
        "            print(f\"  Gold: P1={game_state.player_gold[1]}, P2={game_state.player_gold[2]}\")\n",
        "\n",
        "        if is_llm_bot:\n",
        "            # Bot takes turn with retry logic for API limits (ONLY for LLM bots)\n",
        "            max_retries = 3\n",
        "            retry_delay = 5\n",
        "\n",
        "            for attempt in range(max_retries + 1):\n",
        "                try:\n",
        "                    current_bot.take_turn()\n",
        "                    # ALWAYS wait 0.5s for LLM bots\n",
        "                    time.sleep(0.5)\n",
        "                    break # Success\n",
        "                except Exception as e:\n",
        "                    error_str = str(e)\n",
        "                    # Check for rate limit errors (429) or overloaded errors (503/529)\n",
        "                    is_rate_limit = \"429\" in error_str or \"Too Many Requests\" in error_str or \"Overloaded\" in error_str\n",
        "\n",
        "                    if is_rate_limit and attempt < max_retries:\n",
        "                        wait_time = retry_delay * (2 ** attempt) # Exponential backoff\n",
        "                        if verbose:\n",
        "                            print(f\"‚ö†Ô∏è  Rate limit/API error during {bot_name} turn: {e}\")\n",
        "                            print(f\"‚è≥ Waiting {wait_time}s before retry ({attempt+1}/{max_retries})...\")\n",
        "                        else:\n",
        "                            print(f\"‚è≥ Rate limit hit. Waiting {wait_time}s...\")\n",
        "                        time.sleep(wait_time)\n",
        "                    else:\n",
        "                        # Final failure or non-retriable error\n",
        "                        print(f\"‚ö†Ô∏è  Fatal error during {bot_name} turn: {e}\")\n",
        "                        # Bot forfeits on error\n",
        "                        game_state.game_over = True\n",
        "                        game_state.winner = 1 if current_player == 2 else 2\n",
        "                        break\n",
        "        else:\n",
        "            # Regular bot take turn (no retries, no forced delays)\n",
        "            try:\n",
        "                current_bot.take_turn()\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Fatal error during {bot_name} turn: {e}\")\n",
        "                game_state.game_over = True\n",
        "                game_state.winner = 1 if current_player == 2 else 2\n",
        "\n",
        "        # Show gold changes\n",
        "        if verbose and not game_state.game_over:\n",
        "            gold_change = game_state.player_gold[current_player] - last_gold[current_player]\n",
        "            if gold_change != 0:\n",
        "                print(f\"  Gold change: {gold_change:+d}\")\n",
        "            last_gold[current_player] = game_state.player_gold[current_player]\n",
        "\n",
        "        turn_count += 1\n",
        "\n",
        "        # Check for game over\n",
        "        if game_state.game_over:\n",
        "            break\n",
        "\n",
        "    # Determine winner\n",
        "    if game_state.game_over and game_state.winner:\n",
        "        winner = game_state.winner\n",
        "        winner_name = bot1_name if winner == 1 else bot2_name\n",
        "    elif turn_count >= max_turns:\n",
        "        winner = 0\n",
        "        winner_name = \"Draw (max turns)\"\n",
        "    else:\n",
        "        winner = 0\n",
        "        winner_name = \"Draw\"\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"Game Over! Winner: {winner_name}\")\n",
        "        print(f\"Total turns: {turn_count}\")\n",
        "        print(f\"Final gold - P1: {game_state.player_gold[1]}, P2: {game_state.player_gold[2]}\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "    # Save replay if enabled\n",
        "    if save_replay and abs_replay_dir:\n",
        "        from datetime import datetime\n",
        "        from pathlib import Path\n",
        "\n",
        "        # Generate descriptive filename\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        # Build bot descriptions for filename\n",
        "        def get_bot_desc(bot_name, model):\n",
        "            if model:\n",
        "                # Clean model name for filename (sanitize to alphanumeric, dash, underscore)\n",
        "                import re\n",
        "                clean_model = re.sub(r'[^\\w\\-]', '-', model)\n",
        "                return f\"{bot_name}-{clean_model}\"\n",
        "            return bot_name\n",
        "\n",
        "        bot1_desc = get_bot_desc(bot1_name, player1_model)\n",
        "        bot2_desc = get_bot_desc(bot2_name, player2_model)\n",
        "\n",
        "        # Winner description\n",
        "        if winner == 1:\n",
        "            winner_desc = \"P1wins\"\n",
        "        elif winner == 2:\n",
        "            winner_desc = \"P2wins\"\n",
        "        else:\n",
        "            winner_desc = \"Draw\"\n",
        "\n",
        "        replay_filename = f\"game_{timestamp}_{bot1_desc}_vs_{bot2_desc}_{winner_desc}.json\"\n",
        "        replay_path = Path(abs_replay_dir) / replay_filename\n",
        "\n",
        "        try:\n",
        "            saved_path = game_state.save_replay_to_file(str(replay_path))\n",
        "            if saved_path and verbose:\n",
        "                print(f\"üé¨ Replay saved: {replay_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Error saving replay: {e}\")\n",
        "\n",
        "    # --- Force Save and Cleanup for Logs ---\n",
        "    # Ensure bots save their logs before we check for files\n",
        "    if log_conversations:\n",
        "        for bot in bots.values():\n",
        "            # Try common save method names if they exist\n",
        "            if hasattr(bot, 'save_conversation_log'):\n",
        "                try:\n",
        "                    bot.save_conversation_log()\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ö†Ô∏è Error saving bot logs: {e}\")\n",
        "\n",
        "    # Force cleanup of bot instances to trigger any __del__ log saving\n",
        "    del bot1\n",
        "    del bot2\n",
        "    del bots\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    # ----------------------------------------\n",
        "\n",
        "    # Show log file summary\n",
        "    if log_conversations and abs_log_dir and os.path.exists(abs_log_dir):\n",
        "        log_files = [f for f in os.listdir(abs_log_dir) if f.endswith('.json')]\n",
        "        new_log_count = len(log_files) - log_file_count_before\n",
        "\n",
        "        # If verbose is OFF, we still want a small indicator if logs were saved\n",
        "        if new_log_count > 0:\n",
        "            if verbose:\n",
        "                print(\"\\n\" + \"=\"*60)\n",
        "                print(\"üìä Conversation Logs Summary\")\n",
        "                print(\"=\"*60)\n",
        "                print(f\"‚úÖ Generated {new_log_count} new log file(s)\")\n",
        "                print(f\"üìÅ Absolute path: {abs_log_dir}\")\n",
        "                print(f\"üìù Total log files in directory: {len(log_files)}\")\n",
        "                print(f\"\\nüí° To list log files, run:\")\n",
        "                print(f\"   !ls -lh {abs_log_dir}\")\n",
        "                print(\"=\"*60 + \"\\n\")\n",
        "            else:\n",
        "                # Minimal output for tournaments\n",
        "                print(f\"    üìù +{new_log_count} log(s) saved\")\n",
        "\n",
        "    return winner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tournament"
      },
      "source": [
        "## üèÜ Tournament Runner\n",
        "\n",
        "Run a round-robin tournament between multiple bots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_tournament_function"
      },
      "source": [
        "class TournamentBot:\n",
        "    \"\"\"Configuration for a bot participating in a tournament.\"\"\"\n",
        "    def __init__(self, name: str, bot_class: Union[str, type], model: Optional[str] = None,\n",
        "                 temperature: Optional[float] = None, max_tokens: int = 8000):\n",
        "        self.name = name\n",
        "        self.bot_class = bot_class\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "\n",
        "def run_tournament(\n",
        "    bots: List[TournamentBot],\n",
        "    map_file: str = 'maps/1v1/6x6_beginner.csv',\n",
        "    maps: Optional[List[str]] = None,\n",
        "    map_pool_mode: str = 'all',\n",
        "    games_per_matchup: int = 2,\n",
        "    max_turns: int = 500,\n",
        "    should_reason: bool = False,\n",
        "    log_conversations: bool = False,\n",
        "    conversation_log_dir: Optional[str] = None,\n",
        "    save_replays: bool = False,\n",
        "    replay_dir: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run a round-robin tournament between multiple bots with Elo ratings.\n",
        "\n",
        "    Args:\n",
        "        bots: List of TournamentBot objects describing each bot.\n",
        "        map_file: Path to single map file (default: maps/1v1/6x6_beginner.csv)\n",
        "        maps: List of map file paths to use (takes precedence over map_file)\n",
        "        map_pool_mode: How to select maps - 'all' (play every map), 'cycle', or 'random'\n",
        "        games_per_matchup: Games per side per map (total = 2 * games_per_matchup * num_maps if 'all')\n",
        "        max_turns: Maximum turns per game (default: 500)\n",
        "        should_reason: Include reasoning field in LLM response format for all LLM bots (default: False).\n",
        "            When True, LLM bots will explain their strategic reasoning in each response.\n",
        "        log_conversations: Enable conversation logging for LLM bots (default: False)\n",
        "        conversation_log_dir: Directory for conversation logs (optional)\n",
        "        save_replays: Whether to save replays for all games (default: False)\n",
        "        replay_dir: Directory for replay files (default: llm_replays/)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with tournament results, standings, and Elo ratings\n",
        "    \"\"\"\n",
        "    import time\n",
        "\n",
        "    if len(bots) < 2:\n",
        "        raise ValueError(\"Need at least 2 bots for a tournament\")\n",
        "\n",
        "    # Handle map list\n",
        "    if maps:\n",
        "        map_list = maps\n",
        "    else:\n",
        "        map_list = [map_file]\n",
        "\n",
        "    # Initialize Elo rating system\n",
        "    elo_system = EloRatingSystem()\n",
        "    for bot in bots:\n",
        "        elo_system.initialize_bot(bot.name)\n",
        "\n",
        "    # Handle conversation log directory\n",
        "    abs_log_dir = None\n",
        "    initial_log_count = 0\n",
        "    if log_conversations and conversation_log_dir:\n",
        "        # Convert to absolute path\n",
        "        abs_log_dir = os.path.abspath(conversation_log_dir)\n",
        "        # Create directory if it doesn't exist\n",
        "        try:\n",
        "            os.makedirs(abs_log_dir, exist_ok=True)\n",
        "            print(f\"üìÅ Log directory created/verified: {abs_log_dir}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not create log directory: {e}\")\n",
        "            abs_log_dir = None\n",
        "\n",
        "        # Count existing log files (recursively)\n",
        "        if abs_log_dir and os.path.exists(abs_log_dir):\n",
        "            for root, dirs, files in os.walk(abs_log_dir):\n",
        "                initial_log_count += len([f for f in files if f.endswith('.json')])\n",
        "\n",
        "    # Handle replay directory\n",
        "    abs_replay_dir = None\n",
        "    initial_replay_count = 0\n",
        "    if save_replays:\n",
        "        # Use default directory if not specified\n",
        "        if replay_dir is None:\n",
        "            replay_dir = 'llm_replays'\n",
        "        # Convert to absolute path\n",
        "        abs_replay_dir = os.path.abspath(replay_dir)\n",
        "        # Create directory if it doesn't exist\n",
        "        try:\n",
        "            os.makedirs(abs_replay_dir, exist_ok=True)\n",
        "            print(f\"üé¨ Replay directory created/verified: {abs_replay_dir}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  Warning: Could not create replay directory: {e}\")\n",
        "            abs_replay_dir = None\n",
        "            save_replays = False\n",
        "\n",
        "        # Count existing replay files (recursively)\n",
        "        if abs_replay_dir and os.path.exists(abs_replay_dir):\n",
        "            for root, dirs, files in os.walk(abs_replay_dir):\n",
        "                initial_replay_count += len([f for f in files if f.endswith('.json')])\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"üèÜ TOURNAMENT START\")\n",
        "    print(\"=\"*70)\n",
        "    if len(map_list) == 1:\n",
        "        print(f\"Map: {map_list[0]}\")\n",
        "    else:\n",
        "        print(f\"Maps: {len(map_list)} maps\")\n",
        "        for m in map_list:\n",
        "            print(f\"  - {m}\")\n",
        "        print(f\"Map Pool Mode: {map_pool_mode}\")\n",
        "    print(f\"Participants: {len(bots)}\")\n",
        "\n",
        "    for bot in bots:\n",
        "        model_str = f\" ({bot.model})\" if bot.model else \"\"\n",
        "        temp_str = f\" [temp={bot.temperature}]\" if bot.temperature is not None else \"\"\n",
        "\n",
        "        if bot.bot_class == 'simple':\n",
        "            bot_type_str = \"SimpleBot\"\n",
        "        elif bot.bot_class == 'medium':\n",
        "            bot_type_str = \"MediumBot\"\n",
        "        elif bot.bot_class == 'advanced':\n",
        "            bot_type_str = \"AdvancedBot\"\n",
        "        else:\n",
        "            bot_type_str = bot.bot_class.__name__\n",
        "        print(f\"  - {bot.name}: {bot_type_str}{model_str}{temp_str}\")\n",
        "\n",
        "    # Calculate total games\n",
        "    if map_pool_mode == 'all' and len(map_list) > 1:\n",
        "        games_per_matchup_total = games_per_matchup * 2 * len(map_list)\n",
        "    else:\n",
        "        games_per_matchup_total = games_per_matchup * 2\n",
        "\n",
        "    print(f\"Games per matchup: {games_per_matchup_total}\")\n",
        "    if should_reason:\n",
        "        print(f\"LLM Reasoning: ENABLED (bots will explain their strategy)\")\n",
        "    if log_conversations:\n",
        "        print(f\"LLM Conversation Logging: ENABLED\")\n",
        "        if abs_log_dir:\n",
        "            print(f\"üìÅ Absolute Log Path: {abs_log_dir}\")\n",
        "            print(f\"üìÇ Logs will be grouped by map/matchup\")\n",
        "    if save_replays:\n",
        "        print(f\"Replay Saving: ENABLED\")\n",
        "        if abs_replay_dir:\n",
        "            print(f\"üé¨ Replay Directory: {abs_replay_dir}\")\n",
        "            print(f\"üìÇ Replays will be grouped by map/matchup\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Initialize results tracking\n",
        "    results = defaultdict(lambda: {'wins': 0, 'losses': 0, 'draws': 0})\n",
        "    per_map_stats = defaultdict(lambda: defaultdict(lambda: {'wins': 0, 'losses': 0, 'draws': 0}))\n",
        "    matchup_details = []\n",
        "    current_map_index = 0\n",
        "\n",
        "    # Helper function to select map\n",
        "    def select_map(game_num):\n",
        "        nonlocal current_map_index\n",
        "        if len(map_list) == 1:\n",
        "            return map_list[0]\n",
        "        if map_pool_mode == 'cycle':\n",
        "            selected = map_list[current_map_index % len(map_list)]\n",
        "            current_map_index += 1\n",
        "            return selected\n",
        "        elif map_pool_mode == 'random':\n",
        "            import random\n",
        "            return random.choice(map_list)\n",
        "        else:  # 'all' mode - handled separately\n",
        "            return map_list[(game_num - 1) % len(map_list)]\n",
        "\n",
        "    # Generate all matchups (round-robin)\n",
        "    matchups = []\n",
        "    for i in range(len(bots)):\n",
        "        for j in range(i + 1, len(bots)):\n",
        "            matchups.append((i, j))\n",
        "\n",
        "    total_games = len(matchups) * games_per_matchup_total\n",
        "    print(f\"üìä Total matchups: {len(matchups)}\")\n",
        "    print(f\"üìä Total games: {total_games}\\n\")\n",
        "\n",
        "    game_num = 0\n",
        "\n",
        "    # Known LLM bot class names for detecting if we should log/retry\n",
        "    known_llm_bots = ['OpenAIBot', 'ClaudeBot', 'GeminiBot']\n",
        "\n",
        "    # Run all matchups\n",
        "    for matchup_idx, (i, j) in enumerate(matchups, 1):\n",
        "        bot1 = bots[i]\n",
        "        bot2 = bots[j]\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Matchup {matchup_idx}/{len(matchups)}: {bot1.name} vs {bot2.name}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Check if matchup involves LLM bots to decide on logging/delays\n",
        "        has_llm_bot = False\n",
        "        bot1_is_llm = False\n",
        "        bot2_is_llm = False\n",
        "        if not isinstance(bot1.bot_class, str) and bot1.bot_class.__name__ in known_llm_bots:\n",
        "            has_llm_bot = True\n",
        "            bot1_is_llm = True\n",
        "        if not isinstance(bot2.bot_class, str) and bot2.bot_class.__name__ in known_llm_bots:\n",
        "            has_llm_bot = True\n",
        "            bot2_is_llm = True\n",
        "\n",
        "        matchup_results = {\n",
        "            'bot1': bot1.name,\n",
        "            'bot2': bot2.name,\n",
        "            'bot1_wins': 0,\n",
        "            'bot2_wins': 0,\n",
        "            'draws': 0\n",
        "        }\n",
        "\n",
        "        # Determine which maps to play\n",
        "        if map_pool_mode == 'all' and len(map_list) > 1:\n",
        "            # Play all maps for each side\n",
        "            maps_to_play = map_list\n",
        "        else:\n",
        "            # Single map or cycle/random mode\n",
        "            maps_to_play = [None]  # Will select dynamically\n",
        "\n",
        "        for current_map in maps_to_play:\n",
        "            # Define matchup folder name (consistent for both sides)\n",
        "            matchup_folder = f\"{bot1.name}_vs_{bot2.name}\"\n",
        "\n",
        "            # Play games_per_matchup with bot1 as player 1\n",
        "            for game in range(games_per_matchup):\n",
        "                game_num += 1\n",
        "                selected_map = current_map if current_map else select_map(game_num)\n",
        "                map_name = os.path.basename(selected_map)\n",
        "                print(f\"\\n  Game {game_num}/{total_games}: {bot1.name} (P1) vs {bot2.name} (P2) on {map_name}\")\n",
        "\n",
        "                # Determine subdirectory for map and matchup\n",
        "                map_subdir = os.path.join(os.path.splitext(map_name)[0], matchup_folder)\n",
        "\n",
        "                # Configure specific directories\n",
        "                game_log_dir = None\n",
        "                # Only create conversation log dir if enabled AND we have LLM bots\n",
        "                if log_conversations and has_llm_bot and (abs_log_dir or conversation_log_dir):\n",
        "                    base_log = abs_log_dir if abs_log_dir else conversation_log_dir\n",
        "                    game_log_dir = os.path.join(base_log, map_subdir)\n",
        "\n",
        "                game_replay_dir = None\n",
        "                if save_replays and (abs_replay_dir or replay_dir):\n",
        "                    base_replay = abs_replay_dir if abs_replay_dir else replay_dir\n",
        "                    game_replay_dir = os.path.join(base_replay, map_subdir)\n",
        "\n",
        "                winner = run_single_game(\n",
        "                    bot1.bot_class, bot2.bot_class,\n",
        "                    map_file=selected_map,\n",
        "                    max_turns=max_turns,\n",
        "                    verbose=False,\n",
        "                    player1_model=bot1.model,\n",
        "                    player2_model=bot2.model,\n",
        "                    player1_temperature=bot1.temperature,\n",
        "                    player2_temperature=bot2.temperature,\n",
        "                    player1_max_tokens=bot1.max_tokens,\n",
        "                    player2_max_tokens=bot2.max_tokens,\n",
        "                    player1_should_reason=should_reason if bot1_is_llm else False,\n",
        "                    player2_should_reason=should_reason if bot2_is_llm else False,\n",
        "                    log_conversations=log_conversations if has_llm_bot else False,\n",
        "                    conversation_log_dir=game_log_dir,\n",
        "                    save_replay=save_replays,\n",
        "                    replay_dir=game_replay_dir\n",
        "                )\n",
        "\n",
        "                if winner == 1:\n",
        "                    results[bot1.name]['wins'] += 1\n",
        "                    results[bot2.name]['losses'] += 1\n",
        "                    matchup_results['bot1_wins'] += 1\n",
        "                    per_map_stats[bot1.name][map_name]['wins'] += 1\n",
        "                    per_map_stats[bot2.name][map_name]['losses'] += 1\n",
        "                    elo_system.update_ratings(bot1.name, bot2.name, 1)\n",
        "                    print(f\"    ‚úÖ {bot1.name} wins!\")\n",
        "                elif winner == 2:\n",
        "                    results[bot2.name]['wins'] += 1\n",
        "                    results[bot1.name]['losses'] += 1\n",
        "                    matchup_results['bot2_wins'] += 1\n",
        "                    per_map_stats[bot2.name][map_name]['wins'] += 1\n",
        "                    per_map_stats[bot1.name][map_name]['losses'] += 1\n",
        "                    elo_system.update_ratings(bot1.name, bot2.name, 2)\n",
        "                    print(f\"    ‚úÖ {bot2.name} wins!\")\n",
        "                else:\n",
        "                    results[bot1.name]['draws'] += 1\n",
        "                    results[bot2.name]['draws'] += 1\n",
        "                    matchup_results['draws'] += 1\n",
        "                    per_map_stats[bot1.name][map_name]['draws'] += 1\n",
        "                    per_map_stats[bot2.name][map_name]['draws'] += 1\n",
        "                    elo_system.update_ratings(bot1.name, bot2.name, 0)\n",
        "                    print(f\"    ‚öñÔ∏è  Draw\")\n",
        "\n",
        "                # Small delay to prevent rate limits (only for LLM matches)\n",
        "                if has_llm_bot:\n",
        "                    time.sleep(2)\n",
        "\n",
        "            # Play games_per_matchup with bot2 as player 1 (swap sides)\n",
        "            for game in range(games_per_matchup):\n",
        "                game_num += 1\n",
        "                selected_map = current_map if current_map else select_map(game_num)\n",
        "                map_name = os.path.basename(selected_map)\n",
        "                print(f\"\\n  Game {game_num}/{total_games}: {bot2.name} (P1) vs {bot1.name} (P2) on {map_name}\")\n",
        "\n",
        "                # Determine subdirectory for map and matchup\n",
        "                # Note: Using consistent matchup folder name from outer loop logic\n",
        "                map_subdir = os.path.join(os.path.splitext(map_name)[0], matchup_folder)\n",
        "\n",
        "                # Configure specific directories\n",
        "                game_log_dir = None\n",
        "                # Only create conversation log dir if enabled AND we have LLM bots\n",
        "                if log_conversations and has_llm_bot and (abs_log_dir or conversation_log_dir):\n",
        "                    base_log = abs_log_dir if abs_log_dir else conversation_log_dir\n",
        "                    game_log_dir = os.path.join(base_log, map_subdir)\n",
        "\n",
        "                game_replay_dir = None\n",
        "                if save_replays and (abs_replay_dir or replay_dir):\n",
        "                    base_replay = abs_replay_dir if abs_replay_dir else replay_dir\n",
        "                    game_replay_dir = os.path.join(base_replay, map_subdir)\n",
        "\n",
        "                winner = run_single_game(\n",
        "                    bot2.bot_class, bot1.bot_class,\n",
        "                    map_file=selected_map,\n",
        "                    max_turns=max_turns,\n",
        "                    verbose=False,\n",
        "                    player1_model=bot2.model,\n",
        "                    player2_model=bot1.model,\n",
        "                    player1_temperature=bot2.temperature,\n",
        "                    player2_temperature=bot1.temperature,\n",
        "                    player1_max_tokens=bot2.max_tokens,\n",
        "                    player2_max_tokens=bot1.max_tokens,\n",
        "                    player1_should_reason=should_reason if bot2_is_llm else False,\n",
        "                    player2_should_reason=should_reason if bot1_is_llm else False,\n",
        "                    log_conversations=log_conversations if has_llm_bot else False,\n",
        "                    conversation_log_dir=game_log_dir,\n",
        "                    save_replay=save_replays,\n",
        "                    replay_dir=game_replay_dir\n",
        "                )\n",
        "\n",
        "                if winner == 1:\n",
        "                    results[bot2.name]['wins'] += 1\n",
        "                    results[bot1.name]['losses'] += 1\n",
        "                    matchup_results['bot2_wins'] += 1\n",
        "                    per_map_stats[bot2.name][map_name]['wins'] += 1\n",
        "                    per_map_stats[bot1.name][map_name]['losses'] += 1\n",
        "                    elo_system.update_ratings(bot2.name, bot1.name, 1)\n",
        "                    print(f\"    ‚úÖ {bot2.name} wins!\")\n",
        "                elif winner == 2:\n",
        "                    results[bot1.name]['wins'] += 1\n",
        "                    results[bot2.name]['losses'] += 1\n",
        "                    matchup_results['bot1_wins'] += 1\n",
        "                    per_map_stats[bot1.name][map_name]['wins'] += 1\n",
        "                    per_map_stats[bot2.name][map_name]['losses'] += 1\n",
        "                    elo_system.update_ratings(bot2.name, bot1.name, 2)\n",
        "                    print(f\"    ‚úÖ {bot1.name} wins!\")\n",
        "                else:\n",
        "                    results[bot1.name]['draws'] += 1\n",
        "                    results[bot2.name]['draws'] += 1\n",
        "                    matchup_results['draws'] += 1\n",
        "                    per_map_stats[bot1.name][map_name]['draws'] += 1\n",
        "                    per_map_stats[bot2.name][map_name]['draws'] += 1\n",
        "                    elo_system.update_ratings(bot2.name, bot1.name, 0)\n",
        "                    print(f\"    ‚öñÔ∏è  Draw\")\n",
        "\n",
        "                # Small delay to prevent rate limits (only for LLM matches)\n",
        "                if has_llm_bot:\n",
        "                    time.sleep(2)\n",
        "\n",
        "        # Show matchup summary\n",
        "        print(f\"\\n  Matchup Summary: {bot1.name} {matchup_results['bot1_wins']}-{matchup_results['bot2_wins']}-{matchup_results['draws']} {bot2.name}\")\n",
        "        matchup_details.append(matchup_results)\n",
        "\n",
        "    # Calculate final standings with Elo\n",
        "    standings = []\n",
        "    for bot_name, stats in results.items():\n",
        "        total_games_played = stats['wins'] + stats['losses'] + stats['draws']\n",
        "        win_rate = stats['wins'] / total_games_played if total_games_played > 0 else 0.0\n",
        "        elo = elo_system.get_rating(bot_name)\n",
        "        elo_change = elo_system.get_rating_change(bot_name)\n",
        "\n",
        "        standings.append({\n",
        "            'name': bot_name,\n",
        "            'wins': stats['wins'],\n",
        "            'losses': stats['losses'],\n",
        "            'draws': stats['draws'],\n",
        "            'total': total_games_played,\n",
        "            'win_rate': win_rate,\n",
        "            'elo': round(elo, 0),\n",
        "            'elo_change': round(elo_change, 0)\n",
        "        })\n",
        "\n",
        "    # Sort by Elo rating (descending)\n",
        "    standings.sort(key=lambda x: x['elo'], reverse=True)\n",
        "\n",
        "    # Display final standings\n",
        "    print(\"\\n\\n\" + \"=\"*85)\n",
        "    print(\"üèÜ FINAL STANDINGS\")\n",
        "    print(\"=\"*85)\n",
        "    print(f\"{'Rank':<6}{'Bot':<20}{'Wins':<8}{'Losses':<8}{'Draws':<8}{'Win Rate':<10}{'Elo':<8}{'Œî Elo':<8}\")\n",
        "    print(\"-\"*85)\n",
        "\n",
        "    for rank, standing in enumerate(standings, 1):\n",
        "        medal = \"ü•á\" if rank == 1 else (\"ü•à\" if rank == 2 else (\"ü•â\" if rank == 3 else \"  \"))\n",
        "        elo_change_str = f\"{standing['elo_change']:+.0f}\"\n",
        "        print(f\"{medal} {rank:<3}{standing['name']:<20}{standing['wins']:<8}{standing['losses']:<8}\"\n",
        "              f\"{standing['draws']:<8}{standing['win_rate']:.3f}{'':3}{standing['elo']:<8.0f}{elo_change_str:<8}\")\n",
        "\n",
        "    print(\"=\"*85 + \"\\n\")\n",
        "\n",
        "    # Show per-map performance if multiple maps were used\n",
        "    if len(map_list) > 1:\n",
        "        print(\"=\"*85)\n",
        "        print(\"üìà Per-Map Performance\")\n",
        "        print(\"=\"*85)\n",
        "        for m in map_list:\n",
        "            map_name = os.path.basename(m)\n",
        "            print(f\"\\nüó∫Ô∏è  {map_name}\")\n",
        "            # Find best performer on this map\n",
        "            map_wins = {}\n",
        "            for bot_name in per_map_stats:\n",
        "                if map_name in per_map_stats[bot_name]:\n",
        "                    map_wins[bot_name] = per_map_stats[bot_name][map_name]['wins']\n",
        "            if map_wins:\n",
        "                best_bot = max(map_wins.items(), key=lambda x: x[1])[0]\n",
        "                print(f\"   Best Performer: {best_bot}\")\n",
        "        print(\"\\n\" + \"=\"*85 + \"\\n\")\n",
        "\n",
        "    # Show replay file summary\n",
        "    if save_replays and abs_replay_dir and os.path.exists(abs_replay_dir):\n",
        "        # Count new files recursively\n",
        "        current_replay_count = 0\n",
        "        for root, dirs, files in os.walk(abs_replay_dir):\n",
        "            current_replay_count += len([f for f in files if f.endswith('.json')])\n",
        "\n",
        "        new_replay_count = current_replay_count - initial_replay_count\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"üé¨ Tournament Replays Summary\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"‚úÖ Total games played: {total_games}\")\n",
        "        print(f\"üé¨ New replay files generated: {new_replay_count}\")\n",
        "        print(f\"üìÅ Absolute path: {abs_replay_dir}\")\n",
        "        print(f\"üìÇ Organized in subfolders by map/matchup\")\n",
        "        print(f\"üìÑ Total replay files in directory: {current_replay_count}\")\n",
        "        print(f\"\\nüí° To list replay files, run:\")\n",
        "        print(f\"   !find {abs_replay_dir} -name '*.json'\")\n",
        "        print(f\"\\nüí° To view a replay, use the replay player in the game UI\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Show log file summary\n",
        "    if log_conversations and abs_log_dir and os.path.exists(abs_log_dir):\n",
        "        # Count new files recursively\n",
        "        current_log_count = 0\n",
        "        for root, dirs, files in os.walk(abs_log_dir):\n",
        "            current_log_count += len([f for f in files if f.endswith('.json')])\n",
        "\n",
        "        new_log_count = current_log_count - initial_log_count\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"üìä Tournament Logs Summary\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"‚úÖ Total games played: {total_games}\")\n",
        "        print(f\"üìù New log files generated: {new_log_count}\")\n",
        "        print(f\"üìÅ Absolute path: {abs_log_dir}\")\n",
        "        print(f\"üìÇ Organized in subfolders by map/matchup\")\n",
        "        print(f\"üìÑ Total log files in directory: {current_log_count}\")\n",
        "        print(f\"\\nüí° To list log files, run:\")\n",
        "        print(f\"   !find {abs_log_dir} -name '*.json'\")\n",
        "        print(f\"\\nüí° To review logs, see Example 2.6\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        'standings': standings,\n",
        "        'matchups': matchup_details,\n",
        "        'maps_used': map_list,\n",
        "        'map_pool_mode': map_pool_mode,\n",
        "        'games_per_matchup': games_per_matchup,\n",
        "        'should_reason': should_reason,\n",
        "        'elo_history': {bot_name: [round(r, 0) for r in history]\n",
        "                       for bot_name, history in elo_system.rating_history.items()},\n",
        "        'per_map_stats': {bot_name: dict(map_stats)\n",
        "                         for bot_name, map_stats in per_map_stats.items()}\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example_single_game"
      },
      "source": [
        "### Example 1: Single Game - SimpleBot vs SimpleBot\n",
        "\n",
        "Let's start with games between built-in bots to test the system. You can use 'simple', 'medium', or 'advanced' as bot types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "replay-saving-docs"
      },
      "source": [
        "### üé¨ Replay Saving\n",
        "\n",
        "Both `run_single_game()` and `run_tournament()` now support saving game replays!\n",
        "\n",
        "**Features:**\n",
        "- üìº Save replays to view later with the game's replay player\n",
        "- üìÅ Customizable replay directory (default: `llm_replays/`)\n",
        "- üè∑Ô∏è Descriptive filenames with timestamps, bot names, models, and game results\n",
        "- üó∫Ô∏è Replays include map reference for proper playback\n",
        "\n",
        "**Usage Examples:**\n",
        "\n",
        "```python\n",
        "# Single game with replay\n",
        "winner = run_single_game(\n",
        "    player1_bot='simple',\n",
        "    player2_bot='simple',\n",
        "    save_replay=True,\n",
        "    replay_dir='my_replays'  # Optional, defaults to 'llm_replays/'\n",
        ")\n",
        "\n",
        "# Tournament with replays\n",
        "results = run_tournament(\n",
        "    bots=[\n",
        "        ('SimpleBot', 'simple', None),\n",
        "        ('GPT-4o-mini', OpenAIBot, 'gpt-4o-mini')\n",
        "    ],\n",
        "    save_replays=True,\n",
        "    replay_dir='tournament_replays'\n",
        ")\n",
        "```\n",
        "\n",
        "**Replay Filename Format:**\n",
        "```\n",
        "game_20251217_014426_ClaudeBot-claude-3-haiku_vs_SimpleBot_P1wins.json\n",
        "```\n",
        "\n",
        "**Viewing Replays:**\n",
        "- Use the game's built-in replay player UI\n",
        "- Replays are saved as JSON files that can be loaded and viewed\n",
        "- Each replay includes the full game state and action history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "replay-example",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Example: Run a single game with replay saving\n",
        "winner = run_single_game(\n",
        "    player1_bot='simple',\n",
        "    player2_bot='simple',\n",
        "    map_file='maps/1v1/corner_points.csv',\n",
        "    max_turns=10,\n",
        "    verbose=True,\n",
        "    save_replay=True,\n",
        "    replay_dir='llm_replays'  # Optional: defaults to 'llm_replays/'\n",
        ")\n",
        "\n",
        "print(f\"\\nWinner: Player {winner}\" if winner else \"\\nResult: Draw\")\n",
        "print(\"\\nüí° Check the 'llm_replays/' directory for the saved replay file!\")\n",
        "\n",
        "# Try different bot types:\n",
        "# winner = run_single_game(player1_bot='medium', player2_bot='simple', ...)\n",
        "# winner = run_single_game(player1_bot='advanced', player2_bot='medium', ...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example_llm_game"
      },
      "source": [
        "### Example 2: Single Game - LLM Bot vs Built-in Bots\n",
        "\n",
        "Test an LLM bot against built-in bots (SimpleBot, MediumBot, or AdvancedBot). Make sure you have the appropriate API key configured!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_example_llm"
      },
      "source": [
        "# Run a single game: LLM Bot vs SimpleBot\n",
        "\n",
        "# Configuration: Choose 'gemini', 'openai', or 'claude'\n",
        "llm_provider = 'gemini'\n",
        "\n",
        "# Set logging level\n",
        "logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)\n",
        "\n",
        "if llm_provider in llm_bots_available:\n",
        "    # Select appropriate model based on provider\n",
        "    if llm_provider == 'openai':\n",
        "        model_name = 'gpt-5-mini-2025-08-07'\n",
        "    elif llm_provider == 'gemini':\n",
        "        model_name = 'gemini-2.0-flash'\n",
        "    elif llm_provider == 'claude':\n",
        "        model_name = 'claude-haiku-4-5-20251001'\n",
        "    else:\n",
        "        model_name = None\n",
        "\n",
        "    print(f\"üéÆ Starting game with {llm_provider} bot (model: {model_name})...\")\n",
        "\n",
        "    winner = run_single_game(\n",
        "        player1_bot=llm_bots_available[llm_provider],\n",
        "        player2_bot='simple',\n",
        "        map_file='maps/1v1/beginner.csv',\n",
        "        max_turns=20,\n",
        "        verbose=True,\n",
        "        player1_temperature=0.5,\n",
        "        player1_model=model_name,\n",
        "        player1_should_reason=True,  # Enable reasoning to see LLM's strategy\n",
        "        log_conversations=True,\n",
        "        save_replay=True,\n",
        "        conversation_log_dir=\"llm_logs\"\n",
        "    )\n",
        "    print(f\"\\nWinner: Player {winner}\" if winner else \"\\nResult: Draw\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  {llm_provider} bot not available. Please install necessary package and configure API key.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example_logging"
      },
      "source": [
        "### Example 2.5: Single Game with Conversation Logging\n",
        "\n",
        "Enable conversation logging to see the LLM's reasoning process. Logs are saved as JSON files."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import google.colab.drive\n",
        "from google.colab import drive\n",
        "\n",
        "env_str =  \"ReinforceTactics\"\n",
        "log_dir = \"\"\n",
        "parent_path = \"\"\n",
        "use_google_drive = True\n",
        "if use_google_drive:\n",
        "    parent_path = \"/content/gdrive\"\n",
        "    google.colab.drive.mount(parent_path, force_remount=True)\n",
        "    log_dir = \"{}/MyDrive/Finding Theta/logs/{}\".format(parent_path, env_str)\n",
        "else:\n",
        "    log_dir = \"/content/logs/{}\".format(env_str)\n",
        "\n",
        "tournamnet_log_dir = os.path.join(log_dir, 'tournaments')\n",
        "time_folder = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "run_folder = os.path.join(tournamnet_log_dir, time_folder)\n",
        "replay_dir = os.path.join(run_folder, 'replays')\n",
        "conversation_log_dir = os.path.join(run_folder, 'llm_logs')\n",
        "\n",
        "#Create Folders\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "os.makedirs(tournamnet_log_dir, exist_ok=True)\n",
        "os.makedirs(run_folder, exist_ok=True)\n",
        "os.makedirs(replay_dir, exist_ok=True)\n",
        "os.makedirs(conversation_log_dir, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Logs will be saved to: {run_folder}\")\n",
        "print(f\"üìÅ Replay files will be saved to: {replay_dir}\")\n",
        "print(f\"üìÅ Conversation logs will be saved to: {conversation_log_dir}\")"
      ],
      "metadata": {
        "id": "elJQhQauHlyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_example_logging"
      },
      "source": [
        "# Run a single game with conversation logging enabled\n",
        "# Uncomment and run if you have OpenAI API key configured\n",
        "\n",
        "# if 'openai' in llm_bots_available:\n",
        "#     import tempfile\n",
        "#     import logging\n",
        "#\n",
        "#     # Enable DEBUG logging to see conversation logs\n",
        "#     logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)\n",
        "#\n",
        "#     # Create a temporary directory for logs\n",
        "#     with tempfile.TemporaryDirectory() as tmpdir:\n",
        "#         print(f\"Conversation logs will be saved to: {tmpdir}\")\n",
        "#\n",
        "#         winner = run_single_game(\n",
        "#             player1_bot=llm_bots_available['openai'],\n",
        "#             player2_bot='simple',\n",
        "#             map_file='maps/1v1/6x6_beginner.csv',\n",
        "#             max_turns=100,\n",
        "#             verbose=True,\n",
        "#             player1_model='gpt-4o-mini',\n",
        "#             log_conversations=True,\n",
        "#             conversation_log_dir=tmpdir\n",
        "#         )\n",
        "#\n",
        "#         # List the log files\n",
        "#         import os\n",
        "#         log_files = [f for f in os.listdir(tmpdir) if f.endswith('.json')]\n",
        "#         print(f\"\\nüìù Generated {len(log_files)} conversation log files\")\n",
        "#\n",
        "#         # Show a sample log entry\n",
        "#         if log_files:\n",
        "#             import json\n",
        "#             with open(os.path.join(tmpdir, log_files[0])) as f:\n",
        "#                 sample_log = json.load(f)\n",
        "#             print(f\"\\nüìÑ Sample log entry:\")\n",
        "#             print(json.dumps(sample_log, indent=2)[:500] + \"...\")\n",
        "# else:\n",
        "#     print(\"‚ö†Ô∏è  OpenAIBot not available. Please install openai and configure API key.\")\n",
        "\n",
        "print(\"Uncomment the code above to run a game with conversation logging\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Tql9ikGV7T"
      },
      "source": [
        "### Example 2.6: Reviewing Logged Conversations\n",
        "\n",
        "Learn how to review and analyze the logged conversations from your games. This example shows how to:\n",
        "- Use a persistent log directory for Google Colab\n",
        "- List and inspect log files (now one file per game)\n",
        "- Extract key information from conversation logs\n",
        "- Pretty-print log content for analysis\n",
        "\n",
        "**New in this version:**\n",
        "- Each game now creates a **single** log file containing all turns\n",
        "- Filename format: `game_{session_id}_player{N}_model{model}.json`\n",
        "- Makes it easier to review an entire game's decision-making process\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04s3ROJVGV7T"
      },
      "outputs": [],
      "source": [
        "# Example 2.6: Reviewing Logged Conversations\n",
        "# Run a game with persistent logging and then review the logs\n",
        "# Uncomment and run if you have OpenAI API key configured\n",
        "\n",
        "# if 'openai' in llm_bots_available:\n",
        "#     import os\n",
        "#     import json\n",
        "#     from pathlib import Path\n",
        "#     import logging\n",
        "#\n",
        "#     # Enable DEBUG logging to see conversation logs\n",
        "#     logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)\n",
        "#\n",
        "#     # Create persistent log directory (recommended for Google Colab)\n",
        "#     log_dir = '/content/llm_logs/'\n",
        "#     os.makedirs(log_dir, exist_ok=True)\n",
        "#     print(f\"üìÅ Logs will be saved to: {os.path.abspath(log_dir)}\\n\")\n",
        "#\n",
        "#     # Run game with logging\n",
        "#     print(\"üéÆ Running game with conversation logging...\\n\")\n",
        "#     winner = run_single_game(\n",
        "#         player1_bot=llm_bots_available['openai'],\n",
        "#         player2_bot='simple',\n",
        "#         map_file='maps/1v1/6x6_beginner.csv',\n",
        "#         max_turns=50,  # Shorter game for demo\n",
        "#         verbose=True,\n",
        "#         player1_model='gpt-4o-mini',\n",
        "#         log_conversations=True,\n",
        "#         conversation_log_dir=log_dir\n",
        "#     )\n",
        "#\n",
        "#     # Helper function to review conversation logs\n",
        "#     def review_conversation_logs(log_dir):\n",
        "#         \"\"\"Review and display game conversation logs.\"\"\"\n",
        "#         log_path = Path(log_dir)\n",
        "#         # NEW: Logs are now stored as game_{session_id}_player{N}_model{model}.json\n",
        "#         # Each file contains ALL turns from a single game session\n",
        "#         log_files = sorted(log_path.glob('game_*.json'))\n",
        "#\n",
        "#         print(f\"\\n{'='*70}\")\n",
        "#         print(\"üìä CONVERSATION LOGS REVIEW\")\n",
        "#         print(\"=\"*70)\n",
        "#         print(f\"üìÅ Absolute path: {log_path.absolute()}\")\n",
        "#         print(f\"üìù Total game log files: {len(log_files)}\")\n",
        "#         print(f\"üí° Note: Each file now contains ALL turns from one game\\n\")\n",
        "#\n",
        "#         if not log_files:\n",
        "#             print(\"‚ö†Ô∏è  No log files found. Make sure logging was enabled.\")\n",
        "#             return log_files\n",
        "#\n",
        "#         # Show first game log in detail\n",
        "#         log_file = log_files[0]\n",
        "#         print(f\"\\n{'='*70}\")\n",
        "#         print(f\"üìÑ Game log file: {log_file.name}\")\n",
        "#         print(f\"üìÅ Full path: {log_file.absolute()}\")\n",
        "#         print('='*70)\n",
        "#\n",
        "#         with open(log_file) as f:\n",
        "#             log_data = json.load(f)\n",
        "#\n",
        "#         # Display game metadata\n",
        "#         print(f\"üéÆ Game Session ID: {log_data.get('game_session_id', 'N/A')}\")\n",
        "#         print(f\"ü§ñ Model: {log_data.get('model', 'N/A')}\")\n",
        "#         print(f\"üè¢ Provider: {log_data.get('provider', 'N/A')}\")\n",
        "#         print(f\"üë§ Player: {log_data.get('player', 'N/A')}\")\n",
        "#         print(f\"üïê Start Time: {log_data.get('start_time', 'N/A')}\")\n",
        "#         print(f\"üî¢ Total Turns: {len(log_data.get('turns', []))}\")\n",
        "#\n",
        "#         # Show sample turns\n",
        "#         turns = log_data.get('turns', [])\n",
        "#         if turns:\n",
        "#             print(f\"\\n{'='*70}\")\n",
        "#             print(\"üìù Sample Turns (first and last)\")\n",
        "#             print(\"=\"*70)\n",
        "#\n",
        "#             for idx in [0, -1] if len(turns) > 1 else [0]:\n",
        "#                 turn = turns[idx]\n",
        "#                 print(f\"\\nüî¢ Turn {turn.get('turn_number', 'N/A')}:\")\n",
        "#                 print(f\"  üïê Timestamp: {turn.get('timestamp', 'N/A')}\")\n",
        "#\n",
        "#                 # Extract reasoning from response\n",
        "#                 response = turn.get('assistant_response', '')\n",
        "#                 try:\n",
        "#                     response_json = json.loads(response)\n",
        "#                     reasoning = response_json.get('reasoning', 'N/A')\n",
        "#                     actions = response_json.get('actions', [])\n",
        "#\n",
        "#                     print(f\"  ÔøΩÔøΩ Reasoning: {reasoning[:150]}...\")\n",
        "#                     print(f\"  ‚ö° Actions: {len(actions)} action(s)\")\n",
        "#\n",
        "#                     if actions:\n",
        "#                         action_types = [a.get('type', 'unknown') for a in actions[:3]]\n",
        "#                         print(f\"     First few: {', '.join(action_types)}\")\n",
        "#                 except (json.JSONDecodeError, Exception) as e:\n",
        "#                     print(f\"  Response: {response[:100]}...\")\n",
        "#\n",
        "#         print(f\"\\n{'='*70}\")\n",
        "#         print(\"üí° Tips:\")\n",
        "#         print(\"  - Each game_*.json file contains ALL turns from that game\")\n",
        "#         print(\"  - System prompt is stored once at the top of the file\")\n",
        "#         print(\"  - Use 'jq' for advanced JSON parsing: !jq . path/to/log.json\")\n",
        "#         print(\"  - Compare reasoning across different games/models\")\n",
        "#         print(\"=\"*70 + \"\\n\")\n",
        "#\n",
        "#         return log_files\n",
        "#\n",
        "#     # Review the logs\n",
        "#     log_files = review_conversation_logs(log_dir)\n",
        "#\n",
        "#     # Optional: Find games by session ID\n",
        "#     def find_game_log(log_dir, session_id):\n",
        "#         \"\"\"Find log file for a specific game session.\"\"\"\n",
        "#         log_path = Path(log_dir)\n",
        "#         for log_file in log_path.glob('game_*.json'):\n",
        "#             if session_id in log_file.name:\n",
        "#                 return log_file\n",
        "#         return None\n",
        "#\n",
        "#     print(\"\\nüìå Helper functions defined:\")\n",
        "#     print(\"  - review_conversation_logs(log_dir): Review game logs\")\n",
        "#     print(\"  - find_game_log(log_dir, session_id): Find specific game\")\n",
        "# else:\n",
        "#     print(\"‚ö†Ô∏è  OpenAIBot not available. Please install openai and configure API key.\")\n",
        "\n",
        "print(\"Uncomment the code above to run Example 2.6\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example_tournament"
      },
      "source": [
        "### Example 3: Mini Tournament\n",
        "\n",
        "Run a small tournament with available bots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_example_tournament"
      },
      "source": [
        "# # Define tournament participants\n",
        "# # Format: (display_name, bot_class_or_'simple', optional_model_name)\n",
        "\n",
        "# tournament_bots = [\n",
        "#     ('SimpleBot', 'simple', None),\n",
        "#     ('MediumBot', 'medium', None),\n",
        "#     ('AdvancedBot', 'advanced', None),\n",
        "# ]\n",
        "\n",
        "# # Add LLM bots if available and configured\n",
        "# if 'openai' in llm_bots_available and os.environ.get('OPENAI_API_KEY'):\n",
        "#     tournament_bots.append(('GPT-4o-mini', llm_bots_available['openai'], 'gpt-4o-mini'))\n",
        "\n",
        "# if 'claude' in llm_bots_available and os.environ.get('ANTHROPIC_API_KEY'):\n",
        "#     tournament_bots.append(('Claude Haiku', llm_bots_available['claude'], 'claude-3-haiku-20240307'))\n",
        "\n",
        "# if 'gemini' in llm_bots_available and os.environ.get('GOOGLE_API_KEY'):\n",
        "#     tournament_bots.append(('Gemini Flash', llm_bots_available['gemini'], 'gemini-1.5-flash'))\n",
        "\n",
        "# # Run tournament if we have at least 2 bots\n",
        "# if len(tournament_bots) >= 2:\n",
        "#     results = run_tournament(\n",
        "#         bots=tournament_bots,\n",
        "#         map_file='maps/1v1/beginner.csv',\n",
        "#         games_per_matchup=2,  # 2 games per side = 4 total per matchup\n",
        "#         max_turns=100\n",
        "#     )\n",
        "#     print(\"\\n‚úÖ Tournament complete! Results saved in 'results' variable.\")\n",
        "# else:\n",
        "#     print(\"‚ö†Ô∏è  Need at least 2 bots for a tournament.\")\n",
        "#     print(\"   Configure API keys for LLM bots or add more SimpleBots for testing.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_models"
      },
      "source": [
        "## üé® Custom Model Configuration\n",
        "\n",
        "You can specify different models for each LLM provider. Here are some examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "openai_models"
      },
      "source": [
        "### OpenAI Models\n",
        "\n",
        "**Available models:**\n",
        "- `gpt-4o-mini` (default) - Fastest and cheapest, good for testing\n",
        "- `gpt-4o` - More capable, higher cost\n",
        "- `gpt-4-turbo` - Previous generation, balanced\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Using GPT-4o for stronger gameplay\n",
        "winner = run_single_game(\n",
        "    player1_bot=OpenAIBot,\n",
        "    player2_bot='simple',\n",
        "    player1_model='gpt-4o'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "claude_models"
      },
      "source": [
        "### Claude Models\n",
        "\n",
        "**Available models:**\n",
        "- `claude-3-haiku-20240307` (default) - Fastest and cheapest\n",
        "- `claude-3-5-sonnet-20241022` - Most capable, balanced cost\n",
        "- `claude-3-opus-20240229` - Highest capability, highest cost\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Using Claude Sonnet for better strategic play\n",
        "winner = run_single_game(\n",
        "    player1_bot=ClaudeBot,\n",
        "    player2_bot='simple',\n",
        "    player1_model='claude-3-5-sonnet-20241022'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gemini_models"
      },
      "source": [
        "### Gemini Models\n",
        "\n",
        "**Available models:**\n",
        "- `gemini-1.5-flash` (default) - Fast and efficient, free tier available\n",
        "- `gemini-1.5-pro` - More capable, higher cost\n",
        "- `gemini-2.0-flash-exp` - Experimental, cutting edge\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "# Using Gemini Pro for better performance\n",
        "winner = run_single_game(\n",
        "    player1_bot=GeminiBot,\n",
        "    player2_bot='simple',\n",
        "    player1_model='gemini-1.5-pro'\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_tournament_example"
      },
      "source": [
        "### Example: Tournament with Custom Models\n",
        "\n",
        "Compare different models from different providers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "run_custom_tournament"
      },
      "source": [
        "# Advanced tournament: Compare different models\n",
        "# Only run this if you have all API keys configured and don't mind the cost!\n",
        "\n",
        "# Set logging level\n",
        "logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Define bots using the new TournamentBot class\n",
        "advanced_bots = [\n",
        "    TournamentBot('SimpleBot', 'simple'),\n",
        "    TournamentBot('MediumBot', 'medium'),\n",
        "    TournamentBot('AdvancedBot', 'advanced'),\n",
        "    TournamentBot('Claude Haiku 4.5', ClaudeBot, 'claude-haiku-4-5-20251001', temperature=0.5, max_tokens=None),\n",
        "    TournamentBot('Claude Sonnet 4.5', ClaudeBot, 'claude-sonnet-4-5-20250929', temperature=0.5, max_tokens=None),\n",
        "    TournamentBot('Gemini 2.0 Flash', GeminiBot, 'gemini-2.0-flash', temperature=0.5, max_tokens=None),\n",
        "    TournamentBot('ChatGPT 5 Mini', OpenAIBot, 'gpt-5-mini-2025-08-07', max_tokens=None),\n",
        "]\n",
        "\n",
        "maps=['maps/1v1/beginner.csv',\n",
        "      'maps/1v1/funnel_point.csv',\n",
        "      'maps/1v1/center_mountains.csv',\n",
        "      'maps/1v1/corner_points.csv']\n",
        "\n",
        "results = run_tournament(\n",
        "    advanced_bots,\n",
        "    maps=maps,\n",
        "    games_per_matchup=1,\n",
        "    map_pool_mode='all',\n",
        "    max_turns=100,\n",
        "    should_reason=True,  # Enable reasoning for all LLM bots\n",
        "    log_conversations=True,\n",
        "    conversation_log_dir=conversation_log_dir,\n",
        "    save_replays=True,\n",
        "    replay_dir=replay_dir\n",
        ")\n",
        "\n",
        "print(\"Uncomment the code above to run a full model comparison tournament\")\n",
        "print(\"‚ö†Ô∏è  Warning: This will make many API calls and may incur costs!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "documentation"
      },
      "source": [
        "## üìñ Documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29bdb22b"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "import reinforcetactics\n",
        "\n",
        "def analyze_token_usage(log_dir):\n",
        "    \"\"\"Analyze token usage from log files in the specified directory.\"\"\"\n",
        "    if not log_dir or not os.path.exists(log_dir):\n",
        "        return None\n",
        "\n",
        "    token_stats = defaultdict(lambda: {'input_tokens': 0, 'output_tokens': 0, 'count': 0})\n",
        "\n",
        "    # Walk through log directory\n",
        "    for root, _, files in os.walk(log_dir):\n",
        "        for file in files:\n",
        "            if not file.endswith('.json') or not file.startswith('game_'):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                file_path = os.path.join(root, file)\n",
        "                with open(file_path, 'r') as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                model = data.get('model', 'unknown')\n",
        "\n",
        "                # Try to extract usage from turns\n",
        "                turns = data.get('turns', [])\n",
        "                file_input_tokens = 0\n",
        "                file_output_tokens = 0\n",
        "\n",
        "                for turn in turns:\n",
        "                    # Check common usage locations (OpenAI, Anthropic, Gemini patterns)\n",
        "                    usage = turn.get('usage') or turn.get('token_usage') or turn.get('usage_metadata')\n",
        "\n",
        "                    if usage and isinstance(usage, dict):\n",
        "                        # Input Tokens\n",
        "                        file_input_tokens += (usage.get('prompt_tokens') or\n",
        "                                            usage.get('input_tokens') or\n",
        "                                            usage.get('prompt_token_count') or 0)\n",
        "                        # Output Tokens\n",
        "                        file_output_tokens += (usage.get('completion_tokens') or\n",
        "                                             usage.get('output_tokens') or\n",
        "                                             usage.get('candidates_token_count') or 0)\n",
        "\n",
        "                if file_input_tokens > 0 or file_output_tokens > 0:\n",
        "                    token_stats[model]['input_tokens'] += file_input_tokens\n",
        "                    token_stats[model]['output_tokens'] += file_output_tokens\n",
        "                    token_stats[model]['count'] += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error reading log file {file}: {e}\")\n",
        "\n",
        "    return dict(token_stats)\n",
        "\n",
        "def save_tournament_results(results_data, output_dir='tournament_results', llm_log_dir=None):\n",
        "    \"\"\"Save tournament results to CSV and JSON files with metadata.\"\"\"\n",
        "    if not results_data:\n",
        "        print(\"‚ö†Ô∏è No results data to save.\")\n",
        "        return\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Get library version\n",
        "    rt_version = getattr(reinforcetactics, '__version__', 'unknown')\n",
        "\n",
        "    # Prepare data with metadata\n",
        "    # Create a shallow copy to avoid modifying the original dict\n",
        "    data_to_save = results_data.copy()\n",
        "    data_to_save['metadata'] = {\n",
        "        'reinforcetactics_version': rt_version,\n",
        "        'timestamp': timestamp,\n",
        "        'export_time': datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # Analyze token usage if log directory is provided\n",
        "    if llm_log_dir:\n",
        "        print(f\"üìä Analyzing token usage from: {llm_log_dir}\")\n",
        "        token_stats = analyze_token_usage(llm_log_dir)\n",
        "        if token_stats:\n",
        "            data_to_save['token_stats'] = token_stats\n",
        "            print(\"‚úÖ Token usage analysis complete\")\n",
        "\n",
        "            # Create DataFrame for display\n",
        "            token_data = []\n",
        "            for model, stats in token_stats.items():\n",
        "                token_data.append({\n",
        "                    'Model': model,\n",
        "                    'Input Tokens': stats['input_tokens'],\n",
        "                    'Output Tokens': stats['output_tokens'],\n",
        "                    'Total Tokens': stats['input_tokens'] + stats['output_tokens'],\n",
        "                    'Games Logged': stats['count']\n",
        "                })\n",
        "\n",
        "            if token_data:\n",
        "                token_df = pd.DataFrame(token_data)\n",
        "                print(\"\\nüí∞ Token Usage Summary:\")\n",
        "                display(token_df)\n",
        "\n",
        "                # Save token stats to CSV\n",
        "                token_csv_path = os.path.join(output_dir, f'token_usage_{timestamp}.csv')\n",
        "                token_df.to_csv(token_csv_path, index=False)\n",
        "                print(f\"‚úÖ Token stats saved to: {token_csv_path}\")\n",
        "\n",
        "    # 1. Save full results to JSON\n",
        "    json_path = os.path.join(output_dir, f'tournament_results_{timestamp}.json')\n",
        "    try:\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(data_to_save, f, indent=2)\n",
        "        print(f\"‚úÖ Full results saved to: {json_path}\")\n",
        "        # print(f\"‚ÑπÔ∏è  Library Version: {rt_version}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error saving JSON: {e}\")\n",
        "\n",
        "    # 2. Save standings to CSV\n",
        "    if 'standings' in results_data:\n",
        "        csv_path = os.path.join(output_dir, f'tournament_standings_{timestamp}.csv')\n",
        "        try:\n",
        "            df = pd.DataFrame(results_data['standings'])\n",
        "            df.to_csv(csv_path, index=False)\n",
        "            print(f\"‚úÖ Standings saved to: {csv_path}\")\n",
        "            print(\"\\nPreview of CSV:\")\n",
        "            display(df)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error saving CSV: {e}\")\n",
        "\n",
        "    # 3. Save Matchups to CSV\n",
        "    if 'matchups' in results_data and results_data['matchups']:\n",
        "        matchups_csv_path = os.path.join(output_dir, f'tournament_matchups_{timestamp}.csv')\n",
        "        try:\n",
        "            matchups_df = pd.DataFrame(results_data['matchups'])\n",
        "            # Rename columns for better readability\n",
        "            matchups_df_renamed = matchups_df.rename(columns={\n",
        "                'bot1': 'Bot 1',\n",
        "                'bot2': 'Bot 2',\n",
        "                'bot1_wins': 'Bot 1 Wins',\n",
        "                'bot2_wins': 'Bot 2 Wins',\n",
        "                'draws': 'Draws'\n",
        "            })\n",
        "\n",
        "            matchups_df_renamed.to_csv(matchups_csv_path, index=False)\n",
        "            print(f\"‚úÖ Matchups saved to: {matchups_csv_path}\")\n",
        "            # print(\"\\nPreview of Matchups CSV:\")\n",
        "            # display(matchups_df_renamed)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error saving Matchups CSV: {e}\")\n",
        "\n",
        "        # 4. Save Matrix/Cross Table CSV (NEW)\n",
        "        matrix_csv_path = os.path.join(output_dir, f'tournament_matrix_{timestamp}.csv')\n",
        "        try:\n",
        "            # Extract unique bots\n",
        "            bots = set()\n",
        "            for m in results_data['matchups']:\n",
        "                bots.add(m['bot1'])\n",
        "                bots.add(m['bot2'])\n",
        "            bots = sorted(list(bots))\n",
        "\n",
        "            # Create empty DataFrame\n",
        "            matrix_df = pd.DataFrame(index=bots, columns=bots)\n",
        "            # Initialize with \"0-0-0\" for no games played\n",
        "            matrix_df = matrix_df.fillna(\"0-0-0\")\n",
        "\n",
        "            # Fill with W-L-D strings\n",
        "            for m in results_data['matchups']:\n",
        "                b1 = m['bot1']\n",
        "                b2 = m['bot2']\n",
        "                w1 = int(m['bot1_wins'])\n",
        "                w2 = int(m['bot2_wins'])\n",
        "                d = int(m['draws'])\n",
        "\n",
        "                # Format: Wins-Losses-Draws from the perspective of the row bot\n",
        "                matrix_df.loc[b1, b2] = f\"{w1}-{w2}-{d}\"\n",
        "                matrix_df.loc[b2, b1] = f\"{w2}-{w1}-{d}\"\n",
        "\n",
        "            # Mark diagonal\n",
        "            for b in bots:\n",
        "                matrix_df.loc[b, b] = 'X'\n",
        "\n",
        "            matrix_df.to_csv(matrix_csv_path)\n",
        "            print(f\"‚úÖ Matrix table saved to: {matrix_csv_path}\")\n",
        "            print(\"\\nPreview of Tournament Matrix (W-L-D):\")\n",
        "            display(matrix_df)\n",
        "        except Exception as e:\n",
        "             print(f\"‚ö†Ô∏è Error saving Matrix CSV: {e}\")\n",
        "\n",
        "# Save the results from the previous tournament\n",
        "if 'results' in locals():\n",
        "    # Use run_folder if defined, otherwise default\n",
        "    out_dir = run_folder if 'run_folder' in locals() else 'tournament_results'\n",
        "    # Use conversation_log_dir if defined\n",
        "    llm_log_dir = conversation_log_dir if 'conversation_log_dir' in locals() else None\n",
        "\n",
        "    save_tournament_results(results, out_dir, llm_log_dir=llm_log_dir)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è 'results' variable not found. Run a tournament first!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cost_estimates"
      },
      "source": [
        "### üí∞ API Cost Estimates\n",
        "\n",
        "**OpenAI Pricing (approximate, as of 2024):**\n",
        "- GPT-4o-mini: $0.15/1M input tokens, $0.60/1M output tokens\n",
        "- GPT-4o: $2.50/1M input tokens, $10.00/1M output tokens\n",
        "- Typical game: 2,000-10,000 tokens per side\n",
        "- **Cost per game:** $0.0001-0.0005 (mini), $0.005-0.02 (4o)\n",
        "\n",
        "**Anthropic Pricing:**\n",
        "- Claude Haiku: $0.25/1M input tokens, $1.25/1M output tokens\n",
        "- Claude Sonnet: $3.00/1M input tokens, $15.00/1M output tokens\n",
        "- **Cost per game:** $0.0001-0.0003 (Haiku), $0.003-0.015 (Sonnet)\n",
        "\n",
        "**Google Gemini Pricing:**\n",
        "- Gemini Flash: Free tier available (15 RPM), $0.075/1M input, $0.30/1M output\n",
        "- Gemini Pro: $1.25/1M input tokens, $5.00/1M output tokens\n",
        "- **Cost per game:** ~$0 (Flash free tier), $0.0001-0.001 (Flash paid), $0.001-0.005 (Pro)\n",
        "\n",
        "**Tournament Cost Estimates:**\n",
        "- 3 bots, 2 games/matchup: 12 games total\n",
        "- Using mini/haiku/flash: ~$0.001-0.01 total\n",
        "- Using premium models: ~$0.05-0.20 total\n",
        "\n",
        "**Cost Saving Tips:**\n",
        "1. Use mini/haiku/flash models for development and testing\n",
        "2. Use smaller maps (6x6) which need fewer tokens\n",
        "3. Set lower `max_turns` to prevent long games\n",
        "4. Use Gemini Flash free tier for unlimited testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "performance_tips"
      },
      "source": [
        "### ‚ö° Performance Tips\n",
        "\n",
        "**Model Selection for Different Purposes:**\n",
        "\n",
        "**For Testing & Development:**\n",
        "- ‚úÖ GPT-4o-mini - Fast, cheap, decent strategy\n",
        "- ‚úÖ Claude Haiku - Very fast, cheap, good baseline\n",
        "- ‚úÖ Gemini Flash - Free tier, fast, great for testing\n",
        "\n",
        "**For Competitive Play:**\n",
        "- üèÜ GPT-4o - Strong strategic thinking\n",
        "- üèÜ Claude Sonnet 3.5 - Excellent reasoning, good value\n",
        "- üèÜ Gemini Pro 1.5 - Good balance of cost/performance\n",
        "\n",
        "**For Research/Analysis:**\n",
        "- üî¨ Claude Opus - Highest reasoning capability\n",
        "- üî¨ GPT-4 Turbo - Consistent performance\n",
        "\n",
        "**Game Speed:**\n",
        "- Smaller maps (6x6) complete in 1-3 minutes per game\n",
        "- Larger maps (32x32) can take 10-30 minutes per game\n",
        "- LLM API calls add 1-5 seconds per turn\n",
        "- SimpleBot is nearly instant\n",
        "\n",
        "**Tournament Duration Estimates:**\n",
        "- 2 bots, 4 games: ~5-15 minutes\n",
        "- 3 bots, 12 games: ~15-45 minutes\n",
        "- 4 bots, 24 games: ~30-90 minutes\n",
        "- 5 bots, 40 games: ~1-2 hours\n",
        "\n",
        "**Optimization Strategies:**\n",
        "1. Start with 6x6 maps for quick iterations\n",
        "2. Use `games_per_matchup=1` for initial testing\n",
        "3. Set `max_turns=100` for faster games\n",
        "4. Run tournaments with fewer bots initially\n",
        "5. Use verbose=False in run_single_game() to reduce output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "### üîß Troubleshooting\n",
        "\n",
        "**Problem: \"API key not provided\" error**\n",
        "- **Solution:** Make sure you've set the API key in the environment or Colab secrets\n",
        "- Check the API Key Configuration section output\n",
        "- Uncomment and set the API key directly in the configuration cell\n",
        "\n",
        "**Problem: \"openai package not installed\" error**\n",
        "- **Solution:** Run the installation cell again\n",
        "- Or manually install: `!pip install openai>=1.0.0`\n",
        "\n",
        "**Problem: Bot makes invalid moves or errors**\n",
        "- **Solution:** This is expected occasionally with LLMs\n",
        "- The game will skip invalid actions and continue\n",
        "- Try using a more capable model (e.g., GPT-4o instead of mini)\n",
        "- Check the logs to see what actions failed\n",
        "\n",
        "**Problem: Games taking too long**\n",
        "- **Solution:** Reduce `max_turns` parameter\n",
        "- Use smaller maps (6x6 instead of 32x32)\n",
        "- Faster models: Haiku, Flash, or mini\n",
        "\n",
        "**Problem: API rate limits exceeded**\n",
        "- **Solution:** Add delays between games\n",
        "- Use free tier models (Gemini Flash)\n",
        "- Reduce `games_per_matchup`\n",
        "- Spread tournament over multiple sessions\n",
        "\n",
        "**Problem: Out of memory error**\n",
        "- **Solution:** Restart the notebook runtime\n",
        "- Run fewer games at once\n",
        "- Use smaller maps\n",
        "\n",
        "**Problem: \"Map file not found\" error**\n",
        "- **Solution:** Make sure you're in the reinforce-tactics directory\n",
        "- Check the path: `!ls maps/1v1/`\n",
        "- Use absolute paths if needed\n",
        "\n",
        "**Problem: Import errors for LLM bots**\n",
        "- **Solution:** Check that dependencies installed correctly\n",
        "- Verify the repository was cloned successfully\n",
        "- Make sure the repository is in your Python path\n",
        "\n",
        "**Problem: Tournament results seem random**\n",
        "- **Solution:** LLMs have inherent randomness\n",
        "- Increase `games_per_matchup` for more stable results\n",
        "- Temperature parameter affects consistency (set in llm_bot.py)\n",
        "- SimpleBot is deterministic and provides a good baseline\n",
        "\n",
        "**Problem: Cost concerns**\n",
        "- **Solution:** Always use mini/haiku/flash for testing\n",
        "- Monitor API usage in your provider dashboard\n",
        "- Set spending limits in your API account\n",
        "- Test with SimpleBot first (free)\n",
        "- Use Gemini Flash free tier for unlimited testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "next_steps"
      },
      "source": [
        "## üéì Next Steps\n",
        "\n",
        "**New Features Now Available:**\n",
        "1. ‚úÖ Elo rating system - Track bot skill across games\n",
        "2. ‚úÖ Multiple maps support - Run tournaments across different maps\n",
        "3. ‚úÖ Per-map statistics - See which bots excel on specific maps\n",
        "\n",
        "**Experiment Ideas:**\n",
        "1. Compare different models from the same provider\n",
        "2. Test how map size affects bot performance using multiple maps\n",
        "3. Analyze which bots excel at different strategies\n",
        "4. Track Elo rating progression over multiple tournaments\n",
        "5. Compare performance across maps of different sizes\n",
        "\n",
        "**Code Customization:**\n",
        "1. Modify system prompts in `llm_bot.py` for different strategies\n",
        "2. Add logging to track specific metrics\n",
        "3. Create visualization of tournament brackets\n",
        "4. Export results to CSV for analysis\n",
        "5. Build a web interface for live tournaments\n",
        "\n",
        "**Advanced Tournaments:**\n",
        "1. Swiss-system tournament format\n",
        "2. Double elimination brackets\n",
        "3. Time-limited games\n",
        "4. Asymmetric maps\n",
        "5. Team battles (coming soon)\n",
        "\n",
        "**Contributing:**\n",
        "- Found a bug? Open an issue on GitHub\n",
        "- Have an improvement? Submit a pull request\n",
        "- Share your tournament results!\n",
        "\n",
        "**Resources:**\n",
        "- Repository: https://github.com/kuds/reinforce-tactics\n",
        "- Game Rules: See `reinforcetactics/game/llm_bot.py` SYSTEM_PROMPT\n",
        "- Tournament Script: `scripts/tournament.py`\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Gaming! üéÆ**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create version execution\n",
        "notebook_name = \"notebook.ipynb\"\n",
        "%notebook -e $notebook_name"
      ],
      "metadata": {
        "id": "TwgAMcko8uIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "source_file = os.path.join(notebook_name)\n",
        "destination_file = os.path.join(run_folder, notebook_name)\n",
        "\n",
        "try:\n",
        "    shutil.copyfile(notebook_name, destination_file)\n",
        "    print(f\"File '{source_file}' copied to '{destination_file}' successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Source file '{source_file}' not found.\")\n",
        "except shutil.SameFileError:\n",
        "    print(f\"Error: Source and destination are the same file.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "Yy9frS9Q86Rm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "llm_bot_tournament.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}