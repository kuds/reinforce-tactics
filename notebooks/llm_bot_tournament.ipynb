{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/llm_bot_tournament.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# LLM Bot Tournament - Reinforce Tactics\n\nRun interactive tournaments between LLM-powered bots (OpenAI GPT, Claude, Gemini) and built-in rule-based bots (SimpleBot, MediumBot, AdvancedBot)!\n\n**Features:**\n- Single game runner with detailed turn-by-turn logging\n- Round-robin tournament system with multiple games per matchup\n- **Elo rating system** for skill-based rankings\n- **Multiple maps support** with configurable map pool modes\n- Flexible API key configuration (environment variables or Google Colab secrets)\n- Comprehensive statistics: wins, losses, draws, win rates, Elo ratings\n- Customizable model selection (GPT-5.2, Claude Opus 4.6, Gemini 3 Flash, etc.)\n- Conversation logging to debug and analyze LLM reasoning\n\n**Supported Bots:**\n- **SimpleBot**: Basic rule-based bot (always available)\n- **MediumBot**: Improved rule-based bot with advanced strategies (always available)\n- **AdvancedBot**: Most sophisticated rule-based bot (always available)\n- **OpenAIBot**: Uses OpenAI GPT-5+ models (requires `OPENAI_API_KEY`)\n- **ClaudeBot**: Uses Anthropic Claude 4+ models (requires `ANTHROPIC_API_KEY`)\n- **GeminiBot**: Uses Google Gemini 2.5+ models (requires `GOOGLE_API_KEY`)\n\n**Quick Start:**\n1. Install dependencies and clone the repository\n2. Configure API keys for the bots you want to use\n3. Run a single game or full tournament\n4. Analyze the results!\n\n**Estimated API Costs:**\n- **GPT-5 Mini**: ~$0.0005-0.002 per game (recommended for testing)\n- **Claude Haiku 4.5**: ~$0.001-0.005 per game (recommended for testing)\n- **Gemini 2.5 Flash**: ~$0.0001-0.001 per game (budget-friendly)\n- **GPT-5.2**: ~$0.01-0.05 per game (strongest play, higher cost)\n- **Claude Opus 4.6**: ~$0.01-0.04 per game (strongest play, higher cost)\n\n*Costs vary based on game length and map complexity. Use mini/haiku/flash models for testing!*"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üì¶ Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "# Clone the Reinforce Tactics repository if not already present\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path('reinforce-tactics').exists():\n",
    "    print(\"üì• Cloning Reinforce Tactics repository...\")\n",
    "    !git clone https://github.com/kuds/reinforce-tactics.git\n",
    "    print(\"‚úÖ Repository cloned!\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already cloned\")\n",
    "\n",
    "# Change to repository directory\n",
    "os.chdir('reinforce-tactics')\n",
    "print(f\"\\nüìÇ Current directory: {os.getcwd()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install_llm_deps"
   },
   "source": [
    "# Install LLM bot dependencies\n",
    "# These are optional - only install for the bots you plan to use\n",
    "print(\"üì¶ Installing LLM dependencies...\\n\")\n",
    "\n",
    "# Install OpenAI (for GPT models)\n",
    "print(\"Installing OpenAI...\")\n",
    "!pip install -q openai\n",
    "\n",
    "# Install Anthropic (for Claude models)\n",
    "print(\"Installing Anthropic...\")\n",
    "!pip install -q anthropic\n",
    "\n",
    "# Install Google Generative AI (for Gemini models)\n",
    "print(\"Installing Google Gen AI...\")\n",
    "!pip install -q google-genai\n",
    "\n",
    "# Install other dependencies if needed\n",
    "!pip install -q pandas numpy\n",
    "\n",
    "print(\"\\n‚úÖ All LLM dependencies installed!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "add_to_path"
   },
   "source": [
    "# Add repository to Python path\n",
    "import sys\n",
    "repo_path = os.getcwd()\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.insert(0, repo_path)\n",
    "    print(f\"‚úÖ Added to Python path: {repo_path}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Already in Python path: {repo_path}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import platform\n",
    "import torch\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(f\"Python Version: {platform.python_version()}\")\n",
    "print(f\"Torch Version: {version('torch')}\")\n",
    "print(f\"Is Cuda Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Cuda Version: {torch.version.cuda}\")\n",
    "print(f\"Gymnasium Version: {version('gymnasium')}\")\n",
    "print(f\"Numpy Version: {version('numpy')}\")\n",
    "print(f\"OpenAI Version: {version('openai')}\")\n",
    "print(f\"Anthropic Version: {version('anthropic')}\")\n",
    "print(f\"Gemini Version: {version('google.genai')}\")"
   ],
   "metadata": {
    "id": "0jb8deCuTEwq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "api_keys"
   },
   "source": [
    "## üîë API Key Configuration\n",
    "\n",
    "You have two options for setting API keys:\n",
    "\n",
    "### Option 1: Direct Environment Variables (Quick Setup)\n",
    "Set API keys directly in the cells below. **Note:** These will be visible in the notebook.\n",
    "\n",
    "### Option 2: Google Colab Secrets (Recommended for Colab)\n",
    "1. Click the üîë key icon in the left sidebar\n",
    "2. Add secrets with names: `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`\n",
    "3. Toggle \"Notebook access\" on for each secret\n",
    "\n",
    "The code below will check both sources and use Colab secrets if available."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "set_api_keys"
   },
   "source": [
    "import os\n",
    "\n",
    "# Try to use Google Colab secrets first\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    print(\"‚úÖ Google Colab detected - checking for secrets...\\n\")\n",
    "\n",
    "    # Try to get OpenAI key from secrets\n",
    "    try:\n",
    "        openai_key = userdata.get('OPENAI_API_KEY')\n",
    "        os.environ['OPENAI_API_KEY'] = openai_key\n",
    "        print(\"‚úÖ OPENAI_API_KEY loaded from Colab secrets\")\n",
    "    except:\n",
    "        if 'OPENAI_API_KEY' not in os.environ:\n",
    "            print(\"‚ö†Ô∏è  OPENAI_API_KEY not found in Colab secrets\")\n",
    "\n",
    "    # Try to get Anthropic key from secrets\n",
    "    try:\n",
    "        anthropic_key = userdata.get('ANTHROPIC_API_KEY')\n",
    "        os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
    "        print(\"‚úÖ ANTHROPIC_API_KEY loaded from Colab secrets\")\n",
    "    except:\n",
    "        if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "            print(\"‚ö†Ô∏è  ANTHROPIC_API_KEY not found in Colab secrets\")\n",
    "\n",
    "    # Try to get Google key from secrets\n",
    "    try:\n",
    "        google_key = userdata.get('GOOGLE_API_KEY')\n",
    "        os.environ['GOOGLE_API_KEY'] = google_key\n",
    "        print(\"‚úÖ GOOGLE_API_KEY loaded from Colab secrets\")\n",
    "    except:\n",
    "        if 'GOOGLE_API_KEY' not in os.environ:\n",
    "            print(\"‚ö†Ô∏è  GOOGLE_API_KEY not found in Colab secrets\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è  Not running in Google Colab - using environment variables\")\n",
    "\n",
    "# Option 1: Set API keys directly (if not using Colab secrets)\n",
    "# Uncomment and set your keys below if needed:\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-...'\n",
    "# os.environ['ANTHROPIC_API_KEY'] = 'sk-ant-...'\n",
    "# os.environ['GOOGLE_API_KEY'] = 'AI...'\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"API Key Status:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"OpenAI:    {'‚úÖ Configured' if os.environ.get('OPENAI_API_KEY') else '‚ùå Not set'}\")\n",
    "print(f\"Anthropic: {'‚úÖ Configured' if os.environ.get('ANTHROPIC_API_KEY') else '‚ùå Not set'}\")\n",
    "print(f\"Google:    {'‚úÖ Configured' if os.environ.get('GOOGLE_API_KEY') else '‚ùå Not set'}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚ÑπÔ∏è  You can run tournaments with any bots that have API keys configured.\")\n",
    "print(\"   SimpleBot, MediumBot, and AdvancedBot are always available and don't require API keys.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "## üìö Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "import_modules"
   },
   "source": [
    "import logging\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Any, Optional, Tuple, Union\n",
    "\n",
    "# Configure logging to see bot actions\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Import game components\n",
    "from reinforcetactics.core.game_state import GameState\n",
    "from reinforcetactics.game.bot import SimpleBot, MediumBot, AdvancedBot\n",
    "from reinforcetactics.utils.file_io import FileIO\n",
    "\n",
    "# Import LLM bots (with graceful fallback)\n",
    "llm_bots_available = {}\n",
    "\n",
    "try:\n",
    "    from reinforcetactics.game.llm_bot import OpenAIBot\n",
    "    llm_bots_available['openai'] = OpenAIBot\n",
    "    print(\"‚úÖ OpenAIBot available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  OpenAIBot not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from reinforcetactics.game.llm_bot import ClaudeBot\n",
    "    llm_bots_available['claude'] = ClaudeBot\n",
    "    print(\"‚úÖ ClaudeBot available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  ClaudeBot not available: {e}\")\n",
    "\n",
    "try:\n",
    "    from reinforcetactics.game.llm_bot import GeminiBot\n",
    "    llm_bots_available['gemini'] = GeminiBot\n",
    "    print(\"‚úÖ GeminiBot available\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  GeminiBot not available: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Imports complete! {len(llm_bots_available)} LLM bot types available.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elo_system"
   },
   "source": [
    "## üìä Elo Rating System\n",
    "\n",
    "The Elo rating system tracks bot performance across games, providing a skill-based ranking."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "elo_rating_class"
   },
   "source": [
    "# Import EloRatingSystem from the tournament library\n",
    "# This provides a unified implementation used across all tournament runners\n",
    "from reinforcetactics.tournament import EloRatingSystem\n",
    "\n",
    "# The EloRatingSystem class is now imported from the library\n",
    "# Key methods:\n",
    "#   - initialize_bot(name): Initialize a bot with starting Elo\n",
    "#   - update_ratings(bot1, bot2, result): Update ratings after a game\n",
    "#   - get_rating(name): Get current Elo rating\n",
    "#   - get_rating_change(name): Get rating change since start\n",
    "\n",
    "print(\"‚úÖ EloRatingSystem imported from reinforcetactics.tournament\")\n",
    "print(\"   Starting Elo: 1500\")\n",
    "print(\"   K-factor: 32\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "single_game"
   },
   "source": [
    "## üéÆ Single Game Runner\n",
    "\n",
    "Run a single game between two bots with detailed logging."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_single_game_function"
   },
   "source": [
    "def run_single_game(\n",
    "    player1_bot: Union[str, type],\n",
    "    player2_bot: Union[str, type],\n",
    "    map_file: str = 'maps/1v1/6x6_beginner.csv',\n",
    "    max_turns: int = 500,\n",
    "    verbose: bool = True,\n",
    "    player1_model: Optional[str] = None,\n",
    "    player2_model: Optional[str] = None,\n",
    "    player1_temperature: Optional[float] = None,\n",
    "    player2_temperature: Optional[float] = None,\n",
    "    player1_max_tokens: int = 8000,\n",
    "    player2_max_tokens: int = 8000,\n",
    "    player1_should_reason: bool = False,\n",
    "    player2_should_reason: bool = False,\n",
    "    log_conversations: bool = False,\n",
    "    conversation_log_dir: Optional[str] = None,\n",
    "    save_replay: bool = False,\n",
    "    replay_dir: Optional[str] = None\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Run a single game between two bots.\n",
    "\n",
    "    Args:\n",
    "        player1_bot: Bot class or 'simple'/'medium'/'advanced' for built-in bots (plays as Player 1)\n",
    "        player2_bot: Bot class or 'simple'/'medium'/'advanced' for built-in bots (plays as Player 2)\n",
    "        map_file: Path to map file (default: maps/1v1/6x6_beginner.csv)\n",
    "        max_turns: Maximum number of turns to prevent infinite games (default: 500)\n",
    "        verbose: Show turn-by-turn details (default: True)\n",
    "        player1_model: Optional model name for player 1 LLM bot\n",
    "        player2_model: Optional model name for player 2 LLM bot\n",
    "        player1_temperature: Optional temperature for player 1 LLM bot (0.0 to 1.0)\n",
    "        player2_temperature: Optional temperature for player 2 LLM bot (0.0 to 1.0)\n",
    "        player1_max_tokens: Max output tokens for player 1 LLM bot (default: 8000)\n",
    "        player2_max_tokens: Max output tokens for player 2 LLM bot (default: 8000)\n",
    "        player1_should_reason: Include reasoning field in player 1 LLM response format (default: False)\n",
    "        player2_should_reason: Include reasoning field in player 2 LLM response format (default: False)\n",
    "        log_conversations: Enable conversation logging for LLM bots (default: False)\n",
    "        conversation_log_dir: Directory for conversation logs (optional)\n",
    "        save_replay: Whether to save the game replay (default: False)\n",
    "        replay_dir: Directory for replay files (default: llm_replays/)\n",
    "\n",
    "    Returns:\n",
    "        Winner: 1 (player 1 wins), 2 (player 2 wins), or 0 (draw)\n",
    "    \"\"\"\n",
    "    # Load map\n",
    "    map_data = FileIO.load_map(map_file)\n",
    "    if map_data is None:\n",
    "        raise ValueError(f\"Failed to load map: {map_file}\")\n",
    "\n",
    "    # Create game state\n",
    "    game_state = GameState(map_data, num_players=2)\n",
    "\n",
    "    # Store map file reference for replay (required for replay system to load map)\n",
    "    game_state.map_file_used = map_file\n",
    "\n",
    "    # Handle conversation log directory\n",
    "    abs_log_dir = None\n",
    "    log_file_count_before = 0\n",
    "    if log_conversations and conversation_log_dir:\n",
    "        # Convert to absolute path\n",
    "        abs_log_dir = os.path.abspath(conversation_log_dir)\n",
    "        # Create directory if it doesn't exist\n",
    "        try:\n",
    "            os.makedirs(abs_log_dir, exist_ok=True)\n",
    "            if verbose:\n",
    "                print(f\"üìÅ Log directory created/verified: {abs_log_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not create log directory: {e}\")\n",
    "            abs_log_dir = None\n",
    "\n",
    "        # Count existing log files\n",
    "        if abs_log_dir and os.path.exists(abs_log_dir):\n",
    "            log_file_count_before = len([f for f in os.listdir(abs_log_dir) if f.endswith('.json')])\n",
    "\n",
    "    # Handle replay directory\n",
    "    abs_replay_dir = None\n",
    "    if save_replay:\n",
    "        # Use default directory if not specified\n",
    "        if replay_dir is None:\n",
    "            replay_dir = 'llm_replays'\n",
    "        # Convert to absolute path\n",
    "        abs_replay_dir = os.path.abspath(replay_dir)\n",
    "        # Create directory if it doesn't exist\n",
    "        try:\n",
    "            os.makedirs(abs_replay_dir, exist_ok=True)\n",
    "            if verbose:\n",
    "                print(f\"üìÅ Replay directory created/verified: {abs_replay_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Could not create replay directory: {e}\")\n",
    "            abs_replay_dir = None\n",
    "            save_replay = False\n",
    "\n",
    "    # Create bot instances\n",
    "    def create_bot(bot_spec, player_num, model=None, temperature=None, max_tokens=None, should_reason=False):\n",
    "        if bot_spec == 'simple' or bot_spec is None:\n",
    "            return SimpleBot(game_state, player_num)\n",
    "        elif bot_spec == 'medium':\n",
    "            return MediumBot(game_state, player_num)\n",
    "        elif bot_spec == 'advanced':\n",
    "            return AdvancedBot(game_state, player_num)\n",
    "        else:\n",
    "            # It's an LLM bot class\n",
    "            kwargs = {'game_state': game_state, 'player': player_num}\n",
    "            if model:\n",
    "                kwargs['model'] = model\n",
    "            if temperature is not None:\n",
    "                kwargs['temperature'] = temperature\n",
    "            kwargs['max_tokens'] = max_tokens\n",
    "            kwargs['should_reason'] = should_reason\n",
    "            if log_conversations:\n",
    "                kwargs['log_conversations'] = log_conversations\n",
    "            if abs_log_dir:\n",
    "                kwargs['conversation_log_dir'] = abs_log_dir\n",
    "            return bot_spec(**kwargs)\n",
    "\n",
    "    bot1 = create_bot(player1_bot, 1, player1_model, player1_temperature, player1_max_tokens, player1_should_reason)\n",
    "    bot2 = create_bot(player2_bot, 2, player2_model, player2_temperature, player2_max_tokens, player2_should_reason)\n",
    "    bots = {1: bot1, 2: bot2}\n",
    "\n",
    "    # Get bot names\n",
    "    bot1_name = bot1.__class__.__name__\n",
    "    bot2_name = bot2.__class__.__name__\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Game Start: {bot1_name} (P1) vs {bot2_name} (P2)\")\n",
    "        print(f\"Map: {map_file}\")\n",
    "        if player1_should_reason or player2_should_reason:\n",
    "            reasoning_info = []\n",
    "            if player1_should_reason:\n",
    "                reasoning_info.append(\"P1\")\n",
    "            if player2_should_reason:\n",
    "                reasoning_info.append(\"P2\")\n",
    "            print(f\"Reasoning Enabled: {', '.join(reasoning_info)}\")\n",
    "        if player1_temperature is not None or player2_temperature is not None:\n",
    "            temp_info = []\n",
    "            if player1_temperature is not None: temp_info.append(f\"P1={player1_temperature}\")\n",
    "            if player2_temperature is not None: temp_info.append(f\"P2={player2_temperature}\")\n",
    "            print(f\"Temperature: {', '.join(temp_info)}\")\n",
    "        if log_conversations:\n",
    "            print(f\"Conversation Logging: ENABLED\")\n",
    "            if abs_log_dir:\n",
    "                print(f\"üìÅ Absolute Log Path: {abs_log_dir}\")\n",
    "        if save_replay:\n",
    "            print(f\"Replay Saving: ENABLED\")\n",
    "            if abs_replay_dir:\n",
    "                print(f\"üé¨ Replay Directory: {abs_replay_dir}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    # Play the game\n",
    "    turn_count = 0\n",
    "    last_gold = {1: game_state.player_gold[1], 2: game_state.player_gold[2]}\n",
    "\n",
    "    # Import time for retry delays\n",
    "    import time\n",
    "\n",
    "    while not game_state.game_over and game_state.turn_number < max_turns:\n",
    "        current_player = game_state.current_player\n",
    "        current_bot = bots[current_player]\n",
    "        bot_name = bot1_name if current_player == 1 else bot2_name\n",
    "\n",
    "        # Identify if current bot is an LLM bot\n",
    "        is_llm_bot = current_bot.__class__.__name__ in ['OpenAIBot', 'ClaudeBot', 'GeminiBot']\n",
    "\n",
    "        # Show turn info\n",
    "        if verbose:\n",
    "            print(f\"\\n--- Turn {turn_count + 1} - {bot_name} (P{current_player}) ---\")\n",
    "            print(f\"  Gold: P1={game_state.player_gold[1]}, P2={game_state.player_gold[2]}\")\n",
    "\n",
    "        if is_llm_bot:\n",
    "            # Bot takes turn with retry logic for API limits (ONLY for LLM bots)\n",
    "            max_retries = 3\n",
    "            retry_delay = 5\n",
    "\n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    current_bot.take_turn()\n",
    "                    # ALWAYS wait 0.5s for LLM bots\n",
    "                    time.sleep(0.5)\n",
    "                    break # Success\n",
    "                except Exception as e:\n",
    "                    error_str = str(e)\n",
    "                    # Check for rate limit errors (429) or overloaded errors (503/529)\n",
    "                    is_rate_limit = \"429\" in error_str or \"Too Many Requests\" in error_str or \"Overloaded\" in error_str\n",
    "\n",
    "                    if is_rate_limit and attempt < max_retries:\n",
    "                        wait_time = retry_delay * (2 ** attempt) # Exponential backoff\n",
    "                        if verbose:\n",
    "                            print(f\"‚ö†Ô∏è  Rate limit/API error during {bot_name} turn: {e}\")\n",
    "                            print(f\"‚è≥ Waiting {wait_time}s before retry ({attempt+1}/{max_retries})...\")\n",
    "                        else:\n",
    "                            print(f\"‚è≥ Rate limit hit. Waiting {wait_time}s...\")\n",
    "                        time.sleep(wait_time)\n",
    "                    else:\n",
    "                        # Final failure or non-retriable error\n",
    "                        print(f\"‚ö†Ô∏è  Fatal error during {bot_name} turn: {e}\")\n",
    "                        # Bot forfeits on error\n",
    "                        game_state.game_over = True\n",
    "                        game_state.winner = 1 if current_player == 2 else 2\n",
    "                        break\n",
    "        else:\n",
    "            # Regular bot take turn (no retries, no forced delays)\n",
    "            try:\n",
    "                current_bot.take_turn()\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Fatal error during {bot_name} turn: {e}\")\n",
    "                game_state.game_over = True\n",
    "                game_state.winner = 1 if current_player == 2 else 2\n",
    "\n",
    "        # Show gold changes\n",
    "        if verbose and not game_state.game_over:\n",
    "            gold_change = game_state.player_gold[current_player] - last_gold[current_player]\n",
    "            if gold_change != 0:\n",
    "                print(f\"  Gold change: {gold_change:+d}\")\n",
    "            last_gold[current_player] = game_state.player_gold[current_player]\n",
    "\n",
    "        turn_count += 1\n",
    "\n",
    "        # Check for game over\n",
    "        if game_state.game_over:\n",
    "            break\n",
    "\n",
    "    # Determine winner\n",
    "    if game_state.game_over and game_state.winner:\n",
    "        winner = game_state.winner\n",
    "        winner_name = bot1_name if winner == 1 else bot2_name\n",
    "    elif game_state.turn_number >= max_turns:\n",
    "        # Draw due to turn limit - mark game as over\n",
    "        game_state.game_over = True\n",
    "        winner = 0\n",
    "        winner_name = \"Draw (max turns)\"\n",
    "    else:\n",
    "        winner = 0\n",
    "        winner_name = \"Draw\"\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Game Over! Winner: {winner_name}\")\n",
    "        print(f\"Total turns: {turn_count}\")\n",
    "        print(f\"Final gold - P1: {game_state.player_gold[1]}, P2: {game_state.player_gold[2]}\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Save replay if enabled\n",
    "    if save_replay and abs_replay_dir:\n",
    "        from datetime import datetime\n",
    "        from pathlib import Path\n",
    "\n",
    "        # Generate descriptive filename\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Build bot descriptions for filename\n",
    "        def get_bot_desc(bot_name, model):\n",
    "            if model:\n",
    "                # Clean model name for filename (sanitize to alphanumeric, dash, underscore)\n",
    "                import re\n",
    "                clean_model = re.sub(r'[^\\w\\-]', '-', model)\n",
    "                return f\"{bot_name}-{clean_model}\"\n",
    "            return bot_name\n",
    "\n",
    "        bot1_desc = get_bot_desc(bot1_name, player1_model)\n",
    "        bot2_desc = get_bot_desc(bot2_name, player2_model)\n",
    "\n",
    "        # Winner description\n",
    "        if winner == 1:\n",
    "            winner_desc = \"P1wins\"\n",
    "        elif winner == 2:\n",
    "            winner_desc = \"P2wins\"\n",
    "        else:\n",
    "            winner_desc = \"Draw\"\n",
    "\n",
    "        replay_filename = f\"game_{timestamp}_{bot1_desc}_vs_{bot2_desc}_{winner_desc}.json\"\n",
    "        replay_path = Path(abs_replay_dir) / replay_filename\n",
    "\n",
    "        try:\n",
    "            saved_path = game_state.save_replay_to_file(str(replay_path))\n",
    "            if saved_path and verbose:\n",
    "                print(f\"üé¨ Replay saved: {replay_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Error saving replay: {e}\")\n",
    "\n",
    "    # --- Force Save and Cleanup for Logs ---\n",
    "    # Ensure bots save their logs before we check for files\n",
    "    if log_conversations:\n",
    "        for bot in bots.values():\n",
    "            # Try common save method names if they exist\n",
    "            if hasattr(bot, 'save_conversation_log'):\n",
    "                try:\n",
    "                    bot.save_conversation_log()\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error saving bot logs: {e}\")\n",
    "\n",
    "    # Force cleanup of bot instances to trigger any __del__ log saving\n",
    "    del bot1\n",
    "    del bot2\n",
    "    del bots\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    # ----------------------------------------\n",
    "\n",
    "    # Show log file summary\n",
    "    if log_conversations and abs_log_dir and os.path.exists(abs_log_dir):\n",
    "        log_files = [f for f in os.listdir(abs_log_dir) if f.endswith('.json')]\n",
    "        new_log_count = len(log_files) - log_file_count_before\n",
    "\n",
    "        # If verbose is OFF, we still want a small indicator if logs were saved\n",
    "        if new_log_count > 0:\n",
    "            if verbose:\n",
    "                print(\"\\n\" + \"=\"*60)\n",
    "                print(\"üìä Conversation Logs Summary\")\n",
    "                print(\"=\"*60)\n",
    "                print(f\"‚úÖ Generated {new_log_count} new log file(s)\")\n",
    "                print(f\"üìÅ Absolute path: {abs_log_dir}\")\n",
    "                print(f\"üìù Total log files in directory: {len(log_files)}\")\n",
    "                print(f\"\\nüí° To list log files, run:\")\n",
    "                print(f\"   !ls -lh {abs_log_dir}\")\n",
    "                print(\"=\"*60 + \"\\n\")\n",
    "            else:\n",
    "                # Minimal output for tournaments\n",
    "                print(f\"    üìù +{new_log_count} log(s) saved\")\n",
    "\n",
    "    return winner"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tournament"
   },
   "source": [
    "## üèÜ Tournament Runner\n",
    "\n",
    "Run a round-robin tournament between multiple bots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_tournament_function"
   },
   "source": [
    "# Import tournament classes from the library\n",
    "from reinforcetactics.tournament import (\n",
    "    TournamentConfig,\n",
    "    TournamentRunner,\n",
    "    BotDescriptor,\n",
    "    BotType,\n",
    ")\n",
    "from reinforcetactics.tournament.bots import create_bot_instance\n",
    "\n",
    "# For backward compatibility, create wrapper classes\n",
    "TournamentBot = BotDescriptor  # Alias for compatibility\n",
    "\n",
    "def run_tournament(\n",
    "    bots,\n",
    "    maps=None,\n",
    "    map_pool_mode='all',\n",
    "    games_per_matchup=2,\n",
    "    max_turns=500,\n",
    "    output_dir='tournament_results',\n",
    "    save_replays=True,\n",
    "    log_conversations=False,\n",
    "    conversation_log_dir=None,\n",
    "    should_reason=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a round-robin tournament between bots.\n",
    "    \n",
    "    Args:\n",
    "        bots: List of BotDescriptor objects or (name, bot_class) tuples\n",
    "        maps: List of map file paths\n",
    "        map_pool_mode: 'all', 'cycle', or 'random'\n",
    "        games_per_matchup: Games per side (total = 2x this)\n",
    "        max_turns: Maximum turns per game\n",
    "        output_dir: Directory for results\n",
    "        save_replays: Whether to save game replays\n",
    "        log_conversations: Enable LLM conversation logging\n",
    "        conversation_log_dir: Directory for conversation logs\n",
    "        should_reason: Enable LLM reasoning output\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with tournament results\n",
    "    \"\"\"\n",
    "    if maps is None:\n",
    "        maps = ['maps/1v1/6x6_beginner.csv']\n",
    "    \n",
    "    # Convert legacy bot formats to BotDescriptor\n",
    "    bot_descriptors = []\n",
    "    for bot in bots:\n",
    "        if isinstance(bot, BotDescriptor):\n",
    "            bot_descriptors.append(bot)\n",
    "        elif isinstance(bot, tuple):\n",
    "            name, bot_class = bot[0], bot[1]\n",
    "            # Handle different bot types\n",
    "            if bot_class.__name__ == 'SimpleBot':\n",
    "                bot_descriptors.append(BotDescriptor.simple_bot(name))\n",
    "            elif bot_class.__name__ == 'MediumBot':\n",
    "                bot_descriptors.append(BotDescriptor.medium_bot(name))\n",
    "            elif bot_class.__name__ == 'AdvancedBot':\n",
    "                bot_descriptors.append(BotDescriptor.advanced_bot(name))\n",
    "            elif bot_class.__name__ in ('OpenAIBot', 'ClaudeBot', 'GeminiBot'):\n",
    "                provider = {\n",
    "                    'OpenAIBot': 'openai',\n",
    "                    'ClaudeBot': 'anthropic',\n",
    "                    'GeminiBot': 'google'\n",
    "                }.get(bot_class.__name__)\n",
    "                bot_descriptors.append(BotDescriptor.llm_bot(\n",
    "                    name=name,\n",
    "                    provider=provider,\n",
    "                    model=getattr(bot_class, 'default_model', None),\n",
    "                ))\n",
    "    \n",
    "    # Create config\n",
    "    config = TournamentConfig(\n",
    "        name='LLM Bot Tournament',\n",
    "        maps=maps,\n",
    "        map_pool_mode=map_pool_mode,\n",
    "        games_per_side=games_per_matchup,\n",
    "        max_turns=max_turns,\n",
    "        output_dir=output_dir,\n",
    "        save_replays=save_replays,\n",
    "        log_conversations=log_conversations,\n",
    "        conversation_log_dir=conversation_log_dir,\n",
    "        should_reason=should_reason,\n",
    "    )\n",
    "    \n",
    "    # Run tournament\n",
    "    runner = TournamentRunner(config)\n",
    "    results = runner.run(bot_descriptors)\n",
    "    \n",
    "    # Export results\n",
    "    runner.export_results()\n",
    "    \n",
    "    # Return results in legacy format for compatibility\n",
    "    return results.to_dict()\n",
    "\n",
    "print(\"‚úÖ Tournament functions imported from reinforcetactics.tournament\")\n",
    "print(\"   Use run_tournament() to run a tournament\")\n",
    "print(\"   Use BotDescriptor to create bot configurations\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_single_game"
   },
   "source": [
    "### Example 1: Single Game - SimpleBot vs SimpleBot\n",
    "\n",
    "Let's start with games between built-in bots to test the system. You can use 'simple', 'medium', or 'advanced' as bot types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "replay-saving-docs"
   },
   "source": [
    "### üé¨ Replay Saving\n",
    "\n",
    "Both `run_single_game()` and `run_tournament()` now support saving game replays!\n",
    "\n",
    "**Features:**\n",
    "- üìº Save replays to view later with the game's replay player\n",
    "- üìÅ Customizable replay directory (default: `llm_replays/`)\n",
    "- üè∑Ô∏è Descriptive filenames with timestamps, bot names, models, and game results\n",
    "- üó∫Ô∏è Replays include map reference for proper playback\n",
    "\n",
    "**Usage Examples:**\n",
    "\n",
    "```python\n",
    "# Single game with replay\n",
    "winner = run_single_game(\n",
    "    player1_bot='simple',\n",
    "    player2_bot='simple',\n",
    "    save_replay=True,\n",
    "    replay_dir='my_replays'  # Optional, defaults to 'llm_replays/'\n",
    ")\n",
    "\n",
    "# Tournament with replays\n",
    "results = run_tournament(\n",
    "    bots=[\n",
    "        ('SimpleBot', 'simple', None),\n",
    "        ('GPT-4o-mini', OpenAIBot, 'gpt-4o-mini')\n",
    "    ],\n",
    "    save_replays=True,\n",
    "    replay_dir='tournament_replays'\n",
    ")\n",
    "```\n",
    "\n",
    "**Replay Filename Format:**\n",
    "```\n",
    "game_20251217_014426_ClaudeBot-claude-3-haiku_vs_SimpleBot_P1wins.json\n",
    "```\n",
    "\n",
    "**Viewing Replays:**\n",
    "- Use the game's built-in replay player UI\n",
    "- Replays are saved as JSON files that can be loaded and viewed\n",
    "- Each replay includes the full game state and action history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "replay-example",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example: Run a single game with replay saving\n",
    "winner = run_single_game(\n",
    "    player1_bot='simple',\n",
    "    player2_bot='simple',\n",
    "    map_file='maps/1v1/corner_points.csv',\n",
    "    max_turns=10,\n",
    "    verbose=True,\n",
    "    save_replay=True,\n",
    "    replay_dir='llm_replays'  # Optional: defaults to 'llm_replays/'\n",
    ")\n",
    "\n",
    "print(f\"\\nWinner: Player {winner}\" if winner else \"\\nResult: Draw\")\n",
    "print(\"\\nüí° Check the 'llm_replays/' directory for the saved replay file!\")\n",
    "\n",
    "# Try different bot types:\n",
    "# winner = run_single_game(player1_bot='medium', player2_bot='simple', ...)\n",
    "# winner = run_single_game(player1_bot='advanced', player2_bot='medium', ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_llm_game"
   },
   "source": [
    "### Example 2: Single Game - LLM Bot vs Built-in Bots\n",
    "\n",
    "Test an LLM bot against built-in bots (SimpleBot, MediumBot, or AdvancedBot). Make sure you have the appropriate API key configured!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_example_llm"
   },
   "source": "# Run a single game: LLM Bot vs SimpleBot\n\n# Configuration: Choose 'gemini', 'openai', or 'claude'\nllm_provider = 'gemini'\n\n# Set logging level\nlogging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)\n\nif llm_provider in llm_bots_available:\n    # Select appropriate model based on provider\n    if llm_provider == 'openai':\n        model_name = 'gpt-5-mini-2025-08-07'\n    elif llm_provider == 'gemini':\n        model_name = 'gemini-2.5-flash'\n    elif llm_provider == 'claude':\n        model_name = 'claude-haiku-4-5-20251001'\n    else:\n        model_name = None\n\n    print(f\"üéÆ Starting game with {llm_provider} bot (model: {model_name})...\")\n\n    winner = run_single_game(\n        player1_bot=llm_bots_available[llm_provider],\n        player2_bot='simple',\n        map_file='maps/1v1/beginner.csv',\n        max_turns=20,\n        verbose=True,\n        player1_temperature=0.5,\n        player1_model=model_name,\n        player1_should_reason=True,  # Enable reasoning to see LLM's strategy\n        log_conversations=True,\n        save_replay=True,\n        conversation_log_dir=\"llm_logs\"\n    )\n    print(f\"\\nWinner: Player {winner}\" if winner else \"\\nResult: Draw\")\nelse:\n    print(f\"‚ö†Ô∏è  {llm_provider} bot not available. Please install necessary package and configure API key.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_logging"
   },
   "source": [
    "### Example 2.5: Single Game with Conversation Logging\n",
    "\n",
    "Enable conversation logging to see the LLM's reasoning process. Logs are saved as JSON files."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "import google.colab.drive\n",
    "from google.colab import drive\n",
    "\n",
    "env_str =  \"ReinforceTactics\"\n",
    "log_dir = \"\"\n",
    "parent_path = \"\"\n",
    "use_google_drive = True\n",
    "if use_google_drive:\n",
    "    parent_path = \"/content/gdrive\"\n",
    "    google.colab.drive.mount(parent_path, force_remount=True)\n",
    "    log_dir = \"{}/MyDrive/Finding Theta/logs/{}\".format(parent_path, env_str)\n",
    "else:\n",
    "    log_dir = \"/content/logs/{}\".format(env_str)\n",
    "\n",
    "tournamnet_log_dir = os.path.join(log_dir, 'tournaments')\n",
    "time_folder = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "run_folder = os.path.join(tournamnet_log_dir, time_folder)\n",
    "replay_dir = os.path.join(run_folder, 'replays')\n",
    "conversation_log_dir = os.path.join(run_folder, 'llm_logs')\n",
    "\n",
    "#Create Folders\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(tournamnet_log_dir, exist_ok=True)\n",
    "os.makedirs(run_folder, exist_ok=True)\n",
    "os.makedirs(replay_dir, exist_ok=True)\n",
    "os.makedirs(conversation_log_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Logs will be saved to: {run_folder}\")\n",
    "print(f\"üìÅ Replay files will be saved to: {replay_dir}\")\n",
    "print(f\"üìÅ Conversation logs will be saved to: {conversation_log_dir}\")"
   ],
   "metadata": {
    "id": "elJQhQauHlyr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_example_logging"
   },
   "source": [
    "# Run a single game with conversation logging enabled\n",
    "# Uncomment and run if you have OpenAI API key configured\n",
    "\n",
    "# if 'openai' in llm_bots_available:\n",
    "#     import tempfile\n",
    "#     import logging\n",
    "#\n",
    "#     # Enable DEBUG logging to see conversation logs\n",
    "#     logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)\n",
    "#\n",
    "#     # Create a temporary directory for logs\n",
    "#     with tempfile.TemporaryDirectory() as tmpdir:\n",
    "#         print(f\"Conversation logs will be saved to: {tmpdir}\")\n",
    "#\n",
    "#         winner = run_single_game(\n",
    "#             player1_bot=llm_bots_available['openai'],\n",
    "#             player2_bot='simple',\n",
    "#             map_file='maps/1v1/6x6_beginner.csv',\n",
    "#             max_turns=100,\n",
    "#             verbose=True,\n",
    "#             player1_model='gpt-4o-mini',\n",
    "#             log_conversations=True,\n",
    "#             conversation_log_dir=tmpdir\n",
    "#         )\n",
    "#\n",
    "#         # List the log files\n",
    "#         import os\n",
    "#         log_files = [f for f in os.listdir(tmpdir) if f.endswith('.json')]\n",
    "#         print(f\"\\nüìù Generated {len(log_files)} conversation log files\")\n",
    "#\n",
    "#         # Show a sample log entry\n",
    "#         if log_files:\n",
    "#             import json\n",
    "#             with open(os.path.join(tmpdir, log_files[0])) as f:\n",
    "#                 sample_log = json.load(f)\n",
    "#             print(f\"\\nüìÑ Sample log entry:\")\n",
    "#             print(json.dumps(sample_log, indent=2)[:500] + \"...\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è  OpenAIBot not available. Please install openai and configure API key.\")\n",
    "\n",
    "print(\"Uncomment the code above to run a game with conversation logging\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0Tql9ikGV7T"
   },
   "source": [
    "### Example 2.6: Reviewing Logged Conversations\n",
    "\n",
    "Learn how to review and analyze the logged conversations from your games. This example shows how to:\n",
    "- Use a persistent log directory for Google Colab\n",
    "- List and inspect log files (now one file per game)\n",
    "- Extract key information from conversation logs\n",
    "- Pretty-print log content for analysis\n",
    "\n",
    "**New in this version:**\n",
    "- Each game now creates a **single** log file containing all turns\n",
    "- Filename format: `game_{session_id}_player{N}_model{model}.json`\n",
    "- Makes it easier to review an entire game's decision-making process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04s3ROJVGV7T"
   },
   "outputs": [],
   "source": [
    "# Example 2.6: Reviewing Logged Conversations\n",
    "# Run a game with persistent logging and then review the logs\n",
    "# Uncomment and run if you have OpenAI API key configured\n",
    "\n",
    "# if 'openai' in llm_bots_available:\n",
    "#     import os\n",
    "#     import json\n",
    "#     from pathlib import Path\n",
    "#     import logging\n",
    "#\n",
    "#     # Enable DEBUG logging to see conversation logs\n",
    "#     logging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.DEBUG)\n",
    "#\n",
    "#     # Create persistent log directory (recommended for Google Colab)\n",
    "#     log_dir = '/content/llm_logs/'\n",
    "#     os.makedirs(log_dir, exist_ok=True)\n",
    "#     print(f\"üìÅ Logs will be saved to: {os.path.abspath(log_dir)}\\n\")\n",
    "#\n",
    "#     # Run game with logging\n",
    "#     print(\"üéÆ Running game with conversation logging...\\n\")\n",
    "#     winner = run_single_game(\n",
    "#         player1_bot=llm_bots_available['openai'],\n",
    "#         player2_bot='simple',\n",
    "#         map_file='maps/1v1/6x6_beginner.csv',\n",
    "#         max_turns=50,  # Shorter game for demo\n",
    "#         verbose=True,\n",
    "#         player1_model='gpt-4o-mini',\n",
    "#         log_conversations=True,\n",
    "#         conversation_log_dir=log_dir\n",
    "#     )\n",
    "#\n",
    "#     # Helper function to review conversation logs\n",
    "#     def review_conversation_logs(log_dir):\n",
    "#         \"\"\"Review and display game conversation logs.\"\"\"\n",
    "#         log_path = Path(log_dir)\n",
    "#         # NEW: Logs are now stored as game_{session_id}_player{N}_model{model}.json\n",
    "#         # Each file contains ALL turns from a single game session\n",
    "#         log_files = sorted(log_path.glob('game_*.json'))\n",
    "#\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"üìä CONVERSATION LOGS REVIEW\")\n",
    "#         print(\"=\"*70)\n",
    "#         print(f\"üìÅ Absolute path: {log_path.absolute()}\")\n",
    "#         print(f\"üìù Total game log files: {len(log_files)}\")\n",
    "#         print(f\"üí° Note: Each file now contains ALL turns from one game\\n\")\n",
    "#\n",
    "#         if not log_files:\n",
    "#             print(\"‚ö†Ô∏è  No log files found. Make sure logging was enabled.\")\n",
    "#             return log_files\n",
    "#\n",
    "#         # Show first game log in detail\n",
    "#         log_file = log_files[0]\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(f\"üìÑ Game log file: {log_file.name}\")\n",
    "#         print(f\"üìÅ Full path: {log_file.absolute()}\")\n",
    "#         print('='*70)\n",
    "#\n",
    "#         with open(log_file) as f:\n",
    "#             log_data = json.load(f)\n",
    "#\n",
    "#         # Display game metadata\n",
    "#         print(f\"üéÆ Game Session ID: {log_data.get('game_session_id', 'N/A')}\")\n",
    "#         print(f\"ü§ñ Model: {log_data.get('model', 'N/A')}\")\n",
    "#         print(f\"üè¢ Provider: {log_data.get('provider', 'N/A')}\")\n",
    "#         print(f\"üë§ Player: {log_data.get('player', 'N/A')}\")\n",
    "#         print(f\"üïê Start Time: {log_data.get('start_time', 'N/A')}\")\n",
    "#         print(f\"üî¢ Total Turns: {len(log_data.get('turns', []))}\")\n",
    "#\n",
    "#         # Show sample turns\n",
    "#         turns = log_data.get('turns', [])\n",
    "#         if turns:\n",
    "#             print(f\"\\n{'='*70}\")\n",
    "#             print(\"üìù Sample Turns (first and last)\")\n",
    "#             print(\"=\"*70)\n",
    "#\n",
    "#             for idx in [0, -1] if len(turns) > 1 else [0]:\n",
    "#                 turn = turns[idx]\n",
    "#                 print(f\"\\nüî¢ Turn {turn.get('turn_number', 'N/A')}:\")\n",
    "#                 print(f\"  üïê Timestamp: {turn.get('timestamp', 'N/A')}\")\n",
    "#\n",
    "#                 # Extract reasoning from response\n",
    "#                 response = turn.get('assistant_response', '')\n",
    "#                 try:\n",
    "#                     response_json = json.loads(response)\n",
    "#                     reasoning = response_json.get('reasoning', 'N/A')\n",
    "#                     actions = response_json.get('actions', [])\n",
    "#\n",
    "#                     print(f\"  ÔøΩÔøΩ Reasoning: {reasoning[:150]}...\")\n",
    "#                     print(f\"  ‚ö° Actions: {len(actions)} action(s)\")\n",
    "#\n",
    "#                     if actions:\n",
    "#                         action_types = [a.get('type', 'unknown') for a in actions[:3]]\n",
    "#                         print(f\"     First few: {', '.join(action_types)}\")\n",
    "#                 except (json.JSONDecodeError, Exception) as e:\n",
    "#                     print(f\"  Response: {response[:100]}...\")\n",
    "#\n",
    "#         print(f\"\\n{'='*70}\")\n",
    "#         print(\"üí° Tips:\")\n",
    "#         print(\"  - Each game_*.json file contains ALL turns from that game\")\n",
    "#         print(\"  - System prompt is stored once at the top of the file\")\n",
    "#         print(\"  - Use 'jq' for advanced JSON parsing: !jq . path/to/log.json\")\n",
    "#         print(\"  - Compare reasoning across different games/models\")\n",
    "#         print(\"=\"*70 + \"\\n\")\n",
    "#\n",
    "#         return log_files\n",
    "#\n",
    "#     # Review the logs\n",
    "#     log_files = review_conversation_logs(log_dir)\n",
    "#\n",
    "#     # Optional: Find games by session ID\n",
    "#     def find_game_log(log_dir, session_id):\n",
    "#         \"\"\"Find log file for a specific game session.\"\"\"\n",
    "#         log_path = Path(log_dir)\n",
    "#         for log_file in log_path.glob('game_*.json'):\n",
    "#             if session_id in log_file.name:\n",
    "#                 return log_file\n",
    "#         return None\n",
    "#\n",
    "#     print(\"\\nüìå Helper functions defined:\")\n",
    "#     print(\"  - review_conversation_logs(log_dir): Review game logs\")\n",
    "#     print(\"  - find_game_log(log_dir, session_id): Find specific game\")\n",
    "# else:\n",
    "#     print(\"‚ö†Ô∏è  OpenAIBot not available. Please install openai and configure API key.\")\n",
    "\n",
    "print(\"Uncomment the code above to run Example 2.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "example_tournament"
   },
   "source": [
    "### Example 3: Mini Tournament\n",
    "\n",
    "Run a small tournament with available bots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_example_tournament"
   },
   "source": "# # Define tournament participants\n# # Format: (display_name, bot_class_or_'simple', optional_model_name)\n\n# tournament_bots = [\n#     ('SimpleBot', 'simple', None),\n#     ('MediumBot', 'medium', None),\n#     ('AdvancedBot', 'advanced', None),\n# ]\n\n# # Add LLM bots if available and configured\n# if 'openai' in llm_bots_available and os.environ.get('OPENAI_API_KEY'):\n#     tournament_bots.append(('GPT-5 Mini', llm_bots_available['openai'], 'gpt-5-mini-2025-08-07'))\n\n# if 'claude' in llm_bots_available and os.environ.get('ANTHROPIC_API_KEY'):\n#     tournament_bots.append(('Claude Haiku 4.5', llm_bots_available['claude'], 'claude-haiku-4-5-20251001'))\n\n# if 'gemini' in llm_bots_available and os.environ.get('GOOGLE_API_KEY'):\n#     tournament_bots.append(('Gemini Flash', llm_bots_available['gemini'], 'gemini-2.5-flash'))\n\n# # Run tournament if we have at least 2 bots\n# if len(tournament_bots) >= 2:\n#     results = run_tournament(\n#         bots=tournament_bots,\n#         map_file='maps/1v1/beginner.csv',\n#         games_per_matchup=2,  # 2 games per side = 4 total per matchup\n#         max_turns=100\n#     )\n#     print(\"\\n‚úÖ Tournament complete! Results saved in 'results' variable.\")\n# else:\n#     print(\"‚ö†Ô∏è  Need at least 2 bots for a tournament.\")\n#     print(\"   Configure API keys for LLM bots or add more SimpleBots for testing.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_models"
   },
   "source": [
    "## üé® Custom Model Configuration\n",
    "\n",
    "You can specify different models for each LLM provider. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "openai_models"
   },
   "source": "### OpenAI Models\n\n**Available models:**\n- `gpt-5.2` - Flagship reasoning model, most capable\n- `gpt-5-mini-2025-08-07` (default) - Good balance of cost and performance\n- `gpt-5-nano-2025-08-07` - Smallest, fastest, cheapest\n- `gpt-5-2025-08-07` - Full GPT-5 base model\n\n**Example:**\n```python\n# Using GPT-5.2 for strongest gameplay\nwinner = run_single_game(\n    player1_bot=OpenAIBot,\n    player2_bot='simple',\n    player1_model='gpt-5.2'\n)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "claude_models"
   },
   "source": "### Claude Models\n\n**Available models:**\n- `claude-opus-4-6` - Most intelligent, exceptional coding/reasoning\n- `claude-sonnet-4-5-20250929` - Best speed/intelligence balance\n- `claude-haiku-4-5-20251001` (default) - Fastest, near-frontier intelligence\n- `claude-opus-4-5-20251101` - Previous generation Opus\n- `claude-3-5-sonnet-20241022` - Legacy, still supported\n\n**Example:**\n```python\n# Using Claude Opus 4.6 for strongest strategic play\nwinner = run_single_game(\n    player1_bot=ClaudeBot,\n    player2_bot='simple',\n    player1_model='claude-opus-4-6'\n)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gemini_models"
   },
   "source": "### Gemini Models\n\n**Available models:**\n- `gemini-3-pro-preview` - Latest generation, most capable\n- `gemini-3-flash-preview` - Latest generation, fast\n- `gemini-2.5-flash` (default) - Production, fast and efficient with thinking\n- `gemini-2.5-pro` - Production, more capable\n- `gemini-2.5-flash-lite` - Budget option\n\n**Note:** Gemini 2.0 models are deprecated and shutting down March 31, 2026. Gemini 1.x models are already retired.\n\n**Example:**\n```python\n# Using Gemini 3 Flash for latest generation performance\nwinner = run_single_game(\n    player1_bot=GeminiBot,\n    player2_bot='simple',\n    player1_model='gemini-3-flash-preview'\n)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_tournament_example"
   },
   "source": [
    "### Example: Tournament with Custom Models\n",
    "\n",
    "Compare different models from different providers:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_custom_tournament"
   },
   "source": "# Advanced tournament: Compare different models\n# Only run this if you have all API keys configured and don't mind the cost!\n\n# Set logging level\nlogging.getLogger('reinforcetactics.game.llm_bot').setLevel(logging.CRITICAL)\n\n# Define bots using the new TournamentBot class\nadvanced_bots = [\n    TournamentBot('SimpleBot', 'simple'),\n    TournamentBot('MediumBot', 'medium'),\n    TournamentBot('AdvancedBot', 'advanced'),\n    TournamentBot('Claude Haiku 4.5', ClaudeBot, 'claude-haiku-4-5-20251001', temperature=0.5, max_tokens=64_000),\n    TournamentBot('Claude Sonnet 4.5', ClaudeBot, 'claude-sonnet-4-5-20250929', temperature=0.5, max_tokens=64_000),\n    TournamentBot('Claude Opus 4.6', ClaudeBot, 'claude-opus-4-6', temperature=0.5, max_tokens=64_000),\n    TournamentBot('Gemini 2.5 Flash', GeminiBot, 'gemini-2.5-flash', temperature=0.5, max_tokens=8_192),\n    TournamentBot('Gemini 3 Flash', GeminiBot, 'gemini-3-flash-preview', temperature=0.5, max_tokens=65_536),\n    TournamentBot('GPT-5 Mini', OpenAIBot, 'gpt-5-mini-2025-08-07', max_tokens=8_000),\n    TournamentBot('GPT-5.2', OpenAIBot, 'gpt-5.2', max_tokens=16_000),\n]\n\nmaps=['maps/1v1/beginner.csv',\n      'maps/1v1/funnel_point.csv',\n      'maps/1v1/center_mountains.csv',\n      'maps/1v1/corner_points.csv']\n\nresults = run_tournament(\n    advanced_bots,\n    maps=maps,\n    games_per_matchup=1,\n    map_pool_mode='all',\n    max_turns=30,\n    should_reason=True,  # Enable reasoning for all LLM bots\n    log_conversations=True,\n    conversation_log_dir=conversation_log_dir,\n    save_replays=True,\n    replay_dir=replay_dir\n)\n\nprint(\"Uncomment the code above to run a full model comparison tournament\")\nprint(\"‚ö†Ô∏è  Warning: This will make many API calls and may incur costs!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "documentation"
   },
   "source": [
    "## üìñ Documentation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "29bdb22b"
   },
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import reinforcetactics\n",
    "\n",
    "def analyze_token_usage(log_dir):\n",
    "    \"\"\"Analyze token usage from log files in the specified directory.\"\"\"\n",
    "    if not log_dir or not os.path.exists(log_dir):\n",
    "        return None\n",
    "\n",
    "    token_stats = defaultdict(lambda: {'input_tokens': 0, 'output_tokens': 0, 'count': 0})\n",
    "\n",
    "    # Walk through log directory\n",
    "    for root, _, files in os.walk(log_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith('.json') or not file.startswith('game_'):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                model = data.get('model', 'unknown')\n",
    "\n",
    "                # Try to extract usage from turns\n",
    "                turns = data.get('turns', [])\n",
    "                file_input_tokens = 0\n",
    "                file_output_tokens = 0\n",
    "\n",
    "                for turn in turns:\n",
    "                    # Check common usage locations (OpenAI, Anthropic, Gemini patterns)\n",
    "                    usage = turn.get('usage') or turn.get('token_usage') or turn.get('usage_metadata')\n",
    "\n",
    "                    if usage and isinstance(usage, dict):\n",
    "                        # Input Tokens\n",
    "                        file_input_tokens += (usage.get('prompt_tokens') or\n",
    "                                            usage.get('input_tokens') or\n",
    "                                            usage.get('prompt_token_count') or 0)\n",
    "                        # Output Tokens\n",
    "                        file_output_tokens += (usage.get('completion_tokens') or\n",
    "                                             usage.get('output_tokens') or\n",
    "                                             usage.get('candidates_token_count') or 0)\n",
    "\n",
    "                if file_input_tokens > 0 or file_output_tokens > 0:\n",
    "                    token_stats[model]['input_tokens'] += file_input_tokens\n",
    "                    token_stats[model]['output_tokens'] += file_output_tokens\n",
    "                    token_stats[model]['count'] += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error reading log file {file}: {e}\")\n",
    "\n",
    "    return dict(token_stats)\n",
    "\n",
    "def save_tournament_results(results_data, output_dir='tournament_results', llm_log_dir=None):\n",
    "    \"\"\"Save tournament results to CSV and JSON files with metadata.\"\"\"\n",
    "    if not results_data:\n",
    "        print(\"‚ö†Ô∏è No results data to save.\")\n",
    "        return\n",
    "\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Get library version\n",
    "    rt_version = getattr(reinforcetactics, '__version__', 'unknown')\n",
    "\n",
    "    # Prepare data with metadata\n",
    "    # Create a shallow copy to avoid modifying the original dict\n",
    "    data_to_save = results_data.copy()\n",
    "    data_to_save['metadata'] = {\n",
    "        'reinforcetactics_version': rt_version,\n",
    "        'timestamp': timestamp,\n",
    "        'export_time': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Analyze token usage if log directory is provided\n",
    "    if llm_log_dir:\n",
    "        print(f\"üìä Analyzing token usage from: {llm_log_dir}\")\n",
    "        token_stats = analyze_token_usage(llm_log_dir)\n",
    "        if token_stats:\n",
    "            data_to_save['token_stats'] = token_stats\n",
    "            print(\"‚úÖ Token usage analysis complete\")\n",
    "\n",
    "            # Create DataFrame for display\n",
    "            token_data = []\n",
    "            for model, stats in token_stats.items():\n",
    "                token_data.append({\n",
    "                    'Model': model,\n",
    "                    'Input Tokens': stats['input_tokens'],\n",
    "                    'Output Tokens': stats['output_tokens'],\n",
    "                    'Total Tokens': stats['input_tokens'] + stats['output_tokens'],\n",
    "                    'Games Logged': stats['count']\n",
    "                })\n",
    "\n",
    "            if token_data:\n",
    "                token_df = pd.DataFrame(token_data)\n",
    "                print(\"\\nüí∞ Token Usage Summary:\")\n",
    "                display(token_df)\n",
    "\n",
    "                # Save token stats to CSV\n",
    "                token_csv_path = os.path.join(output_dir, f'token_usage_{timestamp}.csv')\n",
    "                token_df.to_csv(token_csv_path, index=False)\n",
    "                print(f\"‚úÖ Token stats saved to: {token_csv_path}\")\n",
    "\n",
    "    # 1. Save full results to JSON\n",
    "    json_path = os.path.join(output_dir, f'tournament_results_{timestamp}.json')\n",
    "    try:\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(data_to_save, f, indent=2)\n",
    "        print(f\"‚úÖ Full results saved to: {json_path}\")\n",
    "        # print(f\"‚ÑπÔ∏è  Library Version: {rt_version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error saving JSON: {e}\")\n",
    "\n",
    "    # 2. Save standings to CSV\n",
    "    if 'standings' in results_data:\n",
    "        csv_path = os.path.join(output_dir, f'tournament_standings_{timestamp}.csv')\n",
    "        try:\n",
    "            df = pd.DataFrame(results_data['standings'])\n",
    "            df.to_csv(csv_path, index=False)\n",
    "            print(f\"‚úÖ Standings saved to: {csv_path}\")\n",
    "            print(\"\\nPreview of CSV:\")\n",
    "            display(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error saving CSV: {e}\")\n",
    "\n",
    "    # 3. Save Matchups to CSV\n",
    "    if 'matchups' in results_data and results_data['matchups']:\n",
    "        matchups_csv_path = os.path.join(output_dir, f'tournament_matchups_{timestamp}.csv')\n",
    "        try:\n",
    "            matchups_df = pd.DataFrame(results_data['matchups'])\n",
    "            # Rename columns for better readability\n",
    "            matchups_df_renamed = matchups_df.rename(columns={\n",
    "                'bot1': 'Bot 1',\n",
    "                'bot2': 'Bot 2',\n",
    "                'bot1_wins': 'Bot 1 Wins',\n",
    "                'bot2_wins': 'Bot 2 Wins',\n",
    "                'draws': 'Draws'\n",
    "            })\n",
    "\n",
    "            matchups_df_renamed.to_csv(matchups_csv_path, index=False)\n",
    "            print(f\"‚úÖ Matchups saved to: {matchups_csv_path}\")\n",
    "            # print(\"\\nPreview of Matchups CSV:\")\n",
    "            # display(matchups_df_renamed)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error saving Matchups CSV: {e}\")\n",
    "\n",
    "        # 4. Save Matrix/Cross Table CSV (NEW)\n",
    "        matrix_csv_path = os.path.join(output_dir, f'tournament_matrix_{timestamp}.csv')\n",
    "        try:\n",
    "            # Extract unique bots\n",
    "            bots = set()\n",
    "            for m in results_data['matchups']:\n",
    "                bots.add(m['bot1'])\n",
    "                bots.add(m['bot2'])\n",
    "            bots = sorted(list(bots))\n",
    "\n",
    "            # Create empty DataFrame\n",
    "            matrix_df = pd.DataFrame(index=bots, columns=bots)\n",
    "            # Initialize with \"0-0-0\" for no games played\n",
    "            matrix_df = matrix_df.fillna(\"0-0-0\")\n",
    "\n",
    "            # Fill with W-L-D strings\n",
    "            for m in results_data['matchups']:\n",
    "                b1 = m['bot1']\n",
    "                b2 = m['bot2']\n",
    "                w1 = int(m['bot1_wins'])\n",
    "                w2 = int(m['bot2_wins'])\n",
    "                d = int(m['draws'])\n",
    "\n",
    "                # Format: Wins-Losses-Draws from the perspective of the row bot\n",
    "                matrix_df.loc[b1, b2] = f\"{w1}-{w2}-{d}\"\n",
    "                matrix_df.loc[b2, b1] = f\"{w2}-{w1}-{d}\"\n",
    "\n",
    "            # Mark diagonal\n",
    "            for b in bots:\n",
    "                matrix_df.loc[b, b] = 'X'\n",
    "\n",
    "            matrix_df.to_csv(matrix_csv_path)\n",
    "            print(f\"‚úÖ Matrix table saved to: {matrix_csv_path}\")\n",
    "            print(\"\\nPreview of Tournament Matrix (W-L-D):\")\n",
    "            display(matrix_df)\n",
    "        except Exception as e:\n",
    "             print(f\"‚ö†Ô∏è Error saving Matrix CSV: {e}\")\n",
    "\n",
    "# Save the results from the previous tournament\n",
    "if 'results' in locals():\n",
    "    # Use run_folder if defined, otherwise default\n",
    "    out_dir = run_folder if 'run_folder' in locals() else 'tournament_results'\n",
    "    # Use conversation_log_dir if defined\n",
    "    llm_log_dir = conversation_log_dir if 'conversation_log_dir' in locals() else None\n",
    "\n",
    "    save_tournament_results(results, out_dir, llm_log_dir=llm_log_dir)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è 'results' variable not found. Run a tournament first!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cost_estimates"
   },
   "source": "### API Cost Estimates\n\n**OpenAI Pricing (approximate):**\n- GPT-5 Mini: ~$0.40/1M input tokens, ~$1.60/1M output tokens\n- GPT-5.2: ~$10/1M input tokens, ~$30/1M output tokens\n- GPT-5 Nano: ~$0.15/1M input tokens\n- Typical game: 2,000-10,000 tokens per side\n- **Cost per game:** $0.0001-0.001 (nano/mini), $0.01-0.05 (5.2)\n\n**Anthropic Pricing:**\n- Claude Haiku 4.5: $1/1M input tokens, $5/1M output tokens\n- Claude Sonnet 4.5: $3/1M input tokens, $15/1M output tokens\n- Claude Opus 4.6: $5/1M input tokens, $25/1M output tokens\n- **Cost per game:** $0.001-0.005 (Haiku), $0.005-0.02 (Sonnet), $0.01-0.04 (Opus)\n\n**Google Gemini Pricing:**\n- Gemini 2.5 Flash: ~$0.15/1M input, $0.60/1M output\n- Gemini 2.5 Flash Lite: ~$0.075/1M input, $0.30/1M output\n- Gemini 2.5 Pro: $1.25/1M input tokens, $5.00/1M output tokens\n- **Cost per game:** ~$0.0001-0.001 (Flash), $0.001-0.005 (Pro)\n\n**Tournament Cost Estimates:**\n- 3 bots, 2 games/matchup: 12 games total\n- Using mini/haiku/flash: ~$0.01-0.05 total\n- Using premium models: ~$0.10-0.50 total\n\n**Cost Saving Tips:**\n1. Use mini/haiku/flash models for development and testing\n2. Use smaller maps (6x6) which need fewer tokens\n3. Set lower `max_turns` to prevent long games\n4. Use Gemini Flash Lite for the most economical testing"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_tips"
   },
   "source": "### Performance Tips\n\n**Model Selection for Different Purposes:**\n\n**For Testing & Development:**\n- GPT-5 Mini - Good balance of cost and performance\n- Claude Haiku 4.5 - Very fast, near-frontier intelligence\n- Gemini 2.5 Flash - Fast with thinking, great for testing\n\n**For Competitive Play:**\n- Claude Opus 4.6 - Most intelligent, exceptional at coding/reasoning\n- GPT-5.2 - Flagship reasoning model\n- Gemini 3 Pro - Latest generation, complex reasoning\n- Claude Sonnet 4.5 - Excellent balance of speed and intelligence\n\n**For Budget-Friendly Research:**\n- Gemini 2.5 Flash Lite - Very economical\n- GPT-5 Nano - Smallest, fastest, cheapest\n- Claude Haiku 4.5 - Fast and affordable\n\n**Game Speed:**\n- Smaller maps (6x6) complete in 1-3 minutes per game\n- Larger maps (32x32) can take 10-30 minutes per game\n- LLM API calls add 1-5 seconds per turn\n- SimpleBot is nearly instant\n\n**Tournament Duration Estimates:**\n- 2 bots, 4 games: ~5-15 minutes\n- 3 bots, 12 games: ~15-45 minutes\n- 4 bots, 24 games: ~30-90 minutes\n- 5 bots, 40 games: ~1-2 hours\n\n**Optimization Strategies:**\n1. Start with 6x6 maps for quick iterations\n2. Use `games_per_matchup=1` for initial testing\n3. Set `max_turns=100` for faster games\n4. Run tournaments with fewer bots initially\n5. Use verbose=False in run_single_game() to reduce output"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "### üîß Troubleshooting\n",
    "\n",
    "**Problem: \"API key not provided\" error**\n",
    "- **Solution:** Make sure you've set the API key in the environment or Colab secrets\n",
    "- Check the API Key Configuration section output\n",
    "- Uncomment and set the API key directly in the configuration cell\n",
    "\n",
    "**Problem: \"openai package not installed\" error**\n",
    "- **Solution:** Run the installation cell again\n",
    "- Or manually install: `!pip install openai>=1.0.0`\n",
    "\n",
    "**Problem: Bot makes invalid moves or errors**\n",
    "- **Solution:** This is expected occasionally with LLMs\n",
    "- The game will skip invalid actions and continue\n",
    "- Try using a more capable model (e.g., GPT-4o instead of mini)\n",
    "- Check the logs to see what actions failed\n",
    "\n",
    "**Problem: Games taking too long**\n",
    "- **Solution:** Reduce `max_turns` parameter\n",
    "- Use smaller maps (6x6 instead of 32x32)\n",
    "- Faster models: Haiku, Flash, or mini\n",
    "\n",
    "**Problem: API rate limits exceeded**\n",
    "- **Solution:** Add delays between games\n",
    "- Use free tier models (Gemini Flash)\n",
    "- Reduce `games_per_matchup`\n",
    "- Spread tournament over multiple sessions\n",
    "\n",
    "**Problem: Out of memory error**\n",
    "- **Solution:** Restart the notebook runtime\n",
    "- Run fewer games at once\n",
    "- Use smaller maps\n",
    "\n",
    "**Problem: \"Map file not found\" error**\n",
    "- **Solution:** Make sure you're in the reinforce-tactics directory\n",
    "- Check the path: `!ls maps/1v1/`\n",
    "- Use absolute paths if needed\n",
    "\n",
    "**Problem: Import errors for LLM bots**\n",
    "- **Solution:** Check that dependencies installed correctly\n",
    "- Verify the repository was cloned successfully\n",
    "- Make sure the repository is in your Python path\n",
    "\n",
    "**Problem: Tournament results seem random**\n",
    "- **Solution:** LLMs have inherent randomness\n",
    "- Increase `games_per_matchup` for more stable results\n",
    "- Temperature parameter affects consistency (set in llm_bot.py)\n",
    "- SimpleBot is deterministic and provides a good baseline\n",
    "\n",
    "**Problem: Cost concerns**\n",
    "- **Solution:** Always use mini/haiku/flash for testing\n",
    "- Monitor API usage in your provider dashboard\n",
    "- Set spending limits in your API account\n",
    "- Test with SimpleBot first (free)\n",
    "- Use Gemini Flash free tier for unlimited testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üéì Next Steps\n",
    "\n",
    "**New Features Now Available:**\n",
    "1. ‚úÖ Elo rating system - Track bot skill across games\n",
    "2. ‚úÖ Multiple maps support - Run tournaments across different maps\n",
    "3. ‚úÖ Per-map statistics - See which bots excel on specific maps\n",
    "\n",
    "**Experiment Ideas:**\n",
    "1. Compare different models from the same provider\n",
    "2. Test how map size affects bot performance using multiple maps\n",
    "3. Analyze which bots excel at different strategies\n",
    "4. Track Elo rating progression over multiple tournaments\n",
    "5. Compare performance across maps of different sizes\n",
    "\n",
    "**Code Customization:**\n",
    "1. Modify system prompts in `llm_bot.py` for different strategies\n",
    "2. Add logging to track specific metrics\n",
    "3. Create visualization of tournament brackets\n",
    "4. Export results to CSV for analysis\n",
    "5. Build a web interface for live tournaments\n",
    "\n",
    "**Advanced Tournaments:**\n",
    "1. Swiss-system tournament format\n",
    "2. Double elimination brackets\n",
    "3. Time-limited games\n",
    "4. Asymmetric maps\n",
    "5. Team battles (coming soon)\n",
    "\n",
    "**Contributing:**\n",
    "- Found a bug? Open an issue on GitHub\n",
    "- Have an improvement? Submit a pull request\n",
    "- Share your tournament results!\n",
    "\n",
    "**Resources:**\n",
    "- Repository: https://github.com/kuds/reinforce-tactics\n",
    "- Game Rules: See `reinforcetactics/game/llm_bot.py` SYSTEM_PROMPT\n",
    "- Tournament Script: `scripts/tournament.py`\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Gaming! üéÆ**"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create version execution\n",
    "notebook_name = \"notebook.ipynb\"\n",
    "%notebook -e $notebook_name"
   ],
   "metadata": {
    "id": "TwgAMcko8uIp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "source_file = os.path.join(notebook_name)\n",
    "destination_file = os.path.join(run_folder, notebook_name)\n",
    "\n",
    "try:\n",
    "    shutil.copyfile(notebook_name, destination_file)\n",
    "    print(f\"File '{source_file}' copied to '{destination_file}' successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Source file '{source_file}' not found.\")\n",
    "except shutil.SameFileError:\n",
    "    print(f\"Error: Source and destination are the same file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ],
   "metadata": {
    "id": "Yy9frS9Q86Rm"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "llm_bot_tournament.ipynb",
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}