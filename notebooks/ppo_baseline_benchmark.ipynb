{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/ppo_baseline_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Reinforce Tactics — PPO Baseline Training Benchmarks\n",
    "\n",
    "This notebook trains a **MaskablePPO** agent against `SimpleBot` on the 6×6 beginner map\n",
    "and records reference metrics at four training checkpoints:\n",
    "\n",
    "| Checkpoint | Timesteps |\n",
    "|------------|-----------|\n",
    "| 1 | 10,000 |\n",
    "| 2 | 50,000 |\n",
    "| 3 | 200,000 |\n",
    "| 4 | 1,000,000 |\n",
    "\n",
    "At each checkpoint the agent is evaluated over **50 episodes** and we record:\n",
    "- **Win rate** (% of games won against SimpleBot)\n",
    "- **Average episode reward**\n",
    "- **Average episode length** (steps)\n",
    "\n",
    "The goal is to provide a **reference curve** so that users can run the same\n",
    "notebook and compare their results to known-good training runs.\n",
    "\n",
    "**Runtime:** CPU is fine (~20–40 min total). GPU will be faster.\n",
    "\n",
    "---\n",
    "\n",
    "### Why MaskablePPO?\n",
    "\n",
    "The game has a `MultiDiscrete` action space where many action combinations\n",
    "are invalid at any given time (e.g. you can’t attack a tile with no enemy).\n",
    "**Action masking** prevents the agent from sampling these invalid actions,\n",
    "which typically yields 2–3× faster convergence compared to plain PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q gymnasium stable-baselines3 sb3-contrib tensorboard pandas numpy torch matplotlib\n\nimport torch\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {DEVICE}\")\nif DEVICE == 'cuda':\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo and install as a package\nimport os, sys\nfrom pathlib import Path\n\nREPO_DIR = Path('reinforce-tactics')\nif REPO_DIR.exists():\n    os.chdir(REPO_DIR)\nelif Path('notebooks').exists():\n    # Already inside the repo\n    os.chdir('..')\nelse:\n    print('Cloning repository...')\n    !git clone https://github.com/kuds/reinforce-tactics.git\n    os.chdir(REPO_DIR)\n\n# Install the package so all imports resolve\n!pip install -q -e .\n\nif os.getcwd() not in sys.path:\n    sys.path.insert(0, os.getcwd())\n\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "\n",
    "from reinforcetactics.rl.masking import make_maskable_env, make_maskable_vec_env\n",
    "\n",
    "print('All imports successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "# --- Benchmark settings ---\nMAP_FILE        = 'maps/1v1/beginner.csv'   # 6x6 beginner map\nOPPONENT        = 'bot'                      # SimpleBot\nMAX_STEPS       = 500                        # max steps per episode\nN_ENVS          = 4                          # parallel training envs\nSEED            = 42\n\n# Action space mode:\n#   'flat_discrete'  — exact per-action masks (recommended, eliminates invalid actions)\n#   'multi_discrete' — per-dimension masks (over-approximation, original behaviour)\nACTION_SPACE    = 'flat_discrete'\n\n# Checkpoints to evaluate\nCHECKPOINTS     = [10_000, 50_000, 200_000, 1_000_000]\nEVAL_EPISODES   = 50                         # episodes per evaluation\n\n# PPO hyperparameters\nPPO_CONFIG = dict(\n    learning_rate = 3e-4,\n    n_steps       = 2048,\n    batch_size    = 64,\n    n_epochs      = 10,\n    gamma         = 0.99,\n    gae_lambda    = 0.95,\n    clip_range    = 0.2,\n    ent_coef      = 0.01,\n    vf_coef       = 0.5,\n    max_grad_norm = 0.5,\n)\n\n# Output paths\nBENCHMARK_DIR = Path('benchmarks/ppo_vs_simplebot')\nBENCHMARK_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f'Map:          {MAP_FILE}')\nprint(f'Opponent:     {OPPONENT}')\nprint(f'Action space: {ACTION_SPACE}')\nprint(f'Checkpoints:  {CHECKPOINTS}')\nprint(f'Eval eps:     {EVAL_EPISODES}')\nprint(f'Output dir:   {BENCHMARK_DIR}')"
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "## 4. Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-envs",
   "metadata": {},
   "outputs": [],
   "source": "# Training envs (vectorized, headless)\nvec_env = make_maskable_vec_env(\n    n_envs=N_ENVS,\n    map_file=MAP_FILE,\n    opponent=OPPONENT,\n    max_steps=MAX_STEPS,\n    seed=SEED,\n    use_subprocess=False,   # DummyVecEnv (safer in notebooks)\n    action_space_type=ACTION_SPACE,\n)\n\n# Separate eval env (single, deterministic)\neval_env = make_maskable_env(\n    map_file=MAP_FILE,\n    opponent=OPPONENT,\n    max_steps=MAX_STEPS,\n    action_space_type=ACTION_SPACE,\n)\n\nprint(f'Observation space: {vec_env.observation_space}')\nprint(f'Action space:      {vec_env.action_space}')"
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 5. Create MaskablePPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskablePPO(\n",
    "    'MultiInputPolicy',\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=str(BENCHMARK_DIR / 'tensorboard'),\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    **PPO_CONFIG,\n",
    ")\n",
    "\n",
    "print('MaskablePPO model created.')\n",
    "print(f'Policy:  {model.policy.__class__.__name__}')\n",
    "print(f'Device:  {model.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-fn-header",
   "metadata": {},
   "source": [
    "## 6. Evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, n_episodes=50):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model and return summary statistics.\n",
    "\n",
    "    Returns dict with: win_rate, avg_reward, std_reward,\n",
    "    avg_length, std_length, wins, losses, draws\n",
    "    \"\"\"\n",
    "    wins, losses, draws = 0, 0, 0\n",
    "    rewards, lengths = [], []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        while not done:\n",
    "            masks = env.action_masks()\n",
    "            action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            ep_len += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        lengths.append(ep_len)\n",
    "\n",
    "        winner = info.get('winner')\n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "        elif winner is not None:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return {\n",
    "        'win_rate':    wins / n_episodes,\n",
    "        'avg_reward':  float(np.mean(rewards)),\n",
    "        'std_reward':  float(np.std(rewards)),\n",
    "        'avg_length':  float(np.mean(lengths)),\n",
    "        'std_length':  float(np.std(lengths)),\n",
    "        'wins':        wins,\n",
    "        'losses':      losses,\n",
    "        'draws':       draws,\n",
    "    }\n",
    "\n",
    "print('evaluate_model() defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 7. Train and evaluate at each checkpoint\n",
    "\n",
    "We train incrementally: 0 → 10K → 50K → 200K → 1M timesteps,\n",
    "evaluating at each checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "trained_so_far = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for checkpoint_ts in CHECKPOINTS:\n",
    "    steps_to_train = checkpoint_ts - trained_so_far\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training {trained_so_far:,} -> {checkpoint_ts:,} '\n",
    "          f'({steps_to_train:,} steps)...')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.learn(\n",
    "        total_timesteps=steps_to_train,\n",
    "        reset_num_timesteps=False,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "    train_time = time.time() - t0\n",
    "    trained_so_far = checkpoint_ts\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt_path = BENCHMARK_DIR / f'model_{checkpoint_ts}.zip'\n",
    "    model.save(str(ckpt_path))\n",
    "    print(f'Saved checkpoint: {ckpt_path}')\n",
    "\n",
    "    # Evaluate\n",
    "    print(f'Evaluating over {EVAL_EPISODES} episodes...')\n",
    "    metrics = evaluate_model(model, eval_env, n_episodes=EVAL_EPISODES)\n",
    "    metrics['timesteps'] = checkpoint_ts\n",
    "    metrics['train_time_s'] = round(train_time, 1)\n",
    "    results.append(metrics)\n",
    "\n",
    "    print(f'  Win rate:       {metrics[\"win_rate\"]*100:.1f}%')\n",
    "    print(f'  Avg reward:     {metrics[\"avg_reward\"]:.1f} '\n",
    "          f'(+/- {metrics[\"std_reward\"]:.1f})')\n",
    "    print(f'  Avg length:     {metrics[\"avg_length\"]:.1f} '\n",
    "          f'(+/- {metrics[\"std_length\"]:.1f})')\n",
    "    print(f'  W/L/D:          {metrics[\"wins\"]}/{metrics[\"losses\"]}/{metrics[\"draws\"]}')\n",
    "    print(f'  Training time:  {train_time:.1f}s')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nTotal wall time: {total_time/60:.1f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 8. Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df['win_rate_pct'] = (df['win_rate'] * 100).round(1)\n",
    "df['avg_reward'] = df['avg_reward'].round(1)\n",
    "df['avg_length'] = df['avg_length'].round(1)\n",
    "\n",
    "display_df = df[['timesteps', 'win_rate_pct', 'avg_reward', 'avg_length',\n",
    "                  'wins', 'losses', 'draws', 'train_time_s']].copy()\n",
    "display_df.columns = ['Timesteps', 'Win Rate (%)', 'Avg Reward',\n",
    "                       'Avg Length', 'Wins', 'Losses', 'Draws',\n",
    "                       'Train Time (s)']\n",
    "display_df = display_df.set_index('Timesteps')\n",
    "display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plots-header",
   "metadata": {},
   "source": [
    "## 9. Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ts = [r['timesteps'] for r in results]\n",
    "\n",
    "# Win rate\n",
    "ax = axes[0]\n",
    "wr = [r['win_rate'] * 100 for r in results]\n",
    "ax.plot(ts, wr, 'o-', color='#2196F3', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title('Win Rate vs SimpleBot')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim(-5, 105)\n",
    "ax.axhline(y=70, color='green', linestyle='--', alpha=0.5, label='70% target')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Average reward\n",
    "ax = axes[1]\n",
    "avg_r = [r['avg_reward'] for r in results]\n",
    "std_r = [r['std_reward'] for r in results]\n",
    "ax.plot(ts, avg_r, 'o-', color='#FF9800', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_r, std_r)],\n",
    "                [a + s for a, s in zip(avg_r, std_r)],\n",
    "                alpha=0.2, color='#FF9800')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Reward')\n",
    "ax.set_title('Average Episode Reward')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode length\n",
    "ax = axes[2]\n",
    "avg_l = [r['avg_length'] for r in results]\n",
    "std_l = [r['std_length'] for r in results]\n",
    "ax.plot(ts, avg_l, 'o-', color='#4CAF50', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_l, std_l)],\n",
    "                [a + s for a, s in zip(avg_l, std_l)],\n",
    "                alpha=0.2, color='#4CAF50')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Length (steps)')\n",
    "ax.set_title('Average Episode Length')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('PPO Baseline Benchmarks  |  6x6 beginner map  |  vs SimpleBot',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(str(BENCHMARK_DIR / 'training_curves.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(f'Saved plot: {BENCHMARK_DIR / \"training_curves.png\"}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zxt9j70tka",
   "source": "## 9b. Diagnose training failures\n\nIf you see **0% win rate** and all games ending as **draws at 500 steps**, the agent\nis not learning to win — it's farming shaping rewards. Run the cell below to\ndiagnose the specific failure mode and get targeted recommendations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "gm8kecvhgil",
   "source": "def diagnose_training(results):\n    \"\"\"Analyze benchmark results and print a diagnosis.\"\"\"\n    if not results:\n        print(\"No results to analyze.\")\n        return\n\n    # --- Collect signals ---\n    win_rates = [r['win_rate'] for r in results]\n    avg_rewards = [r['avg_reward'] for r in results]\n    avg_lengths = [r['avg_length'] for r in results]\n    draw_counts = [r['draws'] for r in results]\n    loss_counts = [r['losses'] for r in results]\n    win_counts = [r['wins'] for r in results]\n    n_eval = results[0].get('wins', 0) + results[0].get('losses', 0) + results[0].get('draws', 0)\n\n    all_draws = all(d == n_eval for d in draw_counts)\n    all_max_len = all(abs(l - MAX_STEPS) < 1.0 for l in avg_lengths)\n    rewards_positive = all(r > 0 for r in avg_rewards)\n    rewards_declining = len(avg_rewards) >= 2 and avg_rewards[-1] < avg_rewards[0] * 0.7\n    no_wins = all(w == 0.0 for w in win_rates)\n    late_losses = loss_counts[-1] > 0 and all(lc == 0 for lc in loss_counts[:-1])\n    all_losses_late = loss_counts[-1] == n_eval if len(loss_counts) > 0 else False\n    negative_rewards = all(r < -1000 for r in avg_rewards[:-1]) if len(avg_rewards) > 1 else False\n\n    print(\"=\" * 65)\n    print(\"  TRAINING DIAGNOSTICS\")\n    print(\"=\" * 65)\n\n    # --- Summary table ---\n    print(f\"\\n{'Timesteps':>12}  {'WR':>6}  {'Reward':>10}  {'Length':>8}  {'W/L/D'}\")\n    print(\"-\" * 58)\n    for r in results:\n        ts = r['timesteps']\n        wr = r['win_rate'] * 100\n        rw = r['avg_reward']\n        ln = r['avg_length']\n        wld = f\"{r['wins']}/{r['losses']}/{r['draws']}\"\n        print(f\"{ts:>12,}  {wr:>5.1f}%  {rw:>10.1f}  {ln:>8.1f}  {wld}\")\n\n    # --- Failure mode detection ---\n    print(f\"\\n{'─' * 65}\")\n    print(\"  DIAGNOSIS\")\n    print(f\"{'─' * 65}\")\n\n    if no_wins and all_draws and all_max_len and rewards_positive:\n        # Flat-discrete stalemate pattern\n        print(\"\"\"\nFAILURE MODE: Shaping-reward stalemate (flat_discrete)\n\nThe agent takes only valid actions (exact masking is working), earns\npositive shaping rewards by creating units and controlling structures,\nbut never finishes a game within the step limit.\n\nRoot cause: The agent has NEVER experienced a +1000 win or -1000 loss\nreward. With no terminal signal, it optimizes shaping rewards instead\nof pursuing victory. Both sides create units and trade blows without\neither achieving total elimination within 500 steps.\n\"\"\")\n        if rewards_declining:\n            print(f\"⚠  Reward dropped from {avg_rewards[0]:.0f} → {avg_rewards[-1]:.0f}\")\n            print(\"   at 1M steps, suggesting policy degradation over time.\\n\")\n\n    elif no_wins and negative_rewards and all_max_len:\n        # Multi-discrete invalid action penalty pattern\n        print(\"\"\"\nFAILURE MODE: Invalid-action penalty flood (multi_discrete)\n\n~99% of sampled actions are game-invalid due to per-dimension mask\nover-approximation. The -10 penalty per invalid action dominates the\nreward signal (~4,950 per episode), making it impossible to learn\nfrom actual gameplay.\n\"\"\")\n        if all_losses_late:\n            print(\"At 1M steps the agent collapsed to spamming end_turn to avoid\\n\"\n                  \"penalties, letting SimpleBot win every game.\\n\")\n\n    elif no_wins and late_losses:\n        print(\"\"\"\nFAILURE MODE: Policy collapse\n\nEarly episodes stalemate (draws), then the agent collapses to a\ndegenerate strategy (e.g., always ending turn) and starts losing.\n\"\"\")\n    else:\n        # Partial learning or unknown\n        if max(win_rates) > 0:\n            best_idx = win_rates.index(max(win_rates))\n            print(f\"\\nBest win rate: {max(win_rates)*100:.1f}% at \"\n                  f\"{results[best_idx]['timesteps']:,} timesteps.\")\n            if win_rates[-1] < max(win_rates):\n                print(\"Win rate is declining — possible overfitting or learning rate too high.\")\n        else:\n            print(\"\\n0% win rate across all checkpoints. Review environment configuration.\")\n\n    # --- Should I train longer? ---\n    print(f\"{'─' * 65}\")\n    print(\"  SHOULD YOU TRAIN LONGER?\")\n    print(f\"{'─' * 65}\")\n\n    if no_wins and all_draws:\n        print(\"\"\"\n  NO. Training longer will not help.\n\n  The agent is stuck in a local optimum (maximizing shaping rewards\n  without learning to win). More timesteps will not change this —\n  the agent needs a different reward structure to break out.\n\"\"\")\n    elif no_wins:\n        print(\"\"\"\n  NO. The current configuration has a fundamental issue preventing\n  the agent from learning to win. Fix the issues below first.\n\"\"\")\n    elif win_rates[-1] > win_rates[-2] if len(win_rates) >= 2 else False:\n        print(\"\"\"\n  MAYBE. Win rate is still increasing — more training could help.\n  But check if the rate of improvement is slowing significantly.\n\"\"\")\n    else:\n        print(\"\"\"\n  PROBABLY NOT. Win rate has plateaued or is declining.\n  Consider tuning hyperparameters or reward configuration.\n\"\"\")\n\n    # --- Recommendations ---\n    print(f\"{'─' * 65}\")\n    print(\"  RECOMMENDED FIXES (in priority order)\")\n    print(f\"{'─' * 65}\")\n\n    fixes = []\n    if no_wins and all_draws and all_max_len:\n        fixes = [\n            (\"Reduce max_steps to 200\",\n             \"  500 steps is far too many for a 6×6 map. Shorter episodes force\\n\"\n             \"  earlier confrontation and make terminal rewards reachable.\\n\"\n             \"  → Change: MAX_STEPS = 200\"),\n            (\"Add a truncation penalty\",\n             \"  When episodes are truncated (timeout), the agent sees no terminal\\n\"\n             \"  signal. Penalize truncation so the agent learns that stalling is bad.\\n\"\n             \"  → Add to reward_config: 'draw': -200.0\"),\n            (\"Reduce shaping reward magnitudes\",\n             \"  Shaping rewards (structure_control=5.0, unit_diff=1.0) are too\\n\"\n             \"  generous and create a comfortable local optimum. Scale them down.\\n\"\n             \"  → Set: structure_control=1.0, unit_diff=0.3, income_diff=0.05\"),\n            (\"Increase turn_penalty\",\n             \"  The current -0.1 per end_turn creates no urgency. Make stalling\\n\"\n             \"  costly so the agent pushes to end games decisively.\\n\"\n             \"  → Set: turn_penalty=-1.0\"),\n            (\"Increase ent_coef for exploration\",\n             \"  ent_coef=0.01 allows the policy to narrow too quickly. Higher\\n\"\n             \"  entropy keeps the agent exploring aggressive strategies.\\n\"\n             \"  → Set: ent_coef=0.05 (try range 0.02–0.1)\"),\n            (\"Start with a weaker opponent\",\n             \"  Train against 'random' first so the agent experiences wins early,\\n\"\n             \"  then graduate to 'bot' (SimpleBot).\\n\"\n             \"  → Change: OPPONENT = 'random' for initial training\"),\n        ]\n    elif negative_rewards:\n        fixes = [\n            (\"Switch to flat_discrete action space\",\n             \"  Eliminates 99% invalid actions caused by per-dimension masking.\\n\"\n             \"  → Set: ACTION_SPACE = 'flat_discrete'  (already the notebook default)\"),\n            (\"Reduce invalid_action penalty\",\n             \"  If using multi_discrete, reduce from -10 to -0.1.\\n\"\n             \"  → Add to reward_config: 'invalid_action': -0.1\"),\n        ]\n\n    if not fixes:\n        fixes = [\n            (\"Review environment and reward configuration\",\n             \"  Check that action masking, opponent, and rewards are correctly set.\"),\n        ]\n\n    for i, (title, detail) in enumerate(fixes, 1):\n        print(f\"\\n  {i}. {title}\\n{detail}\")\n\n    print(f\"\\n{'=' * 65}\")\n\n\ndiagnose_training(results)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "z7jaii7t2i",
   "source": "## 9c. Evaluation replay log\n\nRecord every move the agent makes during evaluation and save to JSON.\nThis lets you inspect exactly what the agent is doing each step — is it\nspamming end_turn? Never attacking? Ignoring enemies?\n\nSet `N_REPLAY_EPISODES` to control how many episodes to record\n(default 3 to keep file sizes small).",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cw76eafu8a",
   "source": "N_REPLAY_EPISODES = 3   # episodes to record per checkpoint\n\nACTION_NAMES = [\n    'create_unit', 'move', 'attack', 'seize', 'heal',\n    'end_turn', 'paralyze', 'haste', 'defence_buff', 'attack_buff',\n]\nUNIT_NAMES = ['W', 'M', 'C', 'A', 'K', 'R', 'S', 'B']\n\n\ndef _snapshot_game_state(env):\n    \"\"\"Capture a summary of the current game state.\"\"\"\n    gs = env.unwrapped.game_state\n    ap = env.unwrapped.agent_player\n    opp = 3 - ap\n    return {\n        'agent_gold': gs.player_gold.get(ap, 0),\n        'opponent_gold': gs.player_gold.get(opp, 0),\n        'agent_units': sum(1 for u in gs.units if u.player == ap),\n        'opponent_units': sum(1 for u in gs.units if u.player == opp),\n        'agent_structures': len(gs.grid.get_capturable_tiles(player=ap)),\n        'opponent_structures': len(gs.grid.get_capturable_tiles(player=opp)),\n        'turn': gs.turn_number,\n    }\n\n\ndef evaluate_with_replay(model, env, n_episodes=3):\n    \"\"\"\n    Run evaluation episodes and record every action to a replay log.\n\n    Returns:\n        List of episode dicts, each containing 'steps', 'outcome', etc.\n    \"\"\"\n    episodes = []\n\n    for ep_idx in range(n_episodes):\n        obs, _ = env.reset()\n        done = False\n        ep_reward = 0.0\n        steps = []\n        step_num = 0\n\n        while not done:\n            masks = env.action_masks()\n            action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n\n            # Decode the action BEFORE stepping\n            raw_action = action\n            if ACTION_SPACE == 'flat_discrete':\n                action_idx = int(action)\n                inner = env.unwrapped\n                if 0 <= action_idx < len(inner._current_actions):\n                    action_arr = inner._current_actions[action_idx]\n                else:\n                    action_arr = np.array([5, 0, 0, 0, 0, 0])\n            else:\n                action_arr = np.asarray(action)\n\n            action_type = int(action_arr[0])\n            action_name = ACTION_NAMES[action_type] if action_type < len(ACTION_NAMES) else f'unknown_{action_type}'\n            unit_type = UNIT_NAMES[int(action_arr[1]) % 8]\n            from_pos = [int(action_arr[2]), int(action_arr[3])]\n            to_pos = [int(action_arr[4]), int(action_arr[5])]\n\n            # Step\n            obs, reward, terminated, truncated, info = env.step(raw_action)\n            ep_reward += reward\n            step_num += 1\n            done = terminated or truncated\n\n            step_record = {\n                'step': step_num,\n                'action': action_name,\n                'unit_type': unit_type if action_type == 0 else None,\n                'from': from_pos,\n                'to': to_pos,\n                'reward': round(float(reward), 3),\n                'cumulative_reward': round(float(ep_reward), 3),\n                'valid': info.get('valid_action', True),\n                'game_state': _snapshot_game_state(env),\n            }\n            steps.append(step_record)\n\n        winner = info.get('winner')\n        if winner == 1:\n            outcome = 'win'\n        elif winner is not None:\n            outcome = 'loss'\n        else:\n            outcome = 'draw'\n\n        episodes.append({\n            'episode': ep_idx,\n            'outcome': outcome,\n            'total_reward': round(float(ep_reward), 2),\n            'length': step_num,\n            'steps': steps,\n        })\n\n    return episodes\n\n\nprint('evaluate_with_replay() defined.')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "97p28vin5o7",
   "source": "# Record replays from the final checkpoint\nprint(f'Recording {N_REPLAY_EPISODES} replay episodes from final model...\\n')\nreplay_episodes = evaluate_with_replay(model, eval_env, n_episodes=N_REPLAY_EPISODES)\n\n# Save to JSON\nreplay_path = BENCHMARK_DIR / 'eval_replays.json'\nreplay_data = {\n    'metadata': {\n        'timesteps': CHECKPOINTS[-1],\n        'map': MAP_FILE,\n        'opponent': OPPONENT,\n        'action_space': ACTION_SPACE,\n        'max_steps': MAX_STEPS,\n    },\n    'episodes': replay_episodes,\n}\nwith open(replay_path, 'w') as f:\n    json.dump(replay_data, f, indent=2)\nprint(f'Saved replays: {replay_path}')\n\n# --- Print summary for each episode ---\nfor ep in replay_episodes:\n    print(f'\\n{\"─\" * 55}')\n    print(f'Episode {ep[\"episode\"]}  |  outcome={ep[\"outcome\"]}  |  '\n          f'length={ep[\"length\"]}  |  reward={ep[\"total_reward\"]}')\n    print(f'{\"─\" * 55}')\n\n    # Action distribution\n    from collections import Counter\n    action_counts = Counter(s['action'] for s in ep['steps'])\n    total = len(ep['steps'])\n    print(f'\\n  Action distribution ({total} steps):')\n    for action, count in action_counts.most_common():\n        pct = count / total * 100\n        bar = '#' * int(pct / 2)\n        print(f'    {action:15s}  {count:4d}  ({pct:5.1f}%)  {bar}')\n\n    # Invalid action count\n    invalid = sum(1 for s in ep['steps'] if not s['valid'])\n    if invalid > 0:\n        print(f'\\n  Invalid actions: {invalid}/{total} ({invalid/total*100:.1f}%)')\n\n    # Game state at end\n    final = ep['steps'][-1]['game_state']\n    print(f'\\n  Final state (step {ep[\"length\"]}):')\n    print(f'    Agent:    {final[\"agent_units\"]} units, '\n          f'{final[\"agent_structures\"]} structures, '\n          f'{final[\"agent_gold\"]} gold')\n    print(f'    Opponent: {final[\"opponent_units\"]} units, '\n          f'{final[\"opponent_structures\"]} structures, '\n          f'{final[\"opponent_gold\"]} gold')\n\n    # Show first 10 and last 10 moves\n    steps = ep['steps']\n    print(f'\\n  First 10 moves:')\n    for s in steps[:10]:\n        gs = s['game_state']\n        ut = f' ({s[\"unit_type\"]})' if s['unit_type'] else ''\n        valid_marker = '' if s['valid'] else ' [INVALID]'\n        print(f'    step {s[\"step\"]:3d}  {s[\"action\"]:15s}{ut:5s}  '\n              f'{s[\"from\"]}→{s[\"to\"]}  r={s[\"reward\"]:+.1f}  '\n              f'units={gs[\"agent_units\"]}v{gs[\"opponent_units\"]}  '\n              f'turn={gs[\"turn\"]}{valid_marker}')\n\n    if len(steps) > 20:\n        print(f'    ... ({len(steps) - 20} steps omitted) ...')\n\n    if len(steps) > 10:\n        print(f'  Last 10 moves:')\n        for s in steps[-10:]:\n            gs = s['game_state']\n            ut = f' ({s[\"unit_type\"]})' if s['unit_type'] else ''\n            valid_marker = '' if s['valid'] else ' [INVALID]'\n            print(f'    step {s[\"step\"]:3d}  {s[\"action\"]:15s}{ut:5s}  '\n                  f'{s[\"from\"]}→{s[\"to\"]}  r={s[\"reward\"]:+.1f}  '\n                  f'units={gs[\"agent_units\"]}v{gs[\"opponent_units\"]}  '\n                  f'turn={gs[\"turn\"]}{valid_marker}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 10. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results as JSON\n",
    "benchmark_data = {\n",
    "    'metadata': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'map': MAP_FILE,\n",
    "        'opponent': OPPONENT,\n",
    "        'max_steps': MAX_STEPS,\n",
    "        'n_envs': N_ENVS,\n",
    "        'eval_episodes': EVAL_EPISODES,\n",
    "        'seed': SEED,\n",
    "        'device': DEVICE,\n",
    "        'ppo_config': PPO_CONFIG,\n",
    "    },\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "results_path = BENCHMARK_DIR / 'benchmark_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(benchmark_data, f, indent=2)\n",
    "\n",
    "print(f'Saved results:  {results_path}')\n",
    "\n",
    "# Also save as CSV for easy viewing\n",
    "csv_path = BENCHMARK_DIR / 'benchmark_results.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f'Saved CSV:      {csv_path}')\n",
    "\n",
    "# List all saved files\n",
    "print(f'\\nAll benchmark files:')\n",
    "for p in sorted(BENCHMARK_DIR.iterdir()):\n",
    "    size = p.stat().st_size\n",
    "    if size > 1024 * 1024:\n",
    "        size_str = f'{size / 1024 / 1024:.1f} MB'\n",
    "    elif size > 1024:\n",
    "        size_str = f'{size / 1024:.1f} KB'\n",
    "    else:\n",
    "        size_str = f'{size} B'\n",
    "    print(f'  {p.name:40s}  {size_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensorboard-header",
   "metadata": {},
   "source": [
    "## 11. TensorBoard (optional)\n",
    "\n",
    "Launch TensorBoard to inspect detailed training metrics (loss, entropy, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to launch TensorBoard inline:\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir benchmarks/ppo_vs_simplebot/tensorboard\n",
    "\n",
    "print('To view TensorBoard locally, run:')\n",
    "print(f'  tensorboard --logdir {BENCHMARK_DIR / \"tensorboard\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpret-header",
   "metadata": {},
   "source": [
    "## 12. Interpreting the results\n",
    "\n",
    "### What to expect\n",
    "\n",
    "| Timesteps | Expected Win Rate | Notes |\n",
    "|-----------|-------------------|-------|\n",
    "| 10K | 0–15% | Agent is mostly random, learning basic actions |\n",
    "| 50K | 15–40% | Agent starts making meaningful moves |\n",
    "| 200K | 40–70% | Competent play, learns unit creation and combat |\n",
    "| 1M | 60–90%+ | Strong play against SimpleBot |\n",
    "\n",
    "**Note:** Exact numbers depend on hardware and random seed. The important\n",
    "thing is that your curve has a similar *shape* — monotonically increasing\n",
    "win rate with diminishing returns after ~200K steps.\n",
    "\n",
    "### If your results differ significantly\n",
    "\n",
    "- **Much worse:** Check that action masking is working (the agent should\n",
    "  rarely attempt invalid actions). Verify the map file path is correct.\n",
    "- **Much better:** You may have found better hyperparameters! Consider\n",
    "  contributing them back.\n",
    "- **Unstable (oscillating win rate):** Try reducing the learning rate\n",
    "  or increasing the batch size.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "1. **Try different maps:** Larger maps (10×10, 14×14) are harder\n",
    "2. **Tune hyperparameters:** Adjust `ent_coef`, `learning_rate`, etc.\n",
    "3. **Self-play training:** See `train/train_self_play.py`\n",
    "4. **AlphaZero:** See `train/train_alphazero.py` for MCTS-based training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up environments\n",
    "vec_env.close()\n",
    "eval_env.close()\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}