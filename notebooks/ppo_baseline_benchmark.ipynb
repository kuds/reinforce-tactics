{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/ppo_baseline_benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Reinforce Tactics — PPO Baseline Training Benchmarks\n",
    "\n",
    "This notebook trains a **MaskablePPO** agent against `SimpleBot` on the 6×6 beginner map\n",
    "and records reference metrics at four training checkpoints:\n",
    "\n",
    "| Checkpoint | Timesteps |\n",
    "|------------|-----------|\n",
    "| 1 | 10,000 |\n",
    "| 2 | 50,000 |\n",
    "| 3 | 200,000 |\n",
    "| 4 | 1,000,000 |\n",
    "\n",
    "At each checkpoint the agent is evaluated over **50 episodes** and we record:\n",
    "- **Win rate** (% of games won against SimpleBot)\n",
    "- **Average episode reward**\n",
    "- **Average episode length** (steps)\n",
    "\n",
    "The goal is to provide a **reference curve** so that users can run the same\n",
    "notebook and compare their results to known-good training runs.\n",
    "\n",
    "**Runtime:** CPU is fine (~20–40 min total). GPU will be faster.\n",
    "\n",
    "---\n",
    "\n",
    "### Why MaskablePPO?\n",
    "\n",
    "The game has a `MultiDiscrete` action space where many action combinations\n",
    "are invalid at any given time (e.g. you can’t attack a tile with no enemy).\n",
    "**Action masking** prevents the agent from sampling these invalid actions,\n",
    "which typically yields 2–3× faster convergence compared to plain PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q gymnasium stable-baselines3 sb3-contrib tensorboard pandas numpy torch matplotlib\n\nimport torch\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Device: {DEVICE}\")\nif DEVICE == 'cuda':\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": "# Clone repo and install as a package\nimport os, sys\nfrom pathlib import Path\n\nREPO_DIR = Path('reinforce-tactics')\nif REPO_DIR.exists():\n    os.chdir(REPO_DIR)\nelif Path('notebooks').exists():\n    # Already inside the repo\n    os.chdir('..')\nelse:\n    print('Cloning repository...')\n    !git clone https://github.com/kuds/reinforce-tactics.git\n    os.chdir(REPO_DIR)\n\n# Install the package so all imports resolve\n!pip install -q -e .\n\nif os.getcwd() not in sys.path:\n    sys.path.insert(0, os.getcwd())\n\nprint(f\"Working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sb3_contrib import MaskablePPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "\n",
    "from reinforcetactics.rl.masking import make_maskable_env, make_maskable_vec_env\n",
    "\n",
    "print('All imports successful.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Benchmark settings ---\n",
    "MAP_FILE        = 'maps/1v1/beginner.csv'   # 6x6 beginner map\n",
    "OPPONENT        = 'bot'                      # SimpleBot\n",
    "MAX_STEPS       = 500                        # max steps per episode\n",
    "N_ENVS          = 4                          # parallel training envs\n",
    "SEED            = 42\n",
    "\n",
    "# Checkpoints to evaluate\n",
    "CHECKPOINTS     = [10_000, 50_000, 200_000, 1_000_000]\n",
    "EVAL_EPISODES   = 50                         # episodes per evaluation\n",
    "\n",
    "# PPO hyperparameters\n",
    "PPO_CONFIG = dict(\n",
    "    learning_rate = 3e-4,\n",
    "    n_steps       = 2048,\n",
    "    batch_size    = 64,\n",
    "    n_epochs      = 10,\n",
    "    gamma         = 0.99,\n",
    "    gae_lambda    = 0.95,\n",
    "    clip_range    = 0.2,\n",
    "    ent_coef      = 0.01,\n",
    "    vf_coef       = 0.5,\n",
    "    max_grad_norm = 0.5,\n",
    ")\n",
    "\n",
    "# Output paths\n",
    "BENCHMARK_DIR = Path('benchmarks/ppo_vs_simplebot')\n",
    "BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Map:          {MAP_FILE}')\n",
    "print(f'Opponent:     {OPPONENT}')\n",
    "print(f'Checkpoints:  {CHECKPOINTS}')\n",
    "print(f'Eval eps:     {EVAL_EPISODES}')\n",
    "print(f'Output dir:   {BENCHMARK_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-header",
   "metadata": {},
   "source": [
    "## 4. Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-envs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training envs (vectorized, headless)\n",
    "vec_env = make_maskable_vec_env(\n",
    "    n_envs=N_ENVS,\n",
    "    map_file=MAP_FILE,\n",
    "    opponent=OPPONENT,\n",
    "    max_steps=MAX_STEPS,\n",
    "    seed=SEED,\n",
    "    use_subprocess=False,   # DummyVecEnv (safer in notebooks)\n",
    ")\n",
    "\n",
    "# Separate eval env (single, deterministic)\n",
    "eval_env = make_maskable_env(\n",
    "    map_file=MAP_FILE,\n",
    "    opponent=OPPONENT,\n",
    "    max_steps=MAX_STEPS,\n",
    ")\n",
    "\n",
    "print(f'Observation space: {vec_env.observation_space}')\n",
    "print(f'Action space:      {vec_env.action_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## 5. Create MaskablePPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskablePPO(\n",
    "    'MultiInputPolicy',\n",
    "    vec_env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=str(BENCHMARK_DIR / 'tensorboard'),\n",
    "    device=DEVICE,\n",
    "    seed=SEED,\n",
    "    **PPO_CONFIG,\n",
    ")\n",
    "\n",
    "print('MaskablePPO model created.')\n",
    "print(f'Policy:  {model.policy.__class__.__name__}')\n",
    "print(f'Device:  {model.device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-fn-header",
   "metadata": {},
   "source": [
    "## 6. Evaluation helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-fn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, n_episodes=50):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model and return summary statistics.\n",
    "\n",
    "    Returns dict with: win_rate, avg_reward, std_reward,\n",
    "    avg_length, std_length, wins, losses, draws\n",
    "    \"\"\"\n",
    "    wins, losses, draws = 0, 0, 0\n",
    "    rewards, lengths = [], []\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        ep_reward = 0.0\n",
    "        ep_len = 0\n",
    "\n",
    "        while not done:\n",
    "            masks = env.action_masks()\n",
    "            action, _ = model.predict(obs, deterministic=True, action_masks=masks)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            ep_reward += reward\n",
    "            ep_len += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        rewards.append(ep_reward)\n",
    "        lengths.append(ep_len)\n",
    "\n",
    "        winner = info.get('winner')\n",
    "        if winner == 1:\n",
    "            wins += 1\n",
    "        elif winner is not None:\n",
    "            losses += 1\n",
    "        else:\n",
    "            draws += 1\n",
    "\n",
    "    return {\n",
    "        'win_rate':    wins / n_episodes,\n",
    "        'avg_reward':  float(np.mean(rewards)),\n",
    "        'std_reward':  float(np.std(rewards)),\n",
    "        'avg_length':  float(np.mean(lengths)),\n",
    "        'std_length':  float(np.std(lengths)),\n",
    "        'wins':        wins,\n",
    "        'losses':      losses,\n",
    "        'draws':       draws,\n",
    "    }\n",
    "\n",
    "print('evaluate_model() defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train-header",
   "metadata": {},
   "source": [
    "## 7. Train and evaluate at each checkpoint\n",
    "\n",
    "We train incrementally: 0 → 10K → 50K → 200K → 1M timesteps,\n",
    "evaluating at each checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "trained_so_far = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for checkpoint_ts in CHECKPOINTS:\n",
    "    steps_to_train = checkpoint_ts - trained_so_far\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Training {trained_so_far:,} -> {checkpoint_ts:,} '\n",
    "          f'({steps_to_train:,} steps)...')\n",
    "    print(f'{\"=\"*60}')\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.learn(\n",
    "        total_timesteps=steps_to_train,\n",
    "        reset_num_timesteps=False,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "    train_time = time.time() - t0\n",
    "    trained_so_far = checkpoint_ts\n",
    "\n",
    "    # Save checkpoint\n",
    "    ckpt_path = BENCHMARK_DIR / f'model_{checkpoint_ts}.zip'\n",
    "    model.save(str(ckpt_path))\n",
    "    print(f'Saved checkpoint: {ckpt_path}')\n",
    "\n",
    "    # Evaluate\n",
    "    print(f'Evaluating over {EVAL_EPISODES} episodes...')\n",
    "    metrics = evaluate_model(model, eval_env, n_episodes=EVAL_EPISODES)\n",
    "    metrics['timesteps'] = checkpoint_ts\n",
    "    metrics['train_time_s'] = round(train_time, 1)\n",
    "    results.append(metrics)\n",
    "\n",
    "    print(f'  Win rate:       {metrics[\"win_rate\"]*100:.1f}%')\n",
    "    print(f'  Avg reward:     {metrics[\"avg_reward\"]:.1f} '\n",
    "          f'(+/- {metrics[\"std_reward\"]:.1f})')\n",
    "    print(f'  Avg length:     {metrics[\"avg_length\"]:.1f} '\n",
    "          f'(+/- {metrics[\"std_length\"]:.1f})')\n",
    "    print(f'  W/L/D:          {metrics[\"wins\"]}/{metrics[\"losses\"]}/{metrics[\"draws\"]}')\n",
    "    print(f'  Training time:  {train_time:.1f}s')\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f'\\nTotal wall time: {total_time/60:.1f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 8. Results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df['win_rate_pct'] = (df['win_rate'] * 100).round(1)\n",
    "df['avg_reward'] = df['avg_reward'].round(1)\n",
    "df['avg_length'] = df['avg_length'].round(1)\n",
    "\n",
    "display_df = df[['timesteps', 'win_rate_pct', 'avg_reward', 'avg_length',\n",
    "                  'wins', 'losses', 'draws', 'train_time_s']].copy()\n",
    "display_df.columns = ['Timesteps', 'Win Rate (%)', 'Avg Reward',\n",
    "                       'Avg Length', 'Wins', 'Losses', 'Draws',\n",
    "                       'Train Time (s)']\n",
    "display_df = display_df.set_index('Timesteps')\n",
    "display_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plots-header",
   "metadata": {},
   "source": [
    "## 9. Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "ts = [r['timesteps'] for r in results]\n",
    "\n",
    "# Win rate\n",
    "ax = axes[0]\n",
    "wr = [r['win_rate'] * 100 for r in results]\n",
    "ax.plot(ts, wr, 'o-', color='#2196F3', linewidth=2, markersize=8)\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Win Rate (%)')\n",
    "ax.set_title('Win Rate vs SimpleBot')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim(-5, 105)\n",
    "ax.axhline(y=70, color='green', linestyle='--', alpha=0.5, label='70% target')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Average reward\n",
    "ax = axes[1]\n",
    "avg_r = [r['avg_reward'] for r in results]\n",
    "std_r = [r['std_reward'] for r in results]\n",
    "ax.plot(ts, avg_r, 'o-', color='#FF9800', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_r, std_r)],\n",
    "                [a + s for a, s in zip(avg_r, std_r)],\n",
    "                alpha=0.2, color='#FF9800')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Reward')\n",
    "ax.set_title('Average Episode Reward')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode length\n",
    "ax = axes[2]\n",
    "avg_l = [r['avg_length'] for r in results]\n",
    "std_l = [r['std_length'] for r in results]\n",
    "ax.plot(ts, avg_l, 'o-', color='#4CAF50', linewidth=2, markersize=8)\n",
    "ax.fill_between(ts,\n",
    "                [a - s for a, s in zip(avg_l, std_l)],\n",
    "                [a + s for a, s in zip(avg_l, std_l)],\n",
    "                alpha=0.2, color='#4CAF50')\n",
    "ax.set_xlabel('Timesteps')\n",
    "ax.set_ylabel('Average Length (steps)')\n",
    "ax.set_title('Average Episode Length')\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('PPO Baseline Benchmarks  |  6x6 beginner map  |  vs SimpleBot',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(str(BENCHMARK_DIR / 'training_curves.png'),\n",
    "            dpi=150, bbox_inches='tight')\n",
    "print(f'Saved plot: {BENCHMARK_DIR / \"training_curves.png\"}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save-header",
   "metadata": {},
   "source": [
    "## 10. Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save benchmark results as JSON\n",
    "benchmark_data = {\n",
    "    'metadata': {\n",
    "        'date': datetime.now().isoformat(),\n",
    "        'map': MAP_FILE,\n",
    "        'opponent': OPPONENT,\n",
    "        'max_steps': MAX_STEPS,\n",
    "        'n_envs': N_ENVS,\n",
    "        'eval_episodes': EVAL_EPISODES,\n",
    "        'seed': SEED,\n",
    "        'device': DEVICE,\n",
    "        'ppo_config': PPO_CONFIG,\n",
    "    },\n",
    "    'results': results,\n",
    "}\n",
    "\n",
    "results_path = BENCHMARK_DIR / 'benchmark_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(benchmark_data, f, indent=2)\n",
    "\n",
    "print(f'Saved results:  {results_path}')\n",
    "\n",
    "# Also save as CSV for easy viewing\n",
    "csv_path = BENCHMARK_DIR / 'benchmark_results.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f'Saved CSV:      {csv_path}')\n",
    "\n",
    "# List all saved files\n",
    "print(f'\\nAll benchmark files:')\n",
    "for p in sorted(BENCHMARK_DIR.iterdir()):\n",
    "    size = p.stat().st_size\n",
    "    if size > 1024 * 1024:\n",
    "        size_str = f'{size / 1024 / 1024:.1f} MB'\n",
    "    elif size > 1024:\n",
    "        size_str = f'{size / 1024:.1f} KB'\n",
    "    else:\n",
    "        size_str = f'{size} B'\n",
    "    print(f'  {p.name:40s}  {size_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tensorboard-header",
   "metadata": {},
   "source": [
    "## 11. TensorBoard (optional)\n",
    "\n",
    "Launch TensorBoard to inspect detailed training metrics (loss, entropy, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tensorboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to launch TensorBoard inline:\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir benchmarks/ppo_vs_simplebot/tensorboard\n",
    "\n",
    "print('To view TensorBoard locally, run:')\n",
    "print(f'  tensorboard --logdir {BENCHMARK_DIR / \"tensorboard\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpret-header",
   "metadata": {},
   "source": [
    "## 12. Interpreting the results\n",
    "\n",
    "### What to expect\n",
    "\n",
    "| Timesteps | Expected Win Rate | Notes |\n",
    "|-----------|-------------------|-------|\n",
    "| 10K | 0–15% | Agent is mostly random, learning basic actions |\n",
    "| 50K | 15–40% | Agent starts making meaningful moves |\n",
    "| 200K | 40–70% | Competent play, learns unit creation and combat |\n",
    "| 1M | 60–90%+ | Strong play against SimpleBot |\n",
    "\n",
    "**Note:** Exact numbers depend on hardware and random seed. The important\n",
    "thing is that your curve has a similar *shape* — monotonically increasing\n",
    "win rate with diminishing returns after ~200K steps.\n",
    "\n",
    "### If your results differ significantly\n",
    "\n",
    "- **Much worse:** Check that action masking is working (the agent should\n",
    "  rarely attempt invalid actions). Verify the map file path is correct.\n",
    "- **Much better:** You may have found better hyperparameters! Consider\n",
    "  contributing them back.\n",
    "- **Unstable (oscillating win rate):** Try reducing the learning rate\n",
    "  or increasing the batch size.\n",
    "\n",
    "### Next steps\n",
    "\n",
    "1. **Try different maps:** Larger maps (10×10, 14×14) are harder\n",
    "2. **Tune hyperparameters:** Adjust `ent_coef`, `learning_rate`, etc.\n",
    "3. **Self-play training:** See `train/train_self_play.py`\n",
    "4. **AlphaZero:** See `train/train_alphazero.py` for MCTS-based training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up environments\n",
    "vec_env.close()\n",
    "eval_env.close()\n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}