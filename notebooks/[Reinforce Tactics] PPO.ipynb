{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuds/reinforce-tactics/blob/main/notebooks/%5BReinforce%20Tactics%5D%20PPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üéÆ Reinforce Tactics - RL Training in Google Colab\n",
        "\n",
        "This notebook trains an AI agent to play Reinforce Tactics using Reinforcement Learning.\n",
        "\n",
        "**Features:**\n",
        "- Headless training (no GUI needed)\n",
        "- Stable-Baselines3 PPO algorithm\n",
        "- TensorBoard monitoring\n",
        "- Model saving and evaluation\n",
        "- GPU acceleration support\n",
        "\n",
        "**Runtime:** Use GPU runtime for faster training (Runtime ‚Üí Change runtime type ‚Üí GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üì¶ Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "install_deps"
      },
      "source": [
        "# Install dependencies\n",
        "!pip install -q gymnasium stable-baselines3[extra] tensorboard pandas numpy\n",
        "\n",
        "# Check if GPU is available\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"\\n‚úÖ Using device: {device}\")\n",
        "if device == 'cuda':\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone_repo"
      },
      "source": [
        "# Clone repository (replace with your repo URL)\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "if not Path('reinforcetactics').exists():\n",
        "    print(\"üì• Setting up Reinforce Tactics...\")\n",
        "    # For this demo, we'll create the necessary files\n",
        "    # In practice, you would: !git clone https://github.com/your-repo/reinforcetactics.git\n",
        "    !mkdir -p reinforcetactics/{core,game,rl,ui,utils}\n",
        "    !touch reinforcetactics/__init__.py\n",
        "    print(\"‚úÖ Setup complete!\")\n",
        "else:\n",
        "    print(\"‚úÖ Reinforce Tactics already set up\")\n",
        "\n",
        "# Add to Python path\n",
        "import sys\n",
        "if '/content' not in sys.path:\n",
        "    sys.path.insert(0, '/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_files"
      },
      "source": [
        "## üìù Create Minimal Game Files\n",
        "\n",
        "Since we can't upload the full codebase, we'll create minimal versions of the key files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_constants"
      },
      "source": [
        "%%writefile reinforcetactics/constants.py\n",
        "\"\"\"Game constants\"\"\"\n",
        "\n",
        "TILE_SIZE = 32\n",
        "MIN_MAP_SIZE = 20\n",
        "STARTING_GOLD = 250\n",
        "\n",
        "UNIT_DATA = {\n",
        "    'W': {'name': 'Warrior', 'cost': 200, 'movement': 3, 'health': 15, 'attack': 10},\n",
        "    'M': {'name': 'Mage', 'cost': 250, 'movement': 2, 'health': 10, 'attack': {'adjacent': 8, 'range': 12}},\n",
        "    'C': {'name': 'Cleric', 'cost': 200, 'movement': 2, 'health': 8, 'attack': 2}\n",
        "}\n",
        "\n",
        "HEADQUARTERS_INCOME = 150\n",
        "BUILDING_INCOME = 100\n",
        "TOWER_INCOME = 50\n",
        "\n",
        "TOWER_MAX_HEALTH = 30\n",
        "BUILDING_MAX_HEALTH = 40\n",
        "HEADQUARTERS_MAX_HEALTH = 50\n",
        "\n",
        "COUNTER_ATTACK_MULTIPLIER = 0.9\n",
        "PARALYZE_DURATION = 3\n",
        "HEAL_AMOUNT = 5\n",
        "STRUCTURE_REGEN_RATE = 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "create_simple_env"
      },
      "source": [
        "%%writefile reinforcetactics/rl/simple_env.py\n",
        "\"\"\"Simplified Gymnasium environment for Colab training\"\"\"\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "\n",
        "class SimpleStrategyEnv(gym.Env):\n",
        "    \"\"\"Simplified strategy game environment for demonstration.\"\"\"\n",
        "\n",
        "    def __init__(self, grid_size=10, max_steps=200):\n",
        "        super().__init__()\n",
        "\n",
        "        self.grid_size = grid_size\n",
        "        self.max_steps = max_steps\n",
        "        self.current_step = 0\n",
        "\n",
        "        # Simplified observation: grid + global features\n",
        "        self.observation_space = spaces.Dict({\n",
        "            'grid': spaces.Box(low=0, high=10, shape=(grid_size, grid_size, 4), dtype=np.float32),\n",
        "            'global_features': spaces.Box(low=0, high=1000, shape=(6,), dtype=np.float32)\n",
        "        })\n",
        "\n",
        "        # Simplified action space: [action_type, x, y]\n",
        "        self.action_space = spaces.MultiDiscrete([4, grid_size, grid_size])\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.current_step = 0\n",
        "        self.player_hp = 100\n",
        "        self.enemy_hp = 100\n",
        "        self.player_gold = 500\n",
        "        self.enemy_gold = 500\n",
        "        self.player_units = 3\n",
        "        self.enemy_units = 3\n",
        "\n",
        "        # Simple grid representation\n",
        "        self.grid = np.zeros((self.grid_size, self.grid_size, 4), dtype=np.float32)\n",
        "\n",
        "        # Place player units\n",
        "        self.grid[1, 1, 0] = 1  # Player marker\n",
        "        self.grid[1, 1, 1] = 100  # HP\n",
        "\n",
        "        # Place enemy units\n",
        "        self.grid[8, 8, 0] = 2  # Enemy marker\n",
        "        self.grid[8, 8, 1] = 100  # HP\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return {\n",
        "            'grid': self.grid.copy(),\n",
        "            'global_features': np.array([\n",
        "                self.player_gold,\n",
        "                self.enemy_gold,\n",
        "                self.current_step / self.max_steps,\n",
        "                self.player_units,\n",
        "                self.enemy_units,\n",
        "                self.player_hp\n",
        "            ], dtype=np.float32)\n",
        "        }\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "\n",
        "        action_type, x, y = action\n",
        "        x = min(x, self.grid_size - 1)\n",
        "        y = min(y, self.grid_size - 1)\n",
        "\n",
        "        reward = 0.0\n",
        "\n",
        "        # Simple action logic\n",
        "        if action_type == 0:  # Attack\n",
        "            if self.grid[y, x, 0] == 2:  # Enemy present\n",
        "                damage = np.random.randint(5, 15)\n",
        "                self.enemy_hp -= damage\n",
        "                reward += damage * 0.5\n",
        "\n",
        "        elif action_type == 1:  # Create unit\n",
        "            if self.player_gold >= 100:\n",
        "                self.player_gold -= 100\n",
        "                self.player_units += 1\n",
        "                reward += 5\n",
        "\n",
        "        elif action_type == 2:  # Capture\n",
        "            reward += 0.1\n",
        "\n",
        "        elif action_type == 3:  # End turn\n",
        "            self.player_gold += 50\n",
        "            # Enemy turn (simple AI)\n",
        "            if np.random.random() > 0.5 and self.enemy_gold >= 100:\n",
        "                self.enemy_gold -= 100\n",
        "                self.enemy_units += 1\n",
        "            self.enemy_gold += 50\n",
        "\n",
        "        # Enemy attacks (simplified)\n",
        "        if np.random.random() > 0.7:\n",
        "            damage = np.random.randint(3, 10)\n",
        "            self.player_hp -= damage\n",
        "            reward -= damage * 0.3\n",
        "\n",
        "        # Check termination\n",
        "        terminated = False\n",
        "        if self.enemy_hp <= 0:\n",
        "            reward += 1000\n",
        "            terminated = True\n",
        "        elif self.player_hp <= 0:\n",
        "            reward -= 1000\n",
        "            terminated = True\n",
        "\n",
        "        truncated = self.current_step >= self.max_steps\n",
        "\n",
        "        return self._get_obs(), reward, terminated, truncated, {\n",
        "            'player_hp': self.player_hp,\n",
        "            'enemy_hp': self.enemy_hp,\n",
        "            'player_units': self.player_units,\n",
        "            'enemy_units': self.enemy_units\n",
        "        }\n",
        "\n",
        "    def render(self):\n",
        "        pass  # Headless mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training"
      },
      "source": [
        "## üöÄ Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config"
      },
      "source": [
        "# Training configuration\n",
        "config = {\n",
        "    'total_timesteps': 100_000,      # Increase for better results (500k - 1M recommended)\n",
        "    'learning_rate': 3e-4,\n",
        "    'n_steps': 2048,\n",
        "    'batch_size': 64,\n",
        "    'n_epochs': 10,\n",
        "    'gamma': 0.99,\n",
        "    'gae_lambda': 0.95,\n",
        "    'clip_range': 0.2,\n",
        "    'ent_coef': 0.01,\n",
        "    'vf_coef': 0.5,\n",
        "    'max_grad_norm': 0.5,\n",
        "    'n_envs': 4,                     # Parallel environments\n",
        "    'device': device,\n",
        "}\n",
        "\n",
        "print(\"üìã Training Configuration:\")\n",
        "for key, value in config.items():\n",
        "    print(f\"   {key}: {value}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_env"
      },
      "source": [
        "## üéÆ Create Training Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "make_env"
      },
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "from reinforcetactics.rl.simple_env import SimpleStrategyEnv\n",
        "\n",
        "def make_env(rank, seed=0):\n",
        "    \"\"\"Create a single environment.\"\"\"\n",
        "    def _init():\n",
        "        env = SimpleStrategyEnv(grid_size=10, max_steps=200)\n",
        "        env.reset(seed=seed + rank)\n",
        "        return env\n",
        "    set_random_seed(seed)\n",
        "    return _init\n",
        "\n",
        "# Create vectorized environments\n",
        "print(f\"üéÆ Creating {config['n_envs']} parallel environments...\")\n",
        "\n",
        "if config['n_envs'] == 1:\n",
        "    env = DummyVecEnv([make_env(0)])\n",
        "else:\n",
        "    env = SubprocVecEnv([make_env(i) for i in range(config['n_envs'])])\n",
        "\n",
        "# Create evaluation environment\n",
        "eval_env = DummyVecEnv([make_env(999)])\n",
        "\n",
        "print(\"‚úÖ Environments created!\")\n",
        "print(f\"   Observation space: {env.observation_space}\")\n",
        "print(f\"   Action space: {env.action_space}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tensorboard"
      },
      "source": [
        "## üìä Setup TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "setup_tensorboard"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "# Create log directory\n",
        "from datetime import datetime\n",
        "log_dir = f\"./logs/ppo_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Log directory: {log_dir}\")\n",
        "print(\"\\nüéØ TensorBoard will start after training begins...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create_model"
      },
      "source": [
        "## ü§ñ Create and Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train_model"
      },
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
        "\n",
        "print(\"ü§ñ Creating PPO model...\")\n",
        "\n",
        "# Create model\n",
        "model = PPO(\n",
        "    \"MultiInputPolicy\",\n",
        "    env,\n",
        "    learning_rate=config['learning_rate'],\n",
        "    n_steps=config['n_steps'],\n",
        "    batch_size=config['batch_size'],\n",
        "    n_epochs=config['n_epochs'],\n",
        "    gamma=config['gamma'],\n",
        "    gae_lambda=config['gae_lambda'],\n",
        "    clip_range=config['clip_range'],\n",
        "    ent_coef=config['ent_coef'],\n",
        "    vf_coef=config['vf_coef'],\n",
        "    max_grad_norm=config['max_grad_norm'],\n",
        "    verbose=1,\n",
        "    tensorboard_log=log_dir,\n",
        "    device=config['device']\n",
        ")\n",
        "\n",
        "# Setup callbacks\n",
        "eval_callback = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=f\"{log_dir}/best_model\",\n",
        "    log_path=f\"{log_dir}/eval\",\n",
        "    eval_freq=10000,\n",
        "    n_eval_episodes=5,\n",
        "    deterministic=True\n",
        ")\n",
        "\n",
        "checkpoint_callback = CheckpointCallback(\n",
        "    save_freq=20000,\n",
        "    save_path=f\"{log_dir}/checkpoints\",\n",
        "    name_prefix=\"ppo_model\"\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model created!\")\n",
        "print(f\"\\nüéì Training for {config['total_timesteps']:,} timesteps...\\n\")\n",
        "\n",
        "# Train the model\n",
        "try:\n",
        "    model.learn(\n",
        "        total_timesteps=config['total_timesteps'],\n",
        "        callback=[eval_callback, checkpoint_callback],\n",
        "        progress_bar=True\n",
        "    )\n",
        "    print(\"\\n‚úÖ Training completed successfully!\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n‚ö†Ô∏è  Training interrupted by user\")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = f\"{log_dir}/final_model.zip\"\n",
        "model.save(final_model_path)\n",
        "print(f\"üíæ Final model saved to: {final_model_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view_tensorboard"
      },
      "source": [
        "## üìà View Training Progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "launch_tensorboard"
      },
      "source": [
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluate"
      },
      "source": [
        "## üß™ Evaluate Trained Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evaluation"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_agent(model, env, n_episodes=20):\n",
        "    \"\"\"Evaluate the trained agent.\"\"\"\n",
        "    print(f\"\\nüß™ Evaluating agent over {n_episodes} episodes...\\n\")\n",
        "\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    wins = 0\n",
        "\n",
        "    for episode in tqdm(range(n_episodes)):\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic=True)\n",
        "            obs, reward, done, info = env.step(action)\n",
        "            episode_reward += reward[0]\n",
        "            episode_length += 1\n",
        "\n",
        "            if done[0]:\n",
        "                # Check if won (positive large reward)\n",
        "                if reward[0] > 500:\n",
        "                    wins += 1\n",
        "                break\n",
        "\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä Evaluation Results\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Episodes:           {n_episodes}\")\n",
        "    print(f\"Win Rate:           {wins/n_episodes*100:.1f}% ({wins}/{n_episodes})\")\n",
        "    print(f\"Mean Reward:        {np.mean(episode_rewards):.2f} ¬± {np.std(episode_rewards):.2f}\")\n",
        "    print(f\"Mean Length:        {np.mean(episode_lengths):.1f} steps\")\n",
        "    print(f\"Best Reward:        {np.max(episode_rewards):.2f}\")\n",
        "    print(f\"Worst Reward:       {np.min(episode_rewards):.2f}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return {\n",
        "        'win_rate': wins/n_episodes,\n",
        "        'mean_reward': np.mean(episode_rewards),\n",
        "        'std_reward': np.std(episode_rewards),\n",
        "        'mean_length': np.mean(episode_lengths)\n",
        "    }\n",
        "\n",
        "# Evaluate the trained model\n",
        "eval_results = evaluate_agent(model, eval_env, n_episodes=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize"
      },
      "source": [
        "## üìä Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plot_results"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load training data from TensorBoard logs\n",
        "from tensorboard.backend.event_processing import event_accumulator\n",
        "\n",
        "def load_tensorboard_data(log_dir):\n",
        "    \"\"\"Load data from TensorBoard logs.\"\"\"\n",
        "    ea = event_accumulator.EventAccumulator(log_dir)\n",
        "    ea.Reload()\n",
        "\n",
        "    # Get available tags\n",
        "    tags = ea.Tags()['scalars']\n",
        "\n",
        "    data = {}\n",
        "    for tag in tags:\n",
        "        events = ea.Scalars(tag)\n",
        "        data[tag] = pd.DataFrame([\n",
        "            {'step': e.step, 'value': e.value} for e in events\n",
        "        ])\n",
        "\n",
        "    return data\n",
        "\n",
        "# Try to load and plot training data\n",
        "try:\n",
        "    # Find the PPO_1 subdirectory\n",
        "    ppo_dirs = [d for d in os.listdir(log_dir) if d.startswith('PPO_')]\n",
        "    if ppo_dirs:\n",
        "        tb_log_dir = os.path.join(log_dir, ppo_dirs[0])\n",
        "        data = load_tensorboard_data(tb_log_dir)\n",
        "\n",
        "        # Create plots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Training Progress', fontsize=16, fontweight='bold')\n",
        "\n",
        "        # Plot episode reward\n",
        "        if 'rollout/ep_rew_mean' in data:\n",
        "            df = data['rollout/ep_rew_mean']\n",
        "            axes[0, 0].plot(df['step'], df['value'])\n",
        "            axes[0, 0].set_title('Episode Reward Mean')\n",
        "            axes[0, 0].set_xlabel('Steps')\n",
        "            axes[0, 0].set_ylabel('Reward')\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot episode length\n",
        "        if 'rollout/ep_len_mean' in data:\n",
        "            df = data['rollout/ep_len_mean']\n",
        "            axes[0, 1].plot(df['step'], df['value'], color='orange')\n",
        "            axes[0, 1].set_title('Episode Length Mean')\n",
        "            axes[0, 1].set_xlabel('Steps')\n",
        "            axes[0, 1].set_ylabel('Length')\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot loss\n",
        "        if 'train/loss' in data:\n",
        "            df = data['train/loss']\n",
        "            axes[1, 0].plot(df['step'], df['value'], color='red')\n",
        "            axes[1, 0].set_title('Training Loss')\n",
        "            axes[1, 0].set_xlabel('Steps')\n",
        "            axes[1, 0].set_ylabel('Loss')\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot learning rate\n",
        "        if 'train/learning_rate' in data:\n",
        "            df = data['train/learning_rate']\n",
        "            axes[1, 1].plot(df['step'], df['value'], color='green')\n",
        "            axes[1, 1].set_title('Learning Rate')\n",
        "            axes[1, 1].set_xlabel('Steps')\n",
        "            axes[1, 1].set_ylabel('LR')\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"‚úÖ Training plots generated!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No TensorBoard data found yet. Train for a few more steps.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not generate plots: {e}\")\n",
        "    print(\"   This is normal if training just started.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_model"
      },
      "source": [
        "## üíæ Save and Download Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "download_model"
      },
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create a zip file with all models and logs\n",
        "print(\"üì¶ Packaging models and logs...\")\n",
        "\n",
        "output_zip = f\"reinforcetactics_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "shutil.make_archive(output_zip, 'zip', log_dir)\n",
        "\n",
        "print(f\"‚úÖ Created {output_zip}.zip\")\n",
        "print(f\"   Size: {os.path.getsize(output_zip + '.zip') / 1024 / 1024:.2f} MB\")\n",
        "\n",
        "# Download\n",
        "print(\"\\n‚¨áÔ∏è  Downloading...\")\n",
        "files.download(output_zip + '.zip')\n",
        "\n",
        "print(\"\\n‚úÖ Download complete!\")\n",
        "print(\"\\nüìù The zip contains:\")\n",
        "print(\"   - final_model.zip (trained model)\")\n",
        "print(\"   - best_model/ (best performing checkpoint)\")\n",
        "print(\"   - checkpoints/ (periodic checkpoints)\")\n",
        "print(\"   - tensorboard/ (training logs)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_model"
      },
      "source": [
        "## üìÇ Load and Test a Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "test_loaded_model"
      },
      "source": [
        "# Load a previously saved model\n",
        "print(\"üìÇ Loading saved model...\")\n",
        "\n",
        "loaded_model = PPO.load(final_model_path, env=eval_env)\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Quick test\n",
        "print(\"\\nüéÆ Running quick test...\")\n",
        "obs = eval_env.reset()\n",
        "for i in range(5):\n",
        "    action, _ = loaded_model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = eval_env.step(action)\n",
        "    print(f\"Step {i+1}: Action={action}, Reward={reward[0]:.2f}\")\n",
        "    if done[0]:\n",
        "        print(\"Episode finished!\")\n",
        "        break\n",
        "\n",
        "print(\"\\n‚úÖ Model test complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced"
      },
      "source": [
        "## üöÄ Advanced: Hyperparameter Tuning\n",
        "\n",
        "Try different hyperparameters to improve performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyperparam_sweep"
      },
      "source": [
        "# Example hyperparameter configurations to try\n",
        "hyperparam_configs = [\n",
        "    {'learning_rate': 3e-4, 'n_steps': 2048, 'ent_coef': 0.01, 'name': 'baseline'},\n",
        "    {'learning_rate': 1e-4, 'n_steps': 4096, 'ent_coef': 0.005, 'name': 'conservative'},\n",
        "    {'learning_rate': 5e-4, 'n_steps': 1024, 'ent_coef': 0.02, 'name': 'aggressive'},\n",
        "]\n",
        "\n",
        "print(\"üî¨ Hyperparameter configurations available:\")\n",
        "for i, cfg in enumerate(hyperparam_configs):\n",
        "    print(f\"\\n{i+1}. {cfg['name'].title()}:\")\n",
        "    print(f\"   - Learning Rate: {cfg['learning_rate']}\")\n",
        "    print(f\"   - Steps: {cfg['n_steps']}\")\n",
        "    print(f\"   - Entropy Coef: {cfg['ent_coef']}\")\n",
        "\n",
        "print(\"\\nüí° To try a different configuration:\")\n",
        "print(\"   1. Update the 'config' dictionary above\")\n",
        "print(\"   2. Re-run the training cells\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tips"
      },
      "source": [
        "## üí° Tips for Better Training\n",
        "\n",
        "1. **Use GPU Runtime**: Change to GPU runtime for 5-10x faster training\n",
        "2. **Train Longer**: Increase `total_timesteps` to 500k-1M for better results\n",
        "3. **Monitor TensorBoard**: Watch for signs of overfitting or instability\n",
        "4. **Tune Hyperparameters**: Try different learning rates and batch sizes\n",
        "5. **Save Checkpoints**: Models are saved every 20k steps automatically\n",
        "6. **Curriculum Learning**: Start with easier opponents, gradually increase difficulty\n",
        "\n",
        "## üêõ Troubleshooting\n",
        "\n",
        "- **Out of Memory**: Reduce `n_envs` or `batch_size`\n",
        "- **Slow Training**: Enable GPU runtime\n",
        "- **Unstable Learning**: Reduce learning rate, increase batch size\n",
        "- **Not Learning**: Check reward shaping, increase exploration (ent_coef)\n",
        "\n",
        "## üìö Next Steps\n",
        "\n",
        "1. Deploy the trained model to play against humans\n",
        "2. Implement self-play for stronger agents\n",
        "3. Add hierarchical RL for complex strategies\n",
        "4. Create a tournament system for multiple agents\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}